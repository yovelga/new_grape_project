\addvspace {10\p@ }
\contentsline {table}{\numberline {1.1}{\ignorespaces Estimated financial losses from fruit cracking in major crops.\blx@tocontentsinit {0}\parencite {CrackSense2022}. }}{12}{table.1.1}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Pixel-level datasets.}}{44}{table.4.1}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Whole-image datasets.}}{44}{table.4.2}%
\contentsline {table}{\numberline {4.3}{\ignorespaces Exact dataset statistics used in the experiments. For whole-image inference, Row~1 is used for calibration/tuning and Row~2 for held-out testing.}}{45}{table.4.3}%
\contentsline {table}{\numberline {4.4}{\ignorespaces Pixel-level exports: dataset composition.}}{45}{table.4.4}%
\contentsline {table}{\numberline {4.5}{\ignorespaces Multi-class pixel-level dataset: exported pixels by class.}}{45}{table.4.5}%
\contentsline {table}{\numberline {4.6}{\ignorespaces Multi-class dataset: class definitions.}}{48}{table.4.6}%
\contentsline {table}{\numberline {4.7}{\ignorespaces Autoencoder training hyperparameters.}}{54}{table.4.7}%
\contentsline {table}{\numberline {4.8}{\ignorespaces Evaluation units and split strategy used across experiments. ``Independent test'' denotes the data that remain unseen during fitting/tuning for the corresponding protocol.}}{60}{table.4.8}%
\contentsline {table}{\numberline {4.9}{\ignorespaces Evaluation metrics for pixel-level and anomaly detection experiments}}{62}{table.4.9}%
\contentsline {table}{\numberline {4.10}{\ignorespaces Evaluation metrics for whole-image and patch-based detection}}{64}{table.4.10}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Summary statistics of reflectance values across spectral regions for healthy and cracked tissue.}}{67}{table.5.1}%
\contentsline {table}{\numberline {5.2}{\ignorespaces Regional spectral separability metrics between healthy and cracked tissue. Mean values are averaged across wavelengths within each range, while peak values indicate the maximum separability and the corresponding wavelength.}}{67}{table.5.2}%
\contentsline {table}{\numberline {5.3}{\ignorespaces Pixel-level classification metrics (unbalanced training set). Values are reported as mean $\pm $ standard deviation across folds where available. Precision/recall and ROC--AUC refer to the \textit {Cracked} class (positive class); F1 is the weighted F1-score.}}{71}{table.5.3}%
\contentsline {table}{\numberline {5.4}{\ignorespaces Pixel-level classification metrics (balanced training set). Values are reported as mean $\pm $ standard deviation across folds where available. Precision/recall and ROC--AUC refer to the \textit {Cracked} class (positive class); F1 is the weighted F1-score.}}{71}{table.5.4}%
\contentsline {table}{\numberline {5.5}{\ignorespaces Pixel-level classification metrics (\textbf {unbalanced} training set). Values are reported as mean $\pm $ standard deviation across folds.}}{73}{table.5.5}%
\contentsline {table}{\numberline {5.6}{\ignorespaces Pixel-level classification metrics (\textbf {balanced} training set). Values are reported as mean $\pm $ standard deviation across folds.}}{74}{table.5.6}%
\contentsline {table}{\numberline {5.7}{\ignorespaces Pixel-level anomaly detection performance using autoencoders trained on non-cracked samples (unbalanced sampling). Values are reported as mean $\pm $ standard deviation across folds. Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit {Cracked} class (positive class).}}{77}{table.5.7}%
\contentsline {table}{\numberline {5.8}{\ignorespaces Pixel-level anomaly detection performance using autoencoders trained on non-cracked samples (balanced sampling). Values are reported as mean $\pm $ standard deviation across folds. Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit {Cracked} class (positive class).}}{77}{table.5.8}%
\contentsline {table}{\numberline {5.9}{\ignorespaces Pixel-level anomaly detection performance using autoencoders trained exclusively on \textit {Cracked} pixels and evaluated in a multi-class setting (unbalanced sampling). Values are reported as mean across LOGO folds. Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit {Cracked} class (positive class).}}{79}{table.5.9}%
\contentsline {table}{\numberline {5.10}{\ignorespaces Pixel-level anomaly detection performance using autoencoders trained exclusively on \textit {Cracked} pixels and evaluated in a multi-class setting (balanced sampling). Values are reported as mean $\pm $ standard deviation across LOGO folds. Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit {Cracked} class (positive class).}}{79}{table.5.10}%
\contentsline {table}{\numberline {5.11}{\ignorespaces Pixel-level anomaly detection performance using autoencoders trained on all semantic classes except \textit {Cracked} and evaluated in a multi-class setting (unbalanced sampling). Values are reported for the held-out test split. Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit {Cracked} class (positive class).}}{81}{table.5.11}%
\contentsline {table}{\numberline {5.12}{\ignorespaces Pixel-level anomaly detection performance using autoencoders trained on all semantic classes except \textit {Cracked} and evaluated in a multi-class setting (balanced sampling). Values are reported for the held-out test split. Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit {Cracked} class (positive class).}}{81}{table.5.12}%
\contentsline {table}{\numberline {5.13}{\ignorespaces Dataset composition for full-image crack-detection experiments under early- and late-stage acquisition conditions (initial/minimal vs.\ advanced cracking). Row~1 was used for calibration, while Row~2 served as an independent test set.}}{83}{table.5.13}%
\contentsline {table}{\numberline {5.14}{\ignorespaces Full-image crack detection performance for early- and late-stage acquisition experiments (initial/minimal vs.\ advanced cracking). Metrics are reported separately for calibration and test splits. Confusion-matrix entries correspond to the number of grape clusters classified as cracked or healthy. The main performance indicator is \textbf {F1 (Cracked)}. Accuracy, Balanced Accuracy and MCC are reported overall. Precision/Recall/F1/F2 refer to the \textit {Cracked} class (positive class), while Specificity and NPV refer to the \textit {Healthy} class (negative class).}}{84}{table.5.14}%
\contentsline {table}{\numberline {5.15}{\ignorespaces Full-image crack detection using autoencoder-based anomaly detection (128--64--32, trained on \textit {Cracked} pixels). Reconstruction errors (MSE) were converted to pseudo-probabilities via exponential decay, $p_{\mathrm {CRACK}}(x)=\exp (-e(x)/\tau )$; the same post-processing pipeline (thresholding, morphological filtering, patch-based aggregation) was applied. Metrics are reported for calibration and test splits under early- and late-stage conditions. The main performance indicator is \textbf {F1 (Cracked)}. Accuracy, Balanced Accuracy and MCC are reported overall. Precision/Recall/F1/F2 refer to the \textit {Cracked} class (positive class), while Specificity and NPV refer to the \textit {Healthy} class (negative class).}}{85}{table.5.15}%
\contentsline {table}{\numberline {5.16}{\ignorespaces Summary of BFS wavelength-count thresholds for CRACK metrics.}}{87}{table.5.16}%
\contentsline {table}{\numberline {5.17}{\ignorespaces Full-image crack-detection performance for different wavelength subsets under early- and late-stage conditions.}}{88}{table.5.17}%
\contentsline {table}{\numberline {5.18}{\ignorespaces Model Generalization Analysis: Comparison between Calibration and Test MCC.}}{88}{table.5.18}%
\contentsline {table}{\numberline {5.19}{\ignorespaces BFS stability: per-seed results under varying train/test splits.}}{90}{table.5.19}%
\contentsline {table}{\numberline {5.20}{\ignorespaces Pairwise Jaccard similarity of top-30 wavelengths across seeds (varying split). Mean Jaccard = 0.493.}}{90}{table.5.20}%
\contentsline {table}{\numberline {5.21}{\ignorespaces Pairwise Jaccard similarity of top-11 wavelengths across seeds (varying split). Mean Jaccard = 0.398.}}{90}{table.5.21}%
\addvspace {10\p@ }
\addvspace {10\p@ }
