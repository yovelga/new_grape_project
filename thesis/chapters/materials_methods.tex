\chapter{Materials and Methods}

% -----------------------------
% DATA ACQUISITION
% -----------------------------
\section{Data Acquisition}

\subsection{Study Area and Experimental Design}

\noindent
The study was conducted in the Lachish region of Israel, an area characterized by a semi-arid climate with a hot, dry summer and a mild winter. These conditions make the region a central hub for grape cultivation (viticulture) in Israel. The experiment was conducted in a commercial table grape vineyard (\textit{Vitis vinifera} L., cv. `Scarlotta'), in collaboration with the ``Tali Grapes'' brand. The vineyard was grafted onto 1103 Paulsen rootstock and was planted in 2018.

\noindent
Figure~\ref{fig:vineyard_map} summarizes the block structure and irrigation assignment used throughout the study, clarifying the spatial provenance of the samples used later for row-based calibration and testing.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/materials_methods/study_area/vineyard_map.png}
    \caption{Map of the experimental vineyard showing the five randomized blocks (A–E) and the four irrigation regimes applied within each block.}
    \label{fig:vineyard_map}
\end{figure}

\medskip
\noindent
The experimental design followed a Randomized Complete Block Design (RCBD), including
five replicates (Blocks A--E). Within each block, four different irrigation regimes were
tested, resulting in a total of 20 experimental plots ($5~\text{blocks} \times 4~\text{treatments}$).

\medskip
\noindent
Each experimental plot consisted of three consecutive rows of nine vines each
(27 vines per plot). To minimize edge effects and water spillover between irrigation
regimes, only vines from the middle row of each plot were selected for measurement.
From each middle row, 12 representative grape clusters were tagged, yielding a total
sample size of 240 clusters ($20~\text{plots} \times 12~\text{clusters}$).

\medskip
\noindent
The four irrigation regimes tested were:
\begin{itemize}
    \item \textbf{High Irrigation (Blue):} 796 mm of water throughout the season.
    \item \textbf{Low Irrigation (White):} 376 mm of water throughout the season.
    \item \textbf{Reducing Irrigation (Red):} A transition from high to low irrigation
    (total 532 mm), beginning at the post-veraison stage.
    \item \textbf{Increasing Irrigation (Yellow):} A transition from low to high irrigation
    (total 645 mm), beginning at the post-veraison stage.
\end{itemize}

\medskip
\noindent
Irrigation commenced on April 29, 2024, and was applied daily or every other day via
a drip irrigation system.

\medskip
\noindent
The experiment continued until the end of September 2024. Physiological and yield
measurements—including stem water potential, stomatal conductance, fruit properties,
and percentage of healthy clusters—were performed weekly. Physiological data were
collected using a Scholander Pressure Bomb and a Porometer, and soil moisture was
measured using Soil Moisture Sensors.



\subsection{Imaging Systems}

\noindent
To study grape cracking in a comprehensive way, a multi-modal imaging setup was used. The system combined three imaging sensors placed at the same location, each capturing different types of information from a different part of the electromagnetic spectrum. Together, these sensors provided complementary spectral, spatial, and thermal data, allowing a more complete analysis of grape berry condition and cracking development. All images were collected from a fixed distance of approximately 1 m from the grape clusters to ensure consistent scale and alignment across all measurements.

\subsubsection{Hyperspectral Imaging System (HSI)}
\noindent
Hyperspectral data were collected using the \textbf{Specim IQ} (Specim, Spectral Imaging Ltd., Oulu, Finland), a portable snapshot hyperspectral camera designed for field applications. The camera operates in the Visible and Near-Infrared (VNIR) spectral range (400--1000~nm) and produces 3D datacubes of size 512~$\times$~512~$\times$~204 for each snapshot.

\noindent
Figure~\ref{fig:hsi_setup} documents the practical acquisition setup under vineyard conditions; this field configuration is important context for the later performance gap between pixel-level and whole-image inference under background clutter and illumination variability.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/materials_methods/equipment/VICTOR_AND_ALON_WITH_SPECIM_IQ.jpeg}
        \caption{Field deployment of the Specim IQ system in the vineyard.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/materials_methods/equipment/taking_picture_with_hsi.jpg}
        \caption{Hyperspectral image acquisition of grape clusters using the Specim IQ.}
    \end{subfigure}
    \caption{Hyperspectral imaging setup and data acquisition using the Specim IQ system under vineyard conditions.}
    \label{fig:hsi_setup}
\end{figure}



 
\begin{itemize}
    \item \textbf{Spectral Resolution:} The system captures 204 contiguous spectral bands with a Full Width at Half Maximum (FWHM) of 7~nm and a spectral sampling rate of approximately 3~nm. The high spectral fidelity is ensured by a signal-to-noise ratio (SNR) of $>400\!:\!1$.
    \item \textbf{Spatial Resolution:} The sensor has a resolution of $512 \times 512$~pixels. With a $31^\circ \times 31^\circ$ Field of View (FOV), at a 1~m working distance, the camera covers a physical area of approximately $0.55~\text{m} \times 0.55~\text{m}$. This configuration yields a theoretical spatial resolution of approximately \textbf{1.08~mm/pixel}, sufficient for resolving individual berries and larger cracks.
    \item \textbf{Radiometric Calibration:} A spectrally flat, diffuse white reference panel (Spectralon\textsuperscript{\textregistered}, $\approx99\%$ reflectance) was included in every captured scene. This allowed subsequent normalization of raw radiance data into relative reflectance values, accounting for varying natural illumination conditions in the vineyard.
\end{itemize}

\subsubsection{RGB Imaging System}
\noindent
To provide high-resolution visual context and capture fine surface textures that might be missed by the lower spatial resolution of the HSI system, a high-end digital camera was used.

\begin{itemize}
    \item \textbf{Camera \& Sensor:} \textbf{Canon EOS R5}, equipped with a 45-megapixel full-frame ($36 \times 24$~mm) CMOS sensor, producing 14-bit RAW images with a resolution of $8192 \times 4608$~pixels.
    \item \textbf{Optics:} The camera was fitted with a \textbf{35~mm} prime lens, providing a wide Field of View of $54.4^\circ$ (horizontal)~$\times$~$37.8^\circ$ (vertical) and sufficient depth of field to keep the entire cluster in focus.
    \item \textbf{Spatial Resolution:} At the 1~m working distance, this system achieved an extremely high theoretical spatial resolution of approximately \textbf{0.13~mm/pixel}. This level of detail is critical for detecting micro-cracks (hairline fissures) and minute alterations in the berry skin cuticle before they become visible to the naked eye or the HSI sensor.
\end{itemize}

\subsubsection{Thermal Imaging System}
\noindent
Canopy and grape cluster temperatures, which are widely used as indirect indicators of
plant water status, stomatal conductance, and transpirational cooling dynamics, were
monitored using a high-definition thermal imaging system.

\noindent
Figure~\ref{fig:thermal_setup} illustrates the thermal modality acquired alongside Theramal Camera (FLIR T1020).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/materials_methods/equipment/yovel_with_thermal_camera.jpg}
        \caption{FLIR T1020 thermal camera used for vineyard measurements.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/materials_methods/equipment/thermal_image_with_decay_and_cracksjpg.jpg}
        \caption{Thermal image of a grape cluster showing surface temperature variations.}
    \end{subfigure}
    \caption{Thermal imaging system and example thermal data acquired from grape clusters under vineyard conditions.}
    \label{fig:thermal_setup}
\end{figure}

\FloatBarrier



\begin{itemize}
    \item \textbf{Camera:} \textbf{FLIR T1020} (Teledyne FLIR LLC, Wilsonville, OR, USA), a high-performance uncooled microbolometer specifically designed for precision research.
    \item \textbf{Sensor \& Optics:} The camera has a thermal resolution of $1024 \times 768$~pixels with a 17~$\mu$m pixel pitch, operating in the Long-Wave Infrared (LWIR) spectral range ($7.5$--$14~\mu$m). It was equipped with a standard \textbf{34~mm} lens (adjustable focus including autofocus), providing a FOV of $45^\circ$~(H)~$\times$~$34^\circ$~(V).
    \item \textbf{Performance:} The system boasts a high thermal sensitivity (Noise Equivalent Temperature Difference, NETD) of $<20$~mK ($<0.02^\circ$C) at $30^\circ$C, enabling the detection of subtle temperature gradients across the grape cluster.
    \item \textbf{Spatial Resolution:} At the 1~m working distance, the theoretical thermal spatial resolution was approximately \textbf{0.81~mm/pixel}, allowing for accurate temperature extraction from individual berries while minimizing mixed-pixel effects from the surrounding air or leaves.
\end{itemize}

\medskip
\noindent
Due to the high dimensionality of the acquired data, individual hyperspectral images were substantially larger than the corresponding RGB and thermal images. On average, a single hyperspectral datacube occupied approximately \textbf{200~MB}, compared to approximately \textbf{11~MB} for a high-resolution RGB image and \textbf{2.5~MB} for a thermal image. In total, the imaging campaign resulted in a dataset of approximately \textbf{575~GB} of raw data, highlighting the computational and storage challenges associated with multi-modal hyperspectral analysis.


% \medskip
% \noindent
% The experimental campaign comprised two complementary components: (i) an imaging-based
% study focused on hyperspectral and RGB data for crack detection and early-warning screening, and
% (ii) a parallel physiological monitoring framework using conventional plant and soil
% sensors. The present work addresses the imaging component, while the physiological
% measurements provide contextual support and form the basis for future integrative
% analyses.


\subsection{Data Collection Protocol}

\noindent
Data collection was conducted during concentrated field days, typically once a week (Mondays or Thursdays), throughout the growing season from June to the end of September. To ensure consistent lighting conditions, field operations were strictly scheduled between 08:00 and 17:00.

\subsubsection{Daily Schedule and Logistics}
\noindent
Each collection day began at 05:30 at the Volcani Institute with equipment organization and verification. Departure to the vineyard in Lachish occurred at approximately 06:00. Upon arrival at 07:00--08:00, the imaging systems were mounted on their respective tripods and calibrated.

\subsubsection{Sampling Strategy}
\noindent
Due to significant differences in acquisition time between modalities, a split sampling strategy was employed:
\begin{itemize}
    \item \textbf{RGB Imaging:} All 240 marked grape clusters (12 clusters $\times$ 20 plots) were imaged first during each field day to capture them under relatively uniform early-morning lighting.
    \item \textbf{Hyperspectral Imaging (HSI):} Due to the complex acquisition process---which required re-positioning the camera and white reference panel for each cluster, followed by a dedicated spectral focusing procedure---each HSI scan took approximately 2-3 minutes. Consequently, only a subset of 120 clusters was imaged per session. These clusters were selected to represent a balanced cross-section of the experimental blocks, ensuring full dataset coverage over alternating sessions. HSI acquisition typically commenced around 10:00 AM when sunlight was more stable and intense.
    \item \textbf{Thermal Imaging:} Thermal image acquisition was purposely scheduled for later in the day (starting around 12:00 PM), after RGB imaging was complete. This timing leverages the high ambient temperatures typical of midday in the semi-arid Negev region to maximize thermal contrast on the berry surface. Specifically, cracked or decaying areas, often characterized by exposed moist tissue or conversely by localized dehydration, exhibit different thermal signatures compared to healthy, intact skin due to variations in evaporative cooling rates. Capturing images during peak heat enhances these temperature differentials, improving the detectability of such defects.
\end{itemize}

\subsubsection{Field Procedure}
\noindent
To ensure accurate re-imaging, a consistent labeling system was implemented. Target clusters were marked with laminated, color-coded tags attached via plastic cable ties. The tags featured a numeric identifier (1--60), which was repeated across treatments but distinguished by the specific color assigned to each irrigation regime. This design allowed for unambiguous identification and durability against humidity. Furthermore, vine locations were georeferenced using a GPS module integrated into the thermal camera.

\medskip
\noindent
Although only the RGB and HSI datasets were analyzed in the present study, all imaging systems were spatially co-aligned and temporally synchronized during field acquisition to facilitate potential future analyses.

% -----------------------------
% PREPROCESSING
% -----------------------------

% ============================================================
% ULTRA-CONCISE VERSION: Even Shorter
% ============================================================

% Table 4.1a: Pixel-Level
\begin{table}[htbp]
\centering
\caption{Pixel-level datasets.}
\label{tab:dataset_pixel_ultra}
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{2cm} 
                              >{\raggedright\arraybackslash}p{2cm} 
                              >{\raggedright\arraybackslash}X 
                              >{\raggedright\arraybackslash}p{3cm} 
                              >{\centering\arraybackslash}p{0.7cm}}
\toprule
\textbf{Dataset} & \textbf{Labels} & \textbf{Purpose} & \textbf{Method} & \textbf{H.} \\
\midrule

\textbf{Binary} & 
Healthy; Cracked & 
Spectral differences: intact vs damaged & 
\newline RGB Pixel →
\newline SAM2 →
\newline HS segment extract & 
H1, H2 \\

\cmidrule(lr){1-5}

\textbf{Multi-Class (10)} & 
\newline Cracked
\newline Healthy
\newline Leaf
\newline Background 
\newline and more... & 
Spectral signatures: all materials & 
\newline RGB Pixel →
\newline SAM2 →
\newline HS segment extract & 
H3 \\

\bottomrule
\end{tabularx}
\end{table}


% Table 4.1b: Whole-Image
\begin{table}[htbp]
\centering
\caption{Whole-image datasets.}
\label{tab:dataset_image_ultra}
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{2cm} 
                              >{\raggedright\arraybackslash}p{2cm} 
                              >{\raggedright\arraybackslash}X 
                              >{\raggedright\arraybackslash}p{3cm} 
                              >{\centering\arraybackslash}p{0.7cm}}
\toprule
\textbf{Dataset} & \textbf{Acquisition Stage} & \textbf{Purpose} & \textbf{Method} & \textbf{H.} \\
\midrule

\textbf{Late-Stage} & 
Advanced cracking & 
Detection under visible crack conditions & 
\newline Binary label & 
H3 \\

\cmidrule(lr){1-5}

\textbf{Early-Stage} & 
Initial onset & 
Detection at early crack-development stage & 
\newline Binary label &
H4 \\

\bottomrule
\end{tabularx}

\vspace{0.15em}
\footnotesize
\textit{Note:} Early/late refer to acquisition-stage subsets at different crack-progression levels (time of measurement). HSI+RGB pairs. Pixel: segment→label→extract. Image: direct labeling.
\end{table}



\subsection{Dataset Statistics}
\label{sec:dataset_statistics}

% Dataset statistics were computed on 2026-02-13 using: scripts/analysis/compute_dataset_statistics.py
% Sources (exact files):
% - src/preprocessing/dataset_builder_grapes/detection/raw_exported_data/all_origin_signatures_results.csv
% - src/preprocessing/dataset_builder_grapes/detection/raw_exported_data/all_origin_signatures_results_multiclass_2026-01-16.csv
% - src/models/classification/full_image/inference_to_see_results_of_models_feature_selection/data/val_row1_early copy.csv
% - src/models/classification/full_image/inference_to_see_results_of_models_feature_selection/data/val_row1_late.csv
% - src/models/classification/full_image/inference_to_see_results_of_models_feature_selection/data/test_row2_early copy.csv
% - src/models/classification/full_image/inference_to_see_results_of_models_feature_selection/data/test_row2_late.csv
% - data/raw/HSI_tags.csv
% - data/processed/irrigation_color_descriptions.csv

\begin{table}[htbp]
    \centering
    \small
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{} l l r r r @{} }
        \toprule
        \textbf{Dataset} & \textbf{Split / Variant} & \textbf{Samples} & \textbf{Positive} & \textbf{Negative} \\
        & & & \textbf{(CRACK)} & \textbf{(REGULAR)} \\
        \midrule
        Pixel-Level Binary & Exported labeled pixels & 94{,}166 & 24{,}548  & 69{,}618  \\
        Pixel-Level Multi-Class & Exported labeled pixels & 1{,}189{,}041 & 24{,}548 & 69{,}618 \\
        \midrule
        Whole-Image Early & Row~1 calibration  & 172 & 32 & 140 \\
        Whole-Image Early & Row~2 test  & 60 & 32 & 28 \\
        \midrule
        Whole-Image Late & Row~1 calibration  & 160 & 35 & 125 \\
        Whole-Image Late & Row~2 test  & 60 & 34 & 26 \\
        \bottomrule
    \end{tabular}
    \caption{Exact dataset statistics used in the experiments. For whole-image inference, Row~1 is used for calibration/tuning and Row~2 for held-out testing.}
    \label{tab:dataset_statistics}
\end{table}

\FloatBarrier


\noindent\textbf{Pixel-level exports.}

\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{@{} l r l @{}}
\toprule
\textbf{Dataset} & \textbf{Labeled Pixels} & \textbf{Source} \\
\midrule
Binary         & \num{94166}    & 80 segments across 19 hyperspectral images \\
Multi-class    & \num{1189041}  & 227 segments across 54 hyperspectral images \\
\bottomrule
\end{tabular}
\caption{Pixel-level exports: dataset composition.}
\label{tab:pixel_exports}
\end{table}

\smallskip
\noindent\textit{Multi-class exported pixels by class.}

\begin{table}[htbp]
\centering
\small
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\linewidth}{@{} l r X @{}}
\toprule
\textbf{Class} & \textbf{Pixels} & \textbf{Description} \\
\midrule
BACKGROUND       & 378{,}194 & background pixels (e.g., soil and mixed background). \\
WHITE\_REFERENCE & 270{,}159 & White reference panel pixels. \\
TRIPOD          & 137{,}494 & Tripod and imaging setup components. \\
LEAF            & 120{,}552 & Vine foliage. \\
BRANCH           & 78{,}654 & Woody vine structures. \\
REGULAR          & 69{,}618 & Intact grape tissue without visible defects. \\
PLASTIC          & 66{,}185 & Artificial vineyard elements. \\
BURNT\_PIXEL      & 36{,}676 & Saturated or corrupted pixels. \\
CRACK            & 24{,}548 & Grape tissue exhibiting visible cracking symptoms. \\
IRON              & 6{,}961 & Metallic elements. \\
\bottomrule
\end{tabularx}
\caption{Multi-class pixel-level dataset: exported pixels by class.}
\label{tab:multiclass_pixel_distribution}
\end{table}

\FloatBarrier
\medskip
\newpage

% \subsection{Data Management and Pre-processing}
% \noindent
% Given the large volume and multi-modal nature of the data collected throughout the season, a robust and automated data management pipeline was essential. Custom Python scripts were developed to orchestrate the organization, validation, and preprocessing of the raw imagery.

% \noindent
% The pipeline operated in several stages:
% \begin{itemize}
%     \item \textbf{Automated Organization:} Scripts parsed acquisition timestamps and file metadata to automatically sort thousands of raw images into a structured directory hierarchy, organized by acquisition date, sensor type (RGB, Thermal, HSI), and unique cluster ID.
%     \item \textbf{Metadata Verification (HSI):} For the hyperspectral data, a dedicated verification script scanned the XML metadata files associated with each datacube. This ensured that every acquired image was correctly tagged with its corresponding cluster ID in the field, and generated detailed CSV reports mapping successful acquisitions versus missing data points across all dates.
%     \item \textbf{Quality Assurance:} Following automated sorting, a rigorous manual inspection was conducted. Images were reviewed to confirm proper focus, appropriate lighting, and for the HSI data, the presence of a valid white reference panel within the frame for subsequent radiometric calibration. Only complete, verified, and correctly labeled image sets were advanced to the downstream analysis and labeling stages.
% \end{itemize}



\subsection{Pixel-Level Dataset Preparation}
\label{sec:dataset_pixel}

\noindent
To enable efficient creation of the pixel-level dataset—required to test spectral differences between healthy and cracked grape tissues (\hyperref[hyp:spectral_distinguishability]{Hypothesis~1}) and to assess pixel-level classification feasibility using machine learning (\hyperref[hyp:pixel_level_feasibility]{Hypothesis~2})—a custom graphical user interface (GUI) was developed in Python. The tool integrates data visualization, interactive annotation, and model inference in a single platform (see Figure~\ref{fig:gui_sam2_pixelpicker}). The dataset generation workflow proceeded as follows:



\vspace{1\baselineskip}
\FloatBarrier
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figures/materials_methods/gui/gui_sam2_pointer_with_numbers.png}
    \caption{Graphical user interface of the HSI – RGB annotation tool. (1) Hyperspectral image visualized as an RGB composite with a SAM2-generated segmentation overlay, (2) wavelength-selectable grayscale HSI band with the same SAM2 segmentation mask, and (3) RGB image of the same grape cluster captured on the same day using a Canon EOS R5. (4) Spectral signature extracted from a single pixel within the SAM2-segmented region, and (5) autoencoder reconstruction of the same spectral signature, discussed later in this work.}
    \label{fig:gui_sam2_pixelpicker}
\end{figure}
\FloatBarrier

\newpage


\begin{enumerate}
    \item \textbf{Image Loading and Visualization:} The tool displayed the hyperspectral datacube alongside corresponding RGB images (HSI camera and Canon EOS R5). A slider enabled interactive exploration of all 204 spectral bands (400--1000 nm).
    \item \textbf{Interactive Pixel Sampling:} The user selected a seed pixel by clicking anywhere on a target object in either the RGB or HSI image. For the binary dataset, berries exhibiting clear cracked or healthy states were selected.
    \item \textbf{SAM2 Segmentation on RGB:} The seed pixel was passed to \textbf{Segment Anything Model 2 (SAM2)}~\cite{ravi2024sam2} as a point prompt. SAM2 operated on the RGB image to generate a binary segmentation mask of the selected object. This mask was then spatially aligned and applied to the corresponding hyperspectral datacube to identify the same region.
    \item \textbf{HSI Data Extraction and Labeling:} The tool extracted spectral signatures from the hyperspectral datacube for all pixels within the segmented region. The user assigned a class label (e.g., `Cracked' or `Regular' for binary classification) to the object, and the annotated spectral data were stored.
\end{enumerate}

\medskip
\noindent
The final exported binary pixel-level dataset used in this work contains 94{,}166 labeled pixels (24{,}548 CRACK; 69{,}618 REGULAR), as summarized in Table~\ref{tab:dataset_statistics}.

\subsection{Expansion to Multi-Class Dataset}
\noindent
The binary pixel-level dataset contained two classes: healthy grape tissue and cracked grape tissue. A multi-class pixel-level dataset was constructed to include additional classes representing non-grape objects present in vineyard scenes. This dataset was designed for training multi-class classifiers and whole-image semantic segmentation models, as specified in \hyperref[hyp:whole_image_robustness]{Hypothesis~3}.

\medskip
\noindent
The same annotation workflow was applied to acquire spectral samples from background objects. Seed pixels were selected on target objects (leaves, branches, soil, artificial materials, imaging equipment) in the RGB or HSI images. SAM2 segmented each selected object on the RGB image, and the resulting mask was applied to the hyperspectral datacube. Spectral signatures were extracted for all pixels within each segmented region. Each region was assigned a class label, and the labeled spectral samples were stored.

\medskip
\noindent
The multi-class dataset comprised ten classes:

\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{@{} l l @{}}
\toprule
\textbf{Class} & \textbf{Description} \\
\midrule
CRACK & Grape tissue with visible surface cracks \\
REGULAR & Intact grape tissue \\
LEAF & Green vine foliage \\
BRANCH & Woody vine structures (stem, cane) \\
BACKGROUND & Soil, ground, and mixed non-object pixels \\
PLASTIC & Plastic vineyard elements (covers, ties) \\
TRIPOD & Camera tripod components \\
WHITE\_REFERENCE & Calibration panel pixels \\
BURNT\_PIXEL & Saturated or overexposed pixels \\
IRON & Metal objects (wires, stakes) \\
\bottomrule
\end{tabular}
\caption{Multi-class dataset: class definitions.}
\label{tab:multiclass_definitions}
\end{table}

\noindent
The class distribution and dataset size are reported in Table~\ref{tab:multiclass_pixel_distribution}. The multi-class dataset was exported as labeled spectral samples and used for training supervised multi-class classification models.


\subsection{Whole-Image Dataset Construction and Experimental Design}
\label{subsec:whole_image_dataset}

\subsubsection{Dataset Composition}

\noindent
The whole-image dataset is defined at the \emph{(grape\_id, week\_date)} level, where each record corresponds to a single hyperspectral image of a grape cluster. Sample counts and class distributions for all experimental subsets are reported in Table~\ref{tab:dataset_statistics}.

\subsubsection{Spatial Partitioning}

\noindent
A row-based spatial partitioning scheme was applied to ensure physically disjoint datasets. All records acquired from vineyard Row~1 were assigned to the calibration set, while records from Row~2 were reserved as a held-out evaluation set. The two rows are spatially separated within the experimental vineyard, minimizing spatial autocorrelation and preventing information leakage between sets.

\subsubsection{Subset Usage Protocol}

\noindent
The calibration set (Row~1) was used for model training and for internal parameter calibration of the whole-image inference procedure. The held-out test set (Row~2) was used exclusively for final performance evaluation and was not involved in model training, parameter tuning, or threshold selection.

\subsubsection{Training Data Definition}

\noindent
The whole-image pipeline used a pretrained pixel-level classifier. The pixel-level model was trained on spectral signatures (pixels) sampled from Late-stage images in vineyard Row~1 only. Early-stage records were excluded from pixel-level training.

\medskip
\noindent
Whole-image hyperparameter tuning was performed using Row~1 only. Tuned parameters included the spatial aggregation and post-processing settings used to convert pixel-level scores into image-level outputs (e.g., patch aggregation thresholds, crack-pixel criteria, and blob filtering parameters such as minimum/maximum region area).

\subsubsection{Inference on the Held-out Evaluation Set}

\noindent
After training and calibration, the finalized model and inference parameters were applied to the held-out evaluation set (Row~2) without modification.

\subsubsection{Post-Inference Acquisition-Stage Stratification}

\noindent
Following inference, outputs were stratified by acquisition stage for reporting. Stage labels were assigned according to vineyard monitoring records:

\begin{enumerate}
    \item \textbf{Late-stage:} Images acquired after the first recorded cracking event, representing an advanced cracking condition.
    \item \textbf{Early-stage:} The earliest available image associated with the onset of cracking for the same cluster, representing the initial observable stage.
\end{enumerate}

\medskip
\noindent
Both Early-stage and Late-stage results were generated using the same trained pixel-level model. Whole-image inference parameters were tuned separately for Early-stage and Late-stage using Row~1 and then applied to the corresponding stage-specific records in the held-out evaluation set (Row~2).

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.96\linewidth}
    \centering
    Row split $\rightarrow$ Train pixel-level model on pixels from Late-stage (Row~1) $\rightarrow$ Tune full-image aggregation hyperparameters on Row~1 (separately for Early/Late) $\rightarrow$ Freeze parameters $\rightarrow$ Inference on held-out evaluation set (Row~2) $\rightarrow$ Stage-specific reporting (Early/Late)
    \end{minipage}}
    \caption{Protocol overview for the row-based split, training and calibration on Row~1 Late-stage data, held-out evaluation on Row~2, and post-inference reporting stratified by acquisition stage.}
    \label{fig:whole_image_protocol_overview}
\end{figure}


% -----------------------------
% MODELING AND EVALUATION
% -----------------------------
\section{Modeling and Evaluation Methodology}
\label{sec:modeling_evaluation}

\noindent
This section describes the modeling framework and evaluation protocols employed to
analyze hyperspectral data for grape crack detection and early-warning screening. The methodology
is structured to progressively assess spectral discriminability, define appropriate
problem formulations, and evaluate model generalization from pixel-level classification
to whole-image detection, while maintaining reproducibility and methodological
consistency across experiments.




% ============================================================================
% Modeling and Evaluation Methodology — Chapter Structure
% ============================================================================
% This chapter describes the complete methodological pipeline used in this study,
% from spectral preprocessing through pixel-level modeling and whole-image evaluation.
%
% ├─ Global Spectral Preprocessing
% │  ├─ Spectral range selection
% │  ├─ Quality control and outlier handling
% │  └─ Per-spectrum normalization
% │
% ├─ Spectral Discriminability and Informative Wavelength Analysis
% │  └─ Feature selection as a derived outcome
% │     (Fisher Score, Mutual Information, Random Forest importance)
% │
% ├─ Pixel-Level Classification Framework
% │  ├─ Binary Pixel-Level Classification
% │  └─ Multi-Class Pixel-Level Classification
% │
% ├─ Anomaly Detection Framework (One-Class)
% │
% ├─ Dimensionality Reduction Strategy
% │  ├─ For binary pixel-level models
% │  └─ For multi-class pixel-level models
% │
% ├─ Whole-Image Detection and Aggregation Pipeline
% │  ├─ Spatial aggregation and region-level scoring
% │  ├─ Patch-based decision logic
% │  └─ Evaluation across early and late disease stages
% │
% └─ Evaluation Protocols
%    ├─ Leave-One-Group-Out (LOGO) cross-validation
%    └─ Performance metrics per experimental stage
%
% ============================================================================


\subsection{Global Spectral Preprocessing}
\label{subsec:global_preprocessing}

\noindent
All hyperspectral data were subjected to a unified and fixed preprocessing
pipeline prior to any modeling or evaluation. This preprocessing was applied
consistently across all datasets, models, and experimental settings to ensure
methodological uniformity, reproducibility, and fair comparability between
experiments ~\cite{thenkabail2016hyperspectral}.

\subsubsection{Spectral Range Selection}

\noindent
To avoid the inclusion of spectral regions characterized by consistently low
signal-to-noise ratios, the raw spectral signatures were restricted to a
predefined wavelength range of 450--925 nm. Preliminary analysis of the data
revealed high variance at the spectral extremities (400--450 nm and 925--1000 nm).
This observation aligns with previous performance evaluations of the Specim IQ
camera; notably, Behmann et al. \cite{behmann2018specim} reported increased noise
levels in the 400--450 nm range and significant reflectance artifacts in the
near-infrared region (specifically above 925 nm) under outdoor conditions.
Consequently, this effective operating range was selected once and retained
throughout the study to avoid experiment-specific preprocessing decisions and
to ensure consistency across all analyses \cite{bioucas2013hyperspectral}.

\medskip
\noindent
This wavelength window was treated as a
\emph{global} design choice rather than a tunable hyperparameter: it was fixed
prior to model development and kept identical for all cross-validation folds,
feature-selection experiments, and final held-out tests.

\subsubsection{Quality Control and Outlier Handling}
\label{subsubsec:qc_outliers}

\noindent
Hyperspectral datacubes were analyzed in normalized reflectance units, where
spectral values are expected to lie within the physically valid range
$[0,1]$. Due to field acquisition conditions and imperfect radiometric
normalization, isolated violations of this range may occur and indicate
technically corrupted measurements rather than biological variability.

% \medskip
% \noindent
% Outlier detection was performed using a robust multivariate statistical
% approach based on the Mahalanobis distance, computed within each spectral
% class separately. The Minimum Covariance Determinant (MCD)
% estimator~\cite{rousseeuw1999fast} was employed to obtain a robust estimate
% of the multivariate location and scatter matrix, thereby reducing the
% influence of the very outliers targeted for removal.

% \medskip
% \noindent
% For each class, the MCD estimator was fitted to a random subsample of up to
% 20{,}000 spectra (support fraction~$= 0.8$) to ensure computational
% tractability. The squared Mahalanobis distance~$d^2_i$ was then computed for
% every pixel spectrum~$\mathbf{x}_i$ within the class:
% \[
% d^2_i \;=\; (\mathbf{x}_i - \hat{\boldsymbol{\mu}})^\top \,
%             \hat{\boldsymbol{\Sigma}}^{-1} \,
%             (\mathbf{x}_i - \hat{\boldsymbol{\mu}})
% \]
% where $\hat{\boldsymbol{\mu}}$ and $\hat{\boldsymbol{\Sigma}}$ denote the
% robust location and covariance estimates, respectively.



% \medskip
% \noindent
% Spectra exceeding the $(1 - p)$-th percentile of the per-class distance
% distribution were flagged as outliers. The outlier fraction~$p$ was treated as
% a configurable parameter. In the final experiments reported in this thesis,
% outlier removal was not applied ($p = 0$), because the classification models
% employed --- gradient-boosted decision trees (XGBoost) --- are inherently
% robust to moderate spectral noise and isolated anomalous
% pixels~\cite{thenkabail2016hyperspectral}. Retaining all extracted spectra
% ensured maximal representation of rare physiological variability, including
% early-stage stress signatures.

% \medskip
% \noindent
% Since $p = 0$ in the reported experiments, no data-dependent filtering was applied that
% could couple training and evaluation partitions.

% \medskip
% \noindent
% All outlier flags were computed and stored alongside the spectral data to
% enable transparent post-hoc analysis and reproducibility of alternative
% exclusion criteria.

% \medskip
% \noindent
% Preliminary experiments with robust Mahalanobis-based outlier detection (MCD) did not yield consistent improvements and were therefore not included in the final

\subsubsection{Per-Spectrum Normalization}

\noindent
Following quality control, each remaining spectral vector was normalized independently using
 \textit{Standard Normal Variate} (SNV) on a per-pixel basis. In this transformation,
each spectrum was mean-centered and scaled by its standard deviation to reduce additive
and multiplicative effects associated with illumination variability, surface geometry,
and scattering, while preserving relative spectral shape. SNV is widely used in close-range
hyperspectral imaging for reducing non-chemical variability without distorting discriminative spectral features
 \cite{barnes1989snv,mishra2017close,spath2024separating}.

% \medskip
% \noindent
% Per-spectrum normalization was deliberately selected to avoid reliance on
% dataset-level statistics and to eliminate the risk of information leakage between
% training and evaluation sets, particularly under cross-validation schemes such
% as Leave-One-Group-Out (LOGO). Information leakage during preprocessing has been
% identified as a critical issue in hyperspectral image classification, leading to
% over-optimistic performance estimates when not properly controlled
% \cite{li2023information,kaltenborn2025data}.

% \medskip
% \noindent
% Concretely, for each spectrum $\mathbf{x} \in \mathbb{R}^B$, SNV computes
% $\tilde{\mathbf{x}} = (\mathbf{x}-\mu(\mathbf{x}))/(\sigma(\mathbf{x})+\epsilon)$
% using only that spectrum's own mean and standard deviation. No statistics were
% estimated across samples, dates, grape clusters, or folds; therefore, SNV does
% not introduce train--test coupling under LOGO or under the row-based whole-image
% split.

% \medskip
% \noindent
% Any additional transformation that requires learned parameters (e.g., global
% standardization, or supervised feature selection) was fit exclusively on the
% training partition of the relevant protocol and then applied to the
% corresponding held-out data without refitting.

\subsection{Cross-Validation and Data Partitioning Strategy}
\label{subsec:cv_strategy}

\noindent
To assess generalization to unseen biological samples, Leave-One-Group-Out (LOGO)
cross-validation was applied at the grape-cluster acquisition level. LOGO was selected
due to the limited number of available hyperspectral images, which precluded a simple
held-out test set while still requiring rigorous evaluation of cluster-level
generalization. All pixels extracted from the same hyperspectral image (corresponding
to a unique grape cluster) were assigned exclusively to either the training set or the
test set within each fold. This grouping strategy ensured that evaluation was performed
on spectral signatures from previously unseen clusters, preventing spatial or spectral
leakage between partitions.

\medskip
\noindent
For multi-class settings with explicit background/distractor classes, a domain-aware
variant was used: LOGO folds were defined over grape-containing acquisitions, while
non-grape background samples were assigned to a fixed grouped holdout split (80/20) to
prevent leakage across semantically distinct domains.

\medskip
\noindent
Across all reported protocols, leakage prevention was enforced by design: the
wavelength restriction (450--925\,nm) is fixed globally, SNV is computed
independently per spectrum, and any data-dependent operation that requires
fitting parameters or selecting features is carried out using only the training
partition of the relevant protocol (or Row~1 for whole-image calibration) and
then applied to held-out groups (or Row~2) without adaptation.

\subsection{Spectral Discriminability and Informative Wavelength Analysis}
\label{subsec:spectral_discriminability}

\noindent
Prior to model development, a wavelength-wise separability assessment was conducted
to verify that cracked and healthy grape tissues exhibit distinguishable spectral
signatures and to localize the spectral regions contributing to class contrast across
the VIS--NIR range.

\medskip
\noindent
Separability was quantified directly from the pixel-level spectra using three
model-agnostic metrics computed at each wavelength: absolute mean difference
$|\Delta\mu|$, Cohen's $d$ effect size, and a Fisher-style discriminant ratio. These
measures capture both effect magnitude and variance-normalized class separation.

\medskip
\noindent
The analysis was used solely to characterize the spectral distribution of
cracked--healthy contrast and to support interpretation of the spectral-signature
trends reported in the Results. No wavelength subset was selected based on these
metrics.

\subsection{Pixel-Level Classification Framework}
\label{subsec:pixel_level_framework}

\noindent
\textit{Can cracked and healthy grape tissues be spectrally distinguished at the individual pixel level?}

\medskip
\noindent
Pixel-level classification was performed to evaluate whether spectral signatures of individual pixels could discriminate cracked grape tissue from healthy tissue and background materials. Each pixel was treated as an independent observation represented by its SNV-normalized reflectance spectrum.

\subsubsection{Problem Formulation and Class Definitions}

\noindent
Two classification settings were evaluated:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Binary classification:} Pixels were assigned to one of two classes---\textit{Healthy} (negative class, $y{=}0$) or \textit{Cracked} (positive class, $y{=}1$)---based on visual assessment of the corresponding berry tissue during manual annotation.
    
    \item \textbf{Multi-class classification:} The binary formulation was extended to include explicit background classes representing non-berry materials commonly present in vineyard scenes (leaves, branches, soil, and artificial supports). Three multi-class evaluation modes were investigated:
    \begin{itemize}[leftmargin=1.5em,nosep]
        \item \textit{CRACK VS REST}: Binary formulation treating \textit{Cracked} as positive.
        \item \textit{3-CLASSES}: Grouping samples into \textit{Healthy}, \textit{Cracked}, and \textit{not-grape}.
        \item \textit{FULL MULTI-CLASS}: Preserving all 10 original class labels.
    \end{itemize}
\end{itemize}

\subsubsection{Feature Representation and Model Training}

\noindent
Each pixel was represented by its SNV-normalized reflectance vector across the 450--925\,nm spectral range (159 bands at $\sim$3\,nm spacing). No dimensionality reduction or feature selection was applied prior to classification; the full spectral signature was used as input to enable assessment of intrinsic separability without imposed constraints.

\medskip
\noindent
The following supervised learning algorithms were evaluated:
\begin{itemize}[leftmargin=1.5em,nosep]
    \item Partial Least Squares Discriminant Analysis (PLS-DA)
    \item Logistic Regression with L1 regularization
    \item Support Vector Machine with RBF kernel (SVM-RBF)
    \item Random Forest
    \item Gradient-boosted decision trees (XGBoost, used as baseline)
    \item Shallow multi-layer perceptron (MLP)
\end{itemize}
XGBoost was adopted as the baseline classifier for all primary comparisons due to its robustness to high-dimensional correlated features.

\medskip
\noindent
Both balanced and unbalanced training regimes were investigated. Under the unbalanced regime, the natural class distribution was preserved. Under the balanced regime, all classes were undersampled to match the minority class count. For binary classification, majority-class samples were randomly undersampled. For multi-class settings, segment-proportional undersampling was applied: each class was capped at the \textit{Cracked} class count, with samples drawn proportionally from each segment (mask region) to preserve within-class diversity. Models were trained using default or lightly tuned hyperparameters; no extensive grid search was performed.

\subsubsection{Cross-Validation Protocol}

\noindent
Generalization was evaluated using the Leave-One-Group-Out (LOGO) protocol at the grape-cluster acquisition level, as described in \S\ref{subsec:cv_strategy}. For the multi-class formulation, the domain-aware variant was applied.

\subsubsection{Evaluation Metrics}

\noindent
Classification performance was assessed using the metrics and reporting conventions detailed in \S\ref{subsubsec:metrics_rationale}. The primary ranking metric was \textbf{PR--AUC} for the \textit{Cracked} class; supplementary metrics included Accuracy, Balanced Accuracy, Precision, Recall, F1, and ROC--AUC.


\subsection{Anomaly Detection Framework}
\label{subsec:anomaly_detection}

\noindent
\textit{Can crack damage be detected as a spectral anomaly without explicit class labels?}

\subsubsection{One-Class Formulation}

\noindent
An anomaly detection framework was implemented at the pixel level using a one-class learning paradigm. Models were trained exclusively on spectral signatures designated as \textit{normal}, and unseen samples were flagged as anomalous based on deviation from this learned distribution.

\subsubsection{Definition of Normality per Variant}

\noindent
Three experimental configurations were evaluated, each defining normality differently:

\begin{itemize}[leftmargin=1.5em,nosep]
    \item \textbf{Binary one-class:} Normal class comprised Healthy/Regular pixels; anomalies were Cracked pixels.
    \item \textbf{Multi-class (all except Cracked):} Normal class comprised all nine non-\textit{Cracked} classes; anomalies were \textit{Cracked} pixels.
    \item \textbf{Multi-class (Cracked only):} Normal class comprised \textit{Cracked} pixels exclusively; anomalies were all other classes.
\end{itemize}

\subsubsection{Data Partitioning and Leakage Control}

\noindent
Cross-validation strategies were adapted to each variant's data structure:

\begin{itemize}[leftmargin=1.5em,nosep]
    \item \textbf{Binary one-class:} Leave-One-Group-Out (LOGO) at the grape-cluster acquisition level (\S\ref{subsec:cv_strategy}).
    \item \textbf{Multi-class (all except Cracked):} Single grouped train/test split (80/20) stratified by acquisition.
    \item \textbf{Multi-class (Cracked only):} Domain-aware cross-validation combining LOGO over \textit{Cracked}-containing acquisitions with a fixed grouped 80/20 split for non-\textit{Cracked} samples to prevent segment leakage.
\end{itemize}

\noindent
For binary one-class IsolationForest (contamination $=0.05$) was applied to remove outliers from the training normal set prior to autoencoder fitting.

\subsubsection{Spectral Preprocessing}

\noindent
Pixels were preprocessed as described in \S\ref{subsec:global_preprocessing} (SNV normalization, 450--925\,nm range, 159 bands).

\subsubsection{Autoencoder Architecture and Training}

\noindent
All anomaly detection experiments employed a fully connected autoencoder with symmetric encoder--decoder structure. The encoder applied ReLU activations, BatchNorm, and dropout ($p=0.2$) in the first two layers. Several bottleneck sizes were evaluated.

\medskip
\noindent
Let $x \in \mathbb{R}^D$ denote a preprocessed spectrum and $\hat{x}$ its reconstruction. Training minimized $\mathrm{MSE}(x,\hat{x})$, with hyperparameters listed in Table~\ref{tab:autoencoder_training}.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{} l l @{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & Adam \\
Learning rate & $10^{-3}$ \\
Weight decay & $10^{-5}$ \\
LR scheduler & ReduceLROnPlateau (patience=5, factor=0.5) \\
Early stopping & patience=15 \\
Max epochs & 100 \\
Batch size & 256 \\
\bottomrule
\end{tabular}
\caption{Autoencoder training hyperparameters.}
\label{tab:autoencoder_training}
\end{table}

\subsubsection{Anomaly Scoring and Thresholding}

\noindent
The anomaly score was defined as the per-sample reconstruction mean squared error:
\begin{equation}
    e(x) = \frac{1}{D}\,\lVert x-\hat{x} \rVert_2^2.
\end{equation}

\noindent
For each fold or split, an operating threshold $\tau$ was computed as the $99^{\mathrm{th}}$ percentile of reconstruction errors on the training partition of the normal class. A sample was classified as anomalous if $e(x) > \tau$.

\subsubsection{Integration into Whole-Image Inference}

\noindent
For full-image inference, the multi-class autoencoder variant trained on CRACK samples only was selected and exported. This model was integrated into the full-image inference pipeline, transforming reconstruction errors into probability-like crack scores via sigmoid mapping:
\begin{equation}
    p_{\mathrm{CRACK}}(x) = \frac{1}{1 + \exp\left(\frac{e(x) - \tau}{\alpha}\right)},
\label{eq:ae_score_mapping}
\end{equation}
where $e(x)$ is the reconstruction error for spectrum $x$, $\tau$ is the training threshold (99th percentile of CRACK class errors), and $\alpha$ is a temperature parameter ($\approx 0.3\tau$) controlling transition sharpness. This mapping produces a smooth $S$-curve: low reconstruction errors (CRACK-like) map to high probability, while high errors (anomalous) map to low probability. Per-pixel scores were subsequently processed through standard post-processing and aggregation stages.


% \noindent
% The temperature parameter $\alpha$ controls the sharpness of the sigmoid transition
% around the reconstruction threshold $\tau$. Smaller values of $\alpha$ yield a steeper,
% near-binary transition, whereas larger values produce a smoother mapping.
% In this study, $\alpha$ was set to $0.3\tau$, providing a transition scale proportional
% to the dispersion of the training CRACK reconstruction errors. This parameter was
% defined heuristically.

\medskip
\noindent

\subsection{Whole-Image Detection and Aggregation Pipeline}
\label{subsec:full_image_pipeline}

\noindent
\textit{How can spatial context stabilize noisy pixel-level predictions into reliable image-level decisions?}

\medskip
\noindent
Despite strong pixel-level discrimination under controlled labeling conditions, direct application of per-pixel classifiers to unsegmented field images produced spatially scattered predictions: background materials, illumination gradients, and mixed-pixel effects introduced isolated false positives that could not be resolved at the single-pixel level. These limitations motivated a shift toward spatial aggregation at the image level.

\medskip
\noindent
Building upon the pixel-level classification outputs, a whole-image detection and
localization pipeline was developed to enable robust crack identification in realistic
vineyard scenes. Rather than introducing an additional image-level classifier, the
pipeline aggregates pixel-level classification outputs into a system-level framework that enforces
spatial coherence and suppresses isolated false positive responses.

\medskip
\noindent
The pipeline operates on pixel-wise crack probability maps generated by the trained
pixel-level classifiers and progressively aggregates them into region-level, patch-level,
and image-level decisions, bridging the gap between noisy pixel-level outputs and
biologically meaningful image-level interpretation.

\medskip
\noindent
All pixel-level models---including the base classifiers, autoencoder variants, and wavelength-reduced model variants (\S\ref{subsec:dimensionality_reduction})---were fixed at their trained states. No retraining, tuning, or optimization of these models was performed at the whole-image level.

\subsubsection{Pipeline Architecture and Spatial Aggregation}

\noindent
The whole-image aggregation pipeline follows a sequential process, illustrated in
Figure~\ref{fig:whole_image_pipeline}, and consists of the following stages:

\begin{itemize}
    \item \textbf{Input Hyperspectral Image:}
    Each input consists of a full hyperspectral datacube ($H \times W \times B$) acquired
    under field conditions.

    \item \textbf{Pixel-Level Probability Map Generation:}
    The trained pixel-level classifier produces a dense pixel-wise crack probability map
    with values in the range $[0,1]$.

    \item \textbf{Morphological Closing and Region Formation:}
    Spatially adjacent pixels with elevated crack probabilities are grouped into connected
    regions (BLOBs) using morphological closing operations, enforcing spatial coherence.

    \item \textbf{Blob Size Filtering:}
    Small connected regions below a minimum size threshold are removed to suppress isolated
    false positive detections.

    \item \textbf{Patch-Based Division and Decision:}
    The image is divided into fixed-size patches. A patch is marked as crack-positive if
    the proportion of crack-related pixels within the patch exceeds a predefined threshold.

    \item \textbf{Image-Level Decision:}
    The final image-level decision is obtained by aggregating patch-level outcomes. An
    image is classified as \textit{Cracked} if at least one patch is identified as
    crack-positive; otherwise, it is classified as \textit{Non-Cracked}.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/materials_methods/pipeline/whole_image_pipeline.jpg}
    \caption{Whole-image crack-detection pipeline applied to field-acquired cluster scenes (early and late settings) with row-partitioned evaluation (Row~1 calibration, Row~2 test). A pixel-level classifier produces a crack probability map ($[0,1]$), which is spatially aggregated via morphological closing and blob-size filtering; patch rules are then combined into a final image-level decision (Cracked vs.\\ Non-Cracked). Takeaway: spatial aggregation turns noisy pixel scores into a deployable cluster-level decision rule.}
    \label{fig:whole_image_pipeline}
\end{figure}

\subsubsection{Whole-Image Dataset and Evaluation Setup}

\noindent
Whole-image evaluation was conducted on the row-partitioned inference dataset described
in Section~\ref{subsec:whole_image_dataset}. The aggregation pipeline was applied to
patch-based representations extracted from the full images, enabling localized detection
while preserving image-level context. Row~1 was used exclusively for calibration/tuning
of whole-image inference parameters, while Row~2 was reserved as a held-out test set for
final reporting. Exact sample counts and class balance (early/late) are reported in
Table~\ref{tab:dataset_statistics}.

\subsubsection{Evaluation Across Disease Progression Stages}
\label{subsec:early_late_evaluation}

\noindent
To assess robustness across different stages of crack development, whole-image performance
was evaluated separately for early-stage and late-stage samples. Late-stage images
contained clusters with clearly visible macroscopic cracking, whereas early-stage images
corresponded to initial onset (absent-to-minimal/incipient cracking) based on expert monitoring records and visual inspection.
This stratification was applied exclusively at the evaluation stage and did not affect
model training or aggregation design.

\subsection{Dimensionality Reduction Strategy}
\label{subsec:dimensionality_reduction}

\noindent
\textit{How many wavelengths are needed to maintain crack-detection performance?}

\medskip
\noindent
With the full detection pipeline established---from pixel-level classification through spatial aggregation---the final methodological question was whether the complete 159-band spectral representation was necessary, or whether a reduced wavelength set could preserve detection performance while simplifying future sensor requirements.

\medskip
\noindent
Wavelength selection was evaluated separately for binary (\textit{Cracked} vs.\ \textit{Healthy}) and multi-class (with background classes) settings, as formulated in \S\ref{subsec:pixel_level_framework}.

\subsubsection{Spectral Preprocessing}

\noindent
Pixels were preprocessed as described in \S\ref{subsec:global_preprocessing} (SNV normalization, 450--925\,nm range, 159 bands).

\subsubsection{Classification Model}

\noindent
XGBoost was used as the fixed classifier for all wavelength selection experiments, selected as the best-performing model from the preceding pixel-level classification work (\S\ref{subsec:pixel_level_framework}).

\subsubsection{Two Experimental Settings}

\noindent
Wavelength selection was performed in two independent experiments:
\begin{itemize}[leftmargin=1.5em,nosep]
    \item \textbf{Binary setting:} Wavelengths selected for \textit{Cracked} vs.\ \textit{Healthy} classification.
    \item \textbf{Multi-class setting:} Wavelengths selected for multi-class classification with background classes.
\end{itemize}

\subsubsection{Train-Test Split Protocol}

\noindent
For each experimental setting, a single grouped 80/20 held-out split was constructed with group constraints to prevent leakage. For binary classification, the grouping was by grape cluster identity (\textit{CRACK} and \textit{REGULAR} samples). For multi-class classification, grouping was by cluster/segment identity for background classes. The 80\% training subset was used to re-train XGBoost at each BFS step, while the held-out 20\% test subset served as the invariant selection criterion throughout the elimination path.

\subsubsection{Backward Feature Selection (BFS)}

\noindent
A deterministic wrapper method was applied, starting from the full set of 159 wavelengths and iteratively removing the least informative band at each step. At each removal step, the classifier was re-trained on the 80\% training subset and evaluated on the held-out 20\% test subset. The selection metric was PR--AUC for the \textit{Cracked} class. BFS provides a monotonic elimination path, identifying the minimal wavelength subset preserving near-baseline detection performance.

\medskip
\noindent
\textbf{Takeaway.} BFS identified minimal wavelength subsets preserving near-baseline PR--AUC, informing practical multispectral sensor design for field deployment.

% Implementation reference (grouped 80/20 BFS split + leakage assertions):
% src/models/classification/pixel_level/feature_selection/run_bfs_80_20_grapeid.py
% src/models/classification/pixel_level/feature_selection/run_bfs_prauc_stability_5runs.py


\subsection{Evaluation Protocols and Performance Metrics}
\label{subsec:evaluation_protocols}

\noindent
Model performance was evaluated using task-specific protocols and metrics tailored
to each stage of the analysis, reflecting the differing objectives and operational
requirements of pixel-level classification, anomaly detection, and whole-image
detection.

\medskip
\noindent
Evaluation metrics were selected according to the objective of each experimental
stage. Pixel-level and anomaly detection metrics are summarized in
Table~\ref{tab:pixel_eval_metrics}, while system-level evaluation for whole-image
detection is reported in Table~\ref{tab:whole_image_eval_metrics}.

\subsubsection{Metrics Rationale}
\label{subsubsec:metrics_rationale}

\noindent
This study spans multiple tasks with different operating assumptions (pixel-level detection under strong class imbalance, system-level whole-image decisions after spatial aggregation, and wavelength-selection optimization). Accordingly, a task-dependent \emph{primary} metric is defined for each setting, while additional metrics are reported to describe operating-point behavior and error trade-offs.

\medskip
\noindent
\textbf{Pixel-level detection (binary and multi-class).}
The pixel-level objective is to detect cracked tissue, which is typically the minority class. Therefore, the primary ranking metric is \textbf{PR--AUC for the \textit{Cracked} class} (computed with \textit{Cracked} as the positive class; one-vs-rest in multi-class settings). PR--AUC (average precision) is preferred under class imbalance because it focuses on precision--recall trade-offs and its baseline corresponds to the positive-class prevalence, making it more informative than ROC--AUC when negatives dominate. Threshold-dependent metrics (Precision/Recall and, where reported, F1) are included as complementary operating-point summaries.
Unless explicitly subscripted (e.g., F1\textsubscript{CRACK}), reported \textbf{F1 refers to the weighted F1-score} (support-weighted over classes), matching the evaluation scripts used in the experiments.

\medskip
\noindent
\textbf{Whole-image crack classification (spatial aggregation).}
At the image level, performance is reported with explicit emphasis on the \textit{Cracked} class. The \emph{primary} summary metric is \textbf{F1\textsubscript{CRACK}}, supported by \textbf{Precision\textsubscript{CRACK}} and \textbf{Recall\textsubscript{CRACK}} to expose the false-positive/false-negative trade-off at the chosen operating point. \textbf{MCC} is reported as a complementary single-number summary that remains informative under class imbalance. Image-level accuracy is included for completeness, but it is not treated as the main indicator of crack-detection quality. \textbf{F2\textsubscript{CRACK}} is additionally reported when a recall-oriented operating point is of interest.

\medskip
\noindent
\textbf{Backward Feature Selection (BFS) optimization.}
For BFS, the objective metric is \textbf{CRACK PR--AUC} evaluated on a fixed grouped 80/20 hold-out split (Section~\ref{subsec:dimensionality_reduction}). CRACK F1 is additionally plotted to show how feature elimination affects threshold-dependent performance, but selection is driven by the PR--AUC criterion.

\medskip
\noindent
\textbf{Reporting and threshold conventions.}
For cross-validation experiments (pixel-level classification and LOGO anomaly detection), metrics are computed per held-out fold and summarized as mean $\pm$ standard deviation across folds. ROC--AUC and PR--AUC are computed from continuous scores/probabilities and do not require selecting a decision threshold.
Metrics that require binary decisions (Precision/Recall/F1/F2/MCC) use the model's hard class labels under its standard decision rule; for the PLS-DA wrapper this corresponds to thresholding the continuous score at $t=0.5$. For one-class autoencoder experiments, the anomaly threshold is estimated on the training partition of each fold using a percentile of training reconstruction errors and then applied unchanged to the held-out fold. For whole-image experiments, post-processing parameters (including probability and aggregation thresholds) are calibrated on Row~1 only and then fixed for reporting on the held-out Row~2 test set.



\subsubsection{Cross-Validation Strategy}

\noindent
Where cross-validation was used, the LOGO protocol described in \S\ref{subsec:cv_strategy} was applied.

\subsubsection{Evaluation Units and Split Strategy}
\label{subsubsec:eval_units_splits}

\noindent
Table~\ref{tab:evaluation_units_splits} provides a compact summary of the unit of
analysis and the exact independence assumptions used across all reported
experiments.

% Implementation references for split logic (do not edit without updating this table):
% Binary pixel-level LOGO (group = cluster_id extracted from hs_dir):
%   src/preprocessing/spectral_preprocessing.py (extract_cluster_id, preprocess_pixel_level_dataset)
%   src/models/classification/pixel_level/simple_classification_leave_one_out/comare_all_models_binary_13_01/compare_models.py
% Domain-aware multi-class LOGO (LOGO on grape hs_dir + fixed non-grape holdout):
%   src/models/classification/pixel_level/simple_classification_leave_one_out/comare_all_models_multi_class_16_01/compare_models_multi_class.py
% CRACK-focused domain-aware CV (LOGO on CRACK hs_dir + fixed REGULAR/non-grape holdouts):
%   src/models/classification/pixel_level/simple_classification_leave_one_out/comare_all_models_multi_class_16_01/unified_experiment_pipeline_acc.py
% Autoencoder (LOGO):
%   src/models/classification/pixel_level/simple_classification_leave_one_out/autoencoder_multiclass_30_01/autoencoder_anomaly_detection.py
% Autoencoder (CRACK-only training, domain-aware):
%   src/models/classification/pixel_level/simple_classification_leave_one_out/autoencoder_multiclass_30_01/autoencoder_train_crack_only.py
% Autoencoder (single group-wise train/test split; no LOGO):
%   src/models/classification/pixel_level/simple_classification_leave_one_out/autoencoder_multiclass_30_01/autoencoder_detect_crack_multiclass.py

\begin{table}[!htbp]
    \centering
    \caption{Evaluation units and split strategy used across experiments. ``Independent test'' denotes the data that remain unseen during fitting/tuning for the corresponding protocol.}
    \label{tab:evaluation_units_splits}
    \small
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{p{4.2cm} p{2.0cm} p{7.2cm} p{3.2cm}}
        \toprule
        Experiment / task & Unit & Split strategy (group definition) & Independent test \\
        \midrule
        Binary pixel-level classification (2 classes) & Pixel spectrum & LOGO cross-validation with group $=$ \texttt{cluster\_id} extracted from \texttt{hs\_dir} (three levels up in the path). All pixels from a held-out cluster are evaluated together. & Held-out LOGO fold (one cluster group) \\

        Multi-class pixel-level classification (domain-aware LOGO) & Pixel spectrum & LOGO applied to grape samples with group $=$ \texttt{hs\_dir} (image-level identifier), combined with a fixed 80/20 grouped hold-out split for non-grape/background samples (by \texttt{hs\_dir}, with fallback to \texttt{mask\_path} if required). Each fold tests one held-out grape image plus the fixed non-grape hold-out set. & Per fold: held-out grape \texttt{hs\_dir} $+$ fixed non-grape hold-out \\

        CRACK-focused domain-aware CV (unified pipeline) & Pixel spectrum & LOGO applied to CRACK samples only with group $=$ CRACK \texttt{hs\_dir}; REGULAR samples use a fixed 80/20 grouped split (by \texttt{mask\_path}); non-grape samples use a fixed 80/20 grouped split. The same fixed REGULAR/non-grape hold-out portions are reused across folds. & Per fold: held-out CRACK \texttt{hs\_dir} $+$ fixed REGULAR/non-grape hold-out \\

        Autoencoder anomaly detection (one-class; LOGO) & Pixel spectrum & LOGO cross-validation with group $=$ \texttt{cluster\_id} extracted from \texttt{hs\_dir}. Scaling and threshold selection are computed from training partitions only. & Held-out LOGO fold (one cluster group) \\

        Autoencoder anomaly detection (train on all except CRACK; single split) & Pixel spectrum & Single group-wise train/test split without LOGO: unique groups (\texttt{cluster\_id} extracted from \texttt{hs\_dir}) are split 80/20 into train/test groups. The autoencoder is trained on non-CRACK samples from training groups; evaluation is reported on held-out test groups. & Held-out 20\% of groups (cluster\_id) \\

        Whole-image crack detection (spatial aggregation) & Patch; image (cluster acquisition) & Row-partitioned evaluation: Row~1 CSVs are used for calibration/tuning of post-processing parameters; Row~2 CSVs are held out for final testing. Early- and late-stage scenarios are evaluated separately. & Held-out Row~2 (early and late, separately) \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\subsubsection{Pixel-Level Classification Metrics}

\noindent
For binary pixel-level classification, performance was evaluated using both ranking-based
metrics (ROC--AUC and PR--AUC with \textit{Cracked} as the positive class) and operating-point
metrics (Precision/Recall and F1). Under class imbalance, PR--AUC\textsubscript{CRACK} is treated
as the primary summary of separability, while Precision/Recall\textsubscript{CRACK} describe the
trade-off at the operating point used by the classifier.

\medskip
\noindent
Additional metrics, including precision, recall, and ROC-AUC, were reported as
supporting indicators to facilitate comprehensive performance analysis.

\medskip
\noindent
For multi-class pixel-level classification, Macro-F1 and per-class F1-scores were
used as primary metrics to ensure balanced evaluation across all classes, with
particular attention to the \textit{Cracked} class. Confusion matrices were used
to analyze class-specific error patterns and background-related confusions.

\subsubsection{Anomaly Detection Evaluation}

\noindent
For anomaly detection experiments, model performance was evaluated using ROC-AUC
and PR-AUC, reflecting the ranking-based nature of one-class detection and the
absence of a fixed decision threshold. These metrics quantify the ability of the
model to distinguish anomalous (cracked) spectra from normal (healthy) spectral
behavior across a range of operating points.

\subsubsection{Whole-Image and Patch-Based Evaluation}

\noindent
Whole-image detection performance was evaluated at the patch and image levels,
reflecting the system-level objective of identifying cracked grape clusters under
realistic field conditions.

Given the higher cost associated with missed detections, recall-oriented metrics
were emphasized. In particular, Recall and the F2-score for the \textit{Cracked}
class (F2\textsubscript{CRACK}) were used as primary evaluation criteria, assigning
greater weight to recall than precision.

Precision and false positive rates were reported as complementary metrics to
characterize trade-offs and system behavior. Quantitative evaluation was supported
by qualitative visual inspection of detection overlays to analyze spatial
consistency, typical failure modes, and false positive patterns.


\begin{table}[htbp]
\centering
\caption{Evaluation metrics for pixel-level and anomaly detection experiments}
\label{tab:pixel_eval_metrics}
\begin{tabular}{p{3.5cm} p{5.2cm} p{4.5cm}}
\hline
\textbf{Task} & \textbf{Primary Metrics} & \textbf{Secondary Metrics} \\
\hline

Binary pixel-level classification &
PR--AUC\textsubscript{CRACK}, Recall\textsubscript{CRACK} &
Precision\textsubscript{CRACK}, F1 (weighted unless specified), ROC--AUC \\

\hline

Multi-class pixel-level classification &
Macro-F1, F1\textsubscript{CRACK} &
Per-class Precision/Recall, Confusion Matrix \\

\hline

Anomaly detection (one-class) &
ROC-AUC, PR-AUC &
Recall@Fixed-FPR, Precision@Top-K \\

\hline
\end{tabular}
\end{table}

\subsection{Reproducibility and Implementation Details}
\label{subsec:reproducibility}

\noindent
This section summarizes the concrete implementation and execution details needed to rerun the experiments and reproduce the reported results. Where the project does not explicitly record a detail (e.g., exact CPU/GPU model), it is marked as \TODO{...} rather than inferred.

\subsubsection{Software Environment}

\noindent
All experiments were executed in a dedicated Python virtual environment located at \texttt{.venv}. The environment was created with \textbf{Python 3.11.9} (as recorded in \texttt{.venv/pyvenv.cfg}). Dependency versions were pinned in \texttt{requirements.txt}; core libraries used throughout the pipelines include:\newline
\texttt{numpy==2.2.3}, \texttt{pandas==2.2.3}, \texttt{scikit-learn==1.6.1}, \texttt{xgboost==2.1.4}, and \texttt{torch==2.5.1+cu121}.

\medskip
\noindent
The codebase is developed and executed on Windows (paths and helper scripts use Windows conventions). \TODO{Record exact Windows version/build used for final runs (e.g., from system info) and archive it alongside experiment outputs.}

\subsubsection{Randomness and Seed Strategy}

\noindent
Randomness is controlled explicitly in the main training and evaluation scripts. A project-wide default seed of \textbf{42} is used in multiple pipelines.
For the one-class autoencoder experiments, the scripts define a \texttt{set\_seed()} helper that sets seeds for \texttt{random}, \texttt{numpy}, and \texttt{torch} (and, when CUDA is available, \texttt{torch.cuda.manual\_seed\_all}), and enforces deterministic cuDNN behavior by setting \texttt{torch.backends.cudnn.deterministic=True} and \texttt{torch.backends.cudnn.benchmark=False}. In cross-validation settings, some runs additionally use fold-index offsets (e.g., \texttt{seed = 42 + fold\_idx}) to ensure controlled but distinct fold-specific randomness.

\medskip
\noindent
For dataset construction steps that include sampling (e.g., selecting a bounded number of negative samples for full-image datasets), the seed is passed as an explicit \texttt{random\_seed} argument with default \texttt{42}.

\subsubsection{Outputs, Artifacts, and Result Storage}

\noindent
Experiment artifacts are written to the top-level \texttt{experiments/} directory using timestamped subfolders.
For example, the unified pixel-level comparison pipeline creates an \texttt{experiments/unified\_experiment\_.../} folder and stores the exact run configuration in \texttt{experiment\_config.json}, cross-validation manifests (\texttt{cv\_split\_manifest\_*.json}), and consolidated reports exported to \texttt{.xlsx/.csv/.md}.
Similarly, autoencoder experiments store fold-level and summary outputs (e.g., \texttt{all\_fold\_results.csv} and \texttt{summary\_results.csv}) under timestamped \texttt{experiments/autoencoder\_.../} directories.

\medskip
\noindent
\TODO{For long-term reproducibility, archive each experiment folder together with the exact input dataset CSV(s) used, or their immutable hashes/versions.}

\subsubsection{How to Rerun Core Pipelines}

\noindent
Two helper scripts are provided to ensure the correct environment is used:
\begin{itemize}
    \item \texttt{activate\_venv.bat} activates \texttt{.venv} and opens a new \texttt{cmd} session.
    \item \texttt{run\_with\_venv.bat} runs a given Python command using \texttt{.venv\\Scripts\\python.exe}.
\end{itemize}

\noindent
Representative entry points used in this thesis include:
\begin{itemize}
    \item Unified pixel-level experiment pipeline (exports consolidated reports):\\
    \url{src/models/classification/pixel\_level/simple\_classification\_leave\_one\_out/comare\_all\_models\_multi\_class\_16\_01/unified\_experiment\_pipeline\_acc.py}.
    \item One-class autoencoder anomaly detection (LOGO and thresholding per fold):\\
    \url{src/models/classification/pixel\_level/simple\_classification\_leave\_one\_out/autoencoder\_multiclass\_30\_01/autoencoder\_anomaly\_detection.py}.
    \item Full-image dataset builder (creates Row~1/Row~2 CSVs; random seed controls sampling):\\
    \url{src/datasets/full\_image/builders/build\_full\_image\_datasets.py}.
\end{itemize}

\noindent
The full-image inference application is a self-contained subproject under
\url{src/models/classification/full\_image/inference\_to\_see\_results\_of\_models/} and includes a reproducibility-focused smoke test (\texttt{scripts/smoke\_test.py}) that reports the configured \texttt{RANDOM\_SEED} and device.

\medskip
\noindent
\TODO{Record the exact hardware used for final model training/inference (CPU model, RAM, GPU model/VRAM). For GPU runs, capture the device name printed by the scripts (e.g., \texttt{torch.cuda.get\_device\_name(0)}) and store it in the corresponding experiment folder.}

\begin{table}[htbp]
\centering
\caption{Evaluation metrics for whole-image and patch-based detection}
\label{tab:whole_image_eval_metrics}
\begin{tabular}{p{5.5cm} p{4.5cm} p{4.5cm}}
\hline
\textbf{Task} & \textbf{Primary Metrics} & \textbf{Secondary Metrics} \\
\hline

System-level detection of cracked grape clusters &
F1\textsubscript{CRACK}, Precision\textsubscript{CRACK}, Recall\textsubscript{CRACK} &
MCC, Image-level Accuracy, F2\textsubscript{CRACK}, False Positive Rate \\

\hline
\end{tabular}
\end{table}


% Modeling and Evaluation Methodology
% ├─ Global Spectral Preprocessing
% ├─ Spectral Discriminability and Informative Wavelength Analysis
% │  └─ Feature selection as a derived outcome
% ├─ Pixel-Level Classification Framework
% │  ├─ Binary Pixel-Level Classification
% │  └─ Multi-Class Pixel-Level Classification
% ├─ Anomaly Detection Framework (One-Class)
% ├─ Dimensionality Reduction Strategy
% │  ├─ For binary pixel-level models
% │  └─ For multi-class pixel-level models
% ├─ Whole-Image Detection and Aggregation Pipeline
% ├─ Evaluation Protocols
% │  ├─ Leave-One-Group-Out (LOGO) cross-validation
% │  └─ Performance Metrics

