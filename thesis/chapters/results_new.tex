\chapter{Experimental Results}
\label{ch:results}

\section{Overview of Experimental Results}

\noindent
This chapter presents the quantitative results obtained from the experimental evaluation of hyperspectral-based crack detection and early-warning screening methods. The results are organized to follow the logical progression of the experimental pipeline, beginning with spectral characterization and continuing through pixel-level then to whole-image modeling, followed by wavelength-selection analyses.



\subsection{Baselines}
\label{subsec:results_baselines}

\noindent
To support consistent comparisons across tasks, each set of results in this chapter is reported relative to an explicit baseline configuration:

\begin{itemize}
    \item \textbf{Pixel-level classification baseline.} The baseline classifier is \textbf{XGBoost} trained on the \textbf{full hyperspectral signature} (450--925\,nm) after \textbf{SNV normalization}. Baseline performance for the binary task is reported in the XGBoost rows of Table~\ref{tab:pixel_metrics_unbalanced} and Table~\ref{tab:pixel_metrics_balanced}. For the extended labeling/balancing regimes, the corresponding XGBoost baselines appear in Table~\ref{tab:pixel_metrics_unbalanced_all} and Table~\ref{tab:pixel_metrics_balanced_all}.

    \item \textbf{Whole-image crack-detection baseline.} The baseline whole-image system uses \textbf{pixel-level model outputs} as inputs to the same \textbf{spatial post-processing and aggregation pipeline} (thresholding, morphological filtering, patch-level aggregation, and image-level decision logic), \textbf{without any wavelength selection}. Its early/late calibration and held-out test performance is summarized in Table~\ref{tab:full_image_early_late_metrics} (with dataset composition in Table~\ref{tab:full_image_dataset_composition}).

    \item \textbf{BFS wavelength-selection reference.} BFS results are reported as \textbf{performance vs. number of selected wavelengths} relative to the \textbf{full-spectrum baseline} (159 wavelengths within 450--925\,nm). The performance trajectories are shown in Figure~\ref{fig:bfs_dual_metric} and Figure~\ref{fig:bfs_threshold_markers}, and the derived wavelength-count thresholds are summarized in Table~\ref{tab:bfs_thresholds_summary}. Whole-image test-set performance at representative wavelength counts (including the full-spectrum baseline at 159 wavelengths) is reported in Table~\ref{tab:full_image_wavelength_results}.
\end{itemize}

\section{Spectral Separability and Informative Wavelengths}
\label{sec:spectral-characterization}

\noindent
This section examines the spectral properties of grape tissue to establish whether measurable differences exist between healthy and cracked samples prior to any modeling stage.

\subsection{Mean Spectral Signatures of Healthy and Cracked Tissue}

\noindent
Mean spectral signatures were computed for pixels sampled from healthy and cracked grape tissue across the full VIS--NIR range. 
The analysis focuses on class-level spectral trends and variability, without fitting any classification model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/spectral_signatures_mean_std.png}
    \caption{Mean spectral signatures ($\pm$ standard deviation) for healthy and cracked tissue across the VIS--NIR range.}
    \label{fig:mean_signatures}
\end{figure}

\FloatBarrier

\noindent
Cracked tissue shows lower reflectance than healthy tissue across the full VIS--NIR range, with the divergence most pronounced beyond 700\,nm.

\noindent
To provide a compact quantitative summary, reflectance values were aggregated over two broad spectral regions corresponding to the visible (VIS) and near-infrared (NIR) ranges.

\FloatBarrier

\begin{table}[htbp]
    \centering
    \caption{Summary statistics of reflectance values across spectral regions for healthy and cracked tissue.}
    \label{tab:reflectance_stats}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Spectral Region & Class & Mean Reflectance & Std. Dev. & Median \\
        \midrule
        VIS (400--700\,nm) & Healthy & 0.218 & 0.128 & 0.186 \\
                          & Cracked & 0.137 & 0.076 & 0.123 \\
        NIR (700--1000\,nm) & Healthy & 0.660 & 0.114 & 0.630 \\
                           & Cracked & 0.450 & 0.121 & 0.425 \\
        \bottomrule
    \end{tabular}
\end{table}

\FloatBarrier

\noindent
Healthy tissue reflectance exceeds cracked tissue by 32\% in the NIR (0.660 vs.\ 0.450) and 37\% in the VIS (0.218 vs.\ 0.137). Cracked tissue also shows lower VIS standard deviation (0.076 vs.\ 0.128), indicating a more homogeneous spectral response.

\newpage

\subsection{Spectral Separability Across Wavelength Regions}

\noindent
This subsection quantifies the degree of spectral separation between healthy and cracked tissue across predefined wavelength regions.
The goal is to identify spectral ranges exhibiting the strongest inter-class contrast, without invoking classification models.

\noindent
Two filter-based metrics are used. \textbf{Cohen's $d$}~\cite{cohen1988statistical} measures the standardised mean difference between classes, $d = |\mu_1 - \mu_2| / \sigma_{\mathrm{pooled}}$, thereby penalising high intra-class variance; by convention, $d \geq 0.8$ denotes a large effect and $d > 1.2$ a very large effect, indicating robust spectral discriminability~\cite{cohen1988statistical}. The \textbf{Fisher Score} (linear discriminant criterion) quantifies the ratio of inter-class variance to intra-class variance across each wavelength band, making it a standard criterion for discriminative band selection in hyperspectral analysis~\cite{duda2001pattern}; unlike the absolute mean difference, both metrics explicitly account for class spread, providing a more reliable indicator of learnable separation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/separability_metrics.png}
    \caption{Wavelength-wise spectral separability metrics between healthy and cracked tissue, including absolute mean difference, Cohen's $d$, and Fisher score.}
    \label{fig:spectral_separability}
\end{figure}

\noindent
To facilitate regional-level comparison, the wavelength-wise separability measures were aggregated over predefined spectral regions.

\begin{table}[htbp]
    \centering
    \caption{Regional spectral separability metrics between healthy and cracked tissue.
    Mean values are averaged across wavelengths within each range, while peak values
    indicate the maximum separability and the corresponding wavelength.}
    \label{tab:separability_scores}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Metric & VIS Range & NIR Range & Full Spectrum \\
        \midrule
        Mean $|\Delta \mu|$ (Abs. Mean Diff.) 
            & 0.0810 & 0.2101 & 0.1422 \\
        Max $|\Delta \mu|$ (nm) 
            & 0.2325 (708.6) & 0.3704 (726.5) & 0.3704 (726.5) \\
        \midrule
        Mean Cohen's $d$ 
            & 0.6330 & 1.6217 & 1.1014 \\
        Max Cohen's $d$ (nm) 
            & 1.1910 (699.5) & 2.6877 (756.6) & 2.6877 (756.6) \\
        \midrule
        Mean Fisher Score 
            & 0.2895 & 1.6590 & 0.9436 \\
        Max Fisher Score (nm) 
            & 0.8821 (694.5) & 3.6311 (747.5) & 3.6311 (747.5) \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/regional_comparison.png}
    \caption{Regional visual summary of mean reflectance ($\pm$1 STD) and absolute mean difference between healthy and cracked tissue in the VIS and NIR ranges. This figure provides an intuitive visualization of the regional trends quantified in Table~\ref{tab:separability_scores}.}
    \label{fig:regional_summary}
\end{figure}

\FloatBarrier

\medskip
\noindent
The NIR region yields substantially higher separability than the VIS range: mean Cohen's $d$ rises from 0.633 to 1.622, with peak values at 727--757\,nm ($d_{\max} = 2.688$; Fisher$_{\max} = 3.631$).

\medskip
\noindent
The following sections evaluate whether supervised and unsupervised models can exploit these spectral differences for crack detection.


\section{Pixel-Level Binary Classification}
\label{sec:pixel-level-classification}

\noindent
This section reports pixel-level classification performance for distinguishing healthy from cracked tissue using supervised models.
Evaluation uses the LOGO cross-validation protocol described in Section~\ref{subsec:evaluation_protocols}.

\graphicspath{{results/Binary_Pixel_Level_Classification/}}

\noindent
PR--AUC\textsubscript{CRACK} is the primary metric; ROC--AUC, Precision, and Recall for the \textit{Cracked} class are reported as secondary metrics.
Two sampling regimes were compared: the original imbalanced distribution and a class-balanced configuration.

\subsection{Unbalanced vs.\ Balanced Training Sets}

\noindent
Results obtained under both sampling regimes are summarized jointly on the following landscape page.
Table~\ref{tab:pixel_metrics_unbalanced} reports the classification performance obtained using the original unbalanced dataset,
while Table~\ref{tab:pixel_metrics_balanced} presents the corresponding results obtained under balanced sampling.
The identical evaluation protocol and model set are used in both cases, enabling a direct comparison between regimes.

\paragraph{Uncertainty and statistical comparisons.}
Tables~\ref{tab:pixel_metrics_unbalanced} and \ref{tab:pixel_metrics_balanced} report mean $\pm$ standard deviation across folds.
These summaries indicate fold-level variability but do not quantify uncertainty of differences between models or regimes; all comparisons are therefore interpreted descriptively.

\FloatBarrier
% =========================================================
% BOTH TABLES ON SAME LANDSCAPE PAGE (WITH VALUES)
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level classification metrics (unbalanced training set). Values are reported as mean $\pm$ standard deviation across folds where available. Precision/recall and ROC--AUC refer to the \textit{Cracked} class (positive class); F1 is the weighted F1-score.}
    \label{tab:pixel_metrics_unbalanced}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Accuracy & Precision (Cracked) & Recall (Cracked) & Weighted F1 & ROC--AUC (Cracked) \\
        \midrule
        PLS-DA                    & 0.9927 $\pm$ 0.0168 & 0.9999 & 0.9682 & 0.9924 & 1.0000 $\pm$ 0.0000 \\
        Logistic Regression (L1)  & 0.9962 $\pm$ 0.0074 & 0.9890 & 0.9948 & 0.9963 & 0.9996 $\pm$ 0.0012 \\
        SVM (RBF)                 & 0.9938 $\pm$ 0.0094 & 0.9943 & 0.9787 & 0.9938 & 0.9997 $\pm$ 0.0010 \\
        Random Forest            & 0.9968 $\pm$ 0.0055 & 0.9964 & 0.9904 & 0.9968 & 0.9990 $\pm$ 0.0035 \\
        XGBoost                  & 0.9947 $\pm$ 0.0103 & 0.9953 & 0.9798 & 0.9946 & 1.0000 $\pm$ 0.0000 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.3cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level classification metrics (balanced training set). Values are reported as mean $\pm$ standard deviation across folds where available. Precision/recall and ROC--AUC refer to the \textit{Cracked} class (positive class); F1 is the weighted F1-score.}
    \label{tab:pixel_metrics_balanced}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Accuracy & Precision (Cracked) & Recall (Cracked) & Weighted F1 & ROC--AUC (Cracked) \\
        \midrule
        PLS-DA                    & 0.9961 $\pm$ 0.0086 & 0.9998 & 0.9929 & 0.9961 & 1.0000 $\pm$ 0.0000 \\
        Logistic Regression (L1)  & 0.9930 $\pm$ 0.0148 & 0.9994 & 0.9861 & 0.9930 & 0.9999 $\pm$ 0.0002 \\
        SVM (RBF)                 & 0.9876 $\pm$ 0.0202 & 0.9948 & 0.9771 & 0.9875 & 0.9998 $\pm$ 0.0007 \\
        Random Forest            & 0.9935 $\pm$ 0.0110 & 0.9970 & 0.9882 & 0.9934 & 0.9990 $\pm$ 0.0038 \\
        XGBoost                  & 0.9934 $\pm$ 0.0107 & 0.9972 & 0.9882 & 0.9933 & 1.0000 $\pm$ 0.0001 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}

\medskip
\noindent
\textbf{Summary of binary classification performance.}
All five models achieve PR--AUC\textsubscript{CRACK} $\geq 0.987$ and ROC--AUC $\geq 0.999$ under both sampling regimes.

\noindent
\textbf{Balanced vs.\ unbalanced training.}
Balanced training raises Cracked Recall for most models (e.g., PLS-DA: 0.968 $\to$ 0.993) at a marginal accuracy cost.

\medskip
\noindent
The following section extends the classification to multi-class labeling strategies.


\section{Pixel-Level Multi-Class Classification}
\label{sec:pixel-multiclass}

\noindent
This section extends binary crack detection to three labeling strategies:
\textit{crack\_regular\_rest}, \textit{crack\_vs\_rest}, and a full \textit{multi\_class} formulation.
PR--AUC\textsubscript{CRACK} remains the primary metric.
For each strategy, balanced and unbalanced sampling regimes were evaluated, yielding six configurations.
Fold-aggregated summaries are interpreted descriptively; no hypothesis tests are reported.

\subsection{Balanced Training Sets}

\noindent
Results obtained under balanced sampling are summarized in Table~\ref{tab:pixel_metrics_balanced_all}.
The table reports performance for both binary labeling schemes and for the multi-class configuration.

\subsection{Unbalanced Training Sets}

\noindent
Results obtained on naturally imbalanced data are summarized in Table~\ref{tab:pixel_metrics_unbalanced_all}.
The same set of models and metrics are reported for direct comparison with the balanced regime.

\FloatBarrier

% =========================================================
% PAGE 1 (LANDSCAPE): UNBALANCED - SINGLE TABLE
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

\begin{table}[p]
    \centering
    \caption{Pixel-level classification metrics (\textbf{unbalanced} training set). Values are reported as mean $\pm$ standard deviation across folds.}
    \label{tab:pixel_metrics_unbalanced_all}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}llccccccc@{}}
        \toprule
        Label setup & Model &
        Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & Macro-F1 & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_regular\_rest}} \\
        & PLS-DA                & 0.5087 $\pm$ 0.0044 & 0.6879 $\pm$ 0.0239 & 0.8825 $\pm$ 0.1174 & 0.7537 $\pm$ 0.2385 & 0.5819 $\pm$ 0.0199 & 0.9977 $\pm$ 0.0049 & 0.8630 $\pm$ 0.1242 \\
        & Logistic Regression (L1) & 0.8537 $\pm$ 0.0021 & 0.8683 $\pm$ 0.0075 & 0.9654 $\pm$ 0.0185 & 0.9663 $\pm$ 0.0748 & 0.8010 $\pm$ 0.0049 & 0.9993 $\pm$ 0.0021 & 0.9839 $\pm$ 0.0116 \\
        & Random Forest         & 0.9175 $\pm$ 0.0056 & 0.9179 $\pm$ 0.0116 & 0.8942 $\pm$ 0.0644 & 0.9394 $\pm$ 0.0701 & 0.8904 $\pm$ 0.0106 & 0.9975 $\pm$ 0.0037 & 0.9622 $\pm$ 0.0495 \\
        & XGBoost               & 0.9212 $\pm$ 0.0040 & 0.9162 $\pm$ 0.0094 & 0.9804 $\pm$ 0.0118 & 0.9493 $\pm$ 0.0894 & 0.8832 $\pm$ 0.0074 & 0.9992 $\pm$ 0.0017 & 0.9902 $\pm$ 0.0160 \\
        & MLP (Small)           & 0.9175 $\pm$ 0.0059 & 0.9166 $\pm$ 0.0122 & 0.9530 $\pm$ 0.0272 & 0.9729 $\pm$ 0.0398 & 0.8759 $\pm$ 0.0176 & 0.9994 $\pm$ 0.0017 & 0.9911 $\pm$ 0.0122 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_vs\_rest}} \\
        & PLS-DA                & 0.5120 $\pm$ 0.0047 & 0.6940 $\pm$ 0.0189 & 0.8843 $\pm$ 0.1117 & 0.7623 $\pm$ 0.2303 & 0.5893 $\pm$ 0.0164 & 0.9979 $\pm$ 0.0045 & 0.8669 $\pm$ 0.1186 \\
        & Logistic Regression (L1) & 0.8574 $\pm$ 0.0022 & 0.8704 $\pm$ 0.0076 & 0.9669 $\pm$ 0.0190 & 0.9651 $\pm$ 0.0759 & 0.8049 $\pm$ 0.0050 & 0.9993 $\pm$ 0.0022 & 0.9847 $\pm$ 0.0114 \\
        & Random Forest         & 0.9147 $\pm$ 0.0060 & 0.9159 $\pm$ 0.0126 & 0.8871 $\pm$ 0.0701 & 0.9368 $\pm$ 0.0751 & 0.8857 $\pm$ 0.0116 & 0.9975 $\pm$ 0.0041 & 0.9610 $\pm$ 0.0535 \\
        & XGBoost               & 0.9200 $\pm$ 0.0043 & 0.9154 $\pm$ 0.0098 & 0.9809 $\pm$ 0.0120 & 0.9508 $\pm$ 0.0888 & 0.8825 $\pm$ 0.0078 & 0.9992 $\pm$ 0.0018 & 0.9900 $\pm$ 0.0165 \\
        & MLP (Small)           & 0.9163 $\pm$ 0.0062 & 0.9158 $\pm$ 0.0129 & 0.9521 $\pm$ 0.0282 & 0.9736 $\pm$ 0.0390 & 0.8741 $\pm$ 0.0188 & 0.9994 $\pm$ 0.0017 & 0.9912 $\pm$ 0.0122 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{multi\_class}} \\
        & PLS-DA                & 0.9670 $\pm$ 0.0211 & 0.9787 $\pm$ 0.0121 & 0.7204 $\pm$ 0.1749 & 0.9927 $\pm$ 0.0127 & 0.9013 $\pm$ 0.0709 & 0.9978 $\pm$ 0.0025 & 0.9677 $\pm$ 0.0372 \\
        & Logistic Regression (L1) & 0.9972 $\pm$ 0.0031 & 0.9959 $\pm$ 0.0097 & 0.9675 $\pm$ 0.0225 & 0.9977 $\pm$ 0.0078 & 0.9920 $\pm$ 0.0085 & 0.9996 $\pm$ 0.0016 & 0.9971 $\pm$ 0.0079 \\
        & Random Forest         & 0.9965 $\pm$ 0.0030 & 0.9940 $\pm$ 0.0120 & 0.9675 $\pm$ 0.0234 & 0.9969 $\pm$ 0.0090 & 0.9906 $\pm$ 0.0100 & 0.9994 $\pm$ 0.0026 & 0.9970 $\pm$ 0.0077 \\
        & XGBoost               & 0.9978 $\pm$ 0.0021 & 0.9946 $\pm$ 0.0103 & 0.9812 $\pm$ 0.0149 & 0.9908 $\pm$ 0.0206 & 0.9923 $\pm$ 0.0074 & 0.9995 $\pm$ 0.0021 & 0.9977 $\pm$ 0.0069 \\
        & MLP (Small)           & 0.9905 $\pm$ 0.0249 & 0.9890 $\pm$ 0.0224 & 0.9244 $\pm$ 0.1648 & 0.9866 $\pm$ 0.0259 & 0.9699 $\pm$ 0.0704 & 0.9965 $\pm$ 0.0083 & 0.9898 $\pm$ 0.0247 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

% =========================================================
% PAGE 2 (LANDSCAPE): BALANCED - SINGLE TABLE
% =========================================================
\clearpage
\thispagestyle{plain}

\begin{table}[p]
    \centering
    \caption{Pixel-level classification metrics (\textbf{balanced} training set). Values are reported as mean $\pm$ standard deviation across folds.}
    \label{tab:pixel_metrics_balanced_all}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}llccccccc@{}}
        \toprule
        Label setup & Model &
        Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & Macro-F1 & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_regular\_rest}} \\
        & PLS-DA                & 0.4771 $\pm$ 0.0088 & 0.7061 $\pm$ 0.0107 & 0.8433 $\pm$ 0.0910 & 0.9451 $\pm$ 0.1069 & 0.5875 $\pm$ 0.0090 & 0.9982 $\pm$ 0.0046 & 0.9500 $\pm$ 0.0899 \\
        & Logistic Regression (L1) & 0.8629 $\pm$ 0.0033 & 0.8653 $\pm$ 0.0073 & 0.9865 $\pm$ 0.0067 & 0.9671 $\pm$ 0.0739 & 0.8097 $\pm$ 0.0054 & 0.9994 $\pm$ 0.0018 & 0.9874 $\pm$ 0.0090 \\
        & Random Forest         & 0.9277 $\pm$ 0.0046 & 0.9199 $\pm$ 0.0110 & 0.9101 $\pm$ 0.0533 & 0.9194 $\pm$ 0.1124 & 0.9047 $\pm$ 0.0087 & 0.9975 $\pm$ 0.0035 & 0.9725 $\pm$ 0.0438 \\
        & XGBoost               & 0.9242 $\pm$ 0.0026 & 0.9111 $\pm$ 0.0084 & 0.9932 $\pm$ 0.0042 & 0.9437 $\pm$ 0.0954 & 0.8839 $\pm$ 0.0067 & 0.9993 $\pm$ 0.0016 & 0.9919 $\pm$ 0.0175 \\
        & MLP (Small)           & 0.9219 $\pm$ 0.0053 & 0.9140 $\pm$ 0.0125 & 0.9831 $\pm$ 0.0123 & 0.9832 $\pm$ 0.0223 & 0.8869 $\pm$ 0.0167 & 0.9982 $\pm$ 0.0057 & 0.9934 $\pm$ 0.0097 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_vs\_rest}} \\
        & PLS-DA                & 0.4782 $\pm$ 0.0090 & 0.7078 $\pm$ 0.0106 & 0.8445 $\pm$ 0.0904 & 0.9455 $\pm$ 0.1063 & 0.5889 $\pm$ 0.0093 & 0.9982 $\pm$ 0.0046 & 0.9507 $\pm$ 0.0886 \\
        & Logistic Regression (L1) & 0.8642 $\pm$ 0.0032 & 0.8662 $\pm$ 0.0074 & 0.9866 $\pm$ 0.0067 & 0.9674 $\pm$ 0.0738 & 0.8109 $\pm$ 0.0055 & 0.9994 $\pm$ 0.0018 & 0.9876 $\pm$ 0.0089 \\
        & Random Forest         & 0.9261 $\pm$ 0.0049 & 0.9191 $\pm$ 0.0113 & 0.9092 $\pm$ 0.0546 & 0.9192 $\pm$ 0.1128 & 0.9038 $\pm$ 0.0091 & 0.9974 $\pm$ 0.0036 & 0.9722 $\pm$ 0.0444 \\
        & XGBoost               & 0.9238 $\pm$ 0.0027 & 0.9110 $\pm$ 0.0085 & 0.9931 $\pm$ 0.0042 & 0.9439 $\pm$ 0.0954 & 0.8836 $\pm$ 0.0068 & 0.9993 $\pm$ 0.0016 & 0.9919 $\pm$ 0.0176 \\
        & MLP (Small)           & 0.9221 $\pm$ 0.0052 & 0.9143 $\pm$ 0.0126 & 0.9830 $\pm$ 0.0124 & 0.9833 $\pm$ 0.0222 & 0.8871 $\pm$ 0.0165 & 0.9982 $\pm$ 0.0057 & 0.9934 $\pm$ 0.0097 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{multi\_class}} \\
        & PLS-DA                & 0.9666 $\pm$ 0.0210 & 0.9786 $\pm$ 0.0120 & 0.7179 $\pm$ 0.1759 & 0.9926 $\pm$ 0.0129 & 0.9010 $\pm$ 0.0707 & 0.9978 $\pm$ 0.0025 & 0.9674 $\pm$ 0.0375 \\
        & Logistic Regression (L1) & 0.9973 $\pm$ 0.0031 & 0.9960 $\pm$ 0.0097 & 0.9677 $\pm$ 0.0224 & 0.9977 $\pm$ 0.0078 & 0.9921 $\pm$ 0.0085 & 0.9996 $\pm$ 0.0016 & 0.9971 $\pm$ 0.0079 \\
        & Random Forest         & 0.9966 $\pm$ 0.0030 & 0.9941 $\pm$ 0.0120 & 0.9676 $\pm$ 0.0233 & 0.9969 $\pm$ 0.0090 & 0.9907 $\pm$ 0.0100 & 0.9994 $\pm$ 0.0026 & 0.9970 $\pm$ 0.0077 \\
        & XGBoost               & 0.9979 $\pm$ 0.0021 & 0.9947 $\pm$ 0.0103 & 0.9813 $\pm$ 0.0149 & 0.9908 $\pm$ 0.0206 & 0.9924 $\pm$ 0.0074 & 0.9995 $\pm$ 0.0021 & 0.9977 $\pm$ 0.0069 \\
        & MLP (Small)           & 0.9906 $\pm$ 0.0249 & 0.9891 $\pm$ 0.0224 & 0.9246 $\pm$ 0.1647 & 0.9866 $\pm$ 0.0259 & 0.9700 $\pm$ 0.0704 & 0.9965 $\pm$ 0.0083 & 0.9898 $\pm$ 0.0247 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\end{landscape}

\medskip
\noindent
\textbf{Effect of labeling strategy.}
The \textit{multi\_class} formulation achieves the highest PR--AUC\textsubscript{CRACK} across all models ($\geq 0.967$); the two binary schemes yield broadly comparable results for non-PLS-DA models ($\geq 0.863$).

\noindent
\textbf{Balanced vs.\ unbalanced at the multi-class level.}
Class balancing has negligible impact under the \textit{multi\_class} formulation (e.g., XGBoost PR--AUC\textsubscript{CRACK} $= 0.9977$ in both regimes); PLS-DA shows the highest fold variance under binary schemes ($0.8825 \pm 0.1174$ unbalanced).

\medskip
\noindent
\textbf{PR--AUC\textsubscript{CRACK} $> 0.99$} is achieved by Logistic Regression, Random Forest, and XGBoost under the \textit{multi\_class} formulation in both regimes.

\noindent
The next section evaluates reconstruction-based anomaly detection using autoencoders.



% ==========================================================
% Autoencoder-Based Anomaly Detection
% ==========================================================
\section{Autoencoder-Based Anomaly Detection}
\label{sec:anomaly-detection}

\noindent
Pixel-level crack detection was also evaluated using reconstruction-based anomaly detection.
Three autoencoder architectures were examined: (64--32--16), (128--64--32), and (64--32--8).
PR--AUC\textsubscript{CRACK} and ROC--AUC\textsubscript{CRACK} serve as the primary and secondary metrics, respectively.

\noindent
Three training formulations were evaluated:
(i)~a binary one-class model trained on non-cracked pixels;
(ii)~a multi-class model trained exclusively on cracked pixels, treating all other classes as anomalous;
(iii)~a multi-class model trained on all classes except cracked, treating crack pixels as anomalous.
Each formulation was evaluated under balanced and unbalanced sampling.

% =========================================================
% AUTOENCODER ONE-CLASS BINARY: UNBALANCED + BALANCED (SAME PAGE)
% =========================================================
\noindent
Tables~\ref{tab:autoencoder_pixel_results} and~\ref{tab:autoencoder_pixel_results_balanced} report the one-class binary results under unbalanced and balanced sampling, respectively.

\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on non-cracked samples (unbalanced sampling).
    Values are reported as mean $\pm$ standard deviation across folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.8969 $\pm$ 0.1646
        & 0.8226 $\pm$ 0.1925
        & 0.9971 $\pm$ 0.0016
        & 0.9017 $\pm$ 0.1047
        & 0.9814 $\pm$ 0.0327
        & 0.9554 $\pm$ 0.0900
        & 63.34 $\pm$ 2.80
        & 0.043 $\pm$ 0.004 \\

        Autoencoder (128--64--32)
        & 0.8597 $\pm$ 0.1813
        & 0.7544 $\pm$ 0.2308
        & 0.9974 $\pm$ 0.0015
        & 0.8628 $\pm$ 0.1270
        & 0.9673 $\pm$ 0.0469
        & 0.9253 $\pm$ 0.1190
        & 60.90 $\pm$ 1.48
        & 0.057 $\pm$ 0.001 \\

        Autoencoder (64--32--8)
        & 0.8923 $\pm$ 0.1648
        & 0.8126 $\pm$ 0.1938
        & 0.9978 $\pm$ 0.0013
        & 0.8965 $\pm$ 0.1050
        & 0.9799 $\pm$ 0.0334
        & 0.9511 $\pm$ 0.0913
        & 61.02 $\pm$ 1.46
        & 0.041 $\pm$ 0.001 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.0cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on non-cracked samples (balanced sampling).
    Values are reported as mean $\pm$ standard deviation across folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_balanced}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9343 $\pm$ 0.0989
        & 0.9122 $\pm$ 0.1348
        & 0.9906 $\pm$ 0.0340
        & 0.9437 $\pm$ 0.0820
        & 0.9823 $\pm$ 0.0457
        & 0.9702 $\pm$ 0.0592
        & 65.35 $\pm$ 1.90
        & 0.008 $\pm$ 0.002 \\

        Autoencoder (128--64--32)
        & 0.9083 $\pm$ 0.1332
        & 0.8771 $\pm$ 0.1576
        & 0.9982 $\pm$ 0.0058
        & 0.9256 $\pm$ 0.1016
        & 0.9919 $\pm$ 0.0178
        & 0.9833 $\pm$ 0.0357
        & 58.49 $\pm$ 3.87
        & 0.007 $\pm$ 0.001 \\

        Autoencoder (64--32--8)
        & 0.9353 $\pm$ 0.1027
        & 0.9074 $\pm$ 0.1338
        & 0.9977 $\pm$ 0.0100
        & 0.9449 $\pm$ 0.0833
        & 0.9858 $\pm$ 0.0344
        & 0.9757 $\pm$ 0.0501
        & 52.04 $\pm$ 2.44
        & 0.006 $\pm$ 0.002 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}


% =========================================================
% AUTOENCODER CRACK-ONLY MULTI-CLASS: UNBALANCED + BALANCED (SAME PAGE)
% =========================================================
\noindent
Tables~\ref{tab:autoencoder_pixel_results_crack_only_multiclass} and~\ref{tab:autoencoder_pixel_results_crack_only_multiclass_balanced} report the crack-only training variant under unbalanced and balanced sampling, respectively.

\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained exclusively on \textit{Cracked} pixels and evaluated in a multi-class setting (unbalanced sampling).
    Values are reported as mean across LOGO folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_crack_only_multiclass}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9660
        & 0.9995
        & 0.9662
        & 0.9826
        & 0.9858
        & 0.9999
        & 26.96
        & 0.148 \\

        Autoencoder (128--64--32)
        & 0.9861
        & 0.9991
        & 0.9869
        & 0.9930
        & 0.9846
        & 0.9999
        & 27.12
        & 0.207 \\

        Autoencoder (64--32--8)
        & 0.9629
        & 0.9994
        & 0.9632
        & 0.9809
        & 0.9834
        & 0.9999
        & 26.20
        & 0.155 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.0cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained exclusively on \textit{Cracked} pixels and evaluated in a multi-class setting (balanced sampling).
    Values are reported as mean $\pm$ standard deviation across LOGO folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_crack_only_multiclass_balanced}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9660 $\pm$ 0.0106
        & 0.9995 $\pm$ 0.0008
        & 0.9662 $\pm$ 0.0108
        & 0.9826 $\pm$ 0.0055
        & 0.9858 $\pm$ 0.0281
        & 0.9999 $\pm$ 0.0001
        & 61.06 $\pm$ 7.56
        & 0.045 $\pm$ 0.013 \\

        Autoencoder (128--64--32)
        & 0.9861 $\pm$ 0.0052
        & 0.9991 $\pm$ 0.0017
        & 0.9869 $\pm$ 0.0051
        & 0.9930 $\pm$ 0.0027
        & 0.9846 $\pm$ 0.0274
        & 0.9999 $\pm$ 0.0001
        & 63.03 $\pm$ 8.09
        & 0.056 $\pm$ 0.004 \\

        Autoencoder (64--32--8)
        & 0.9629 $\pm$ 0.0149
        & 0.9994 $\pm$ 0.0011
        & 0.9632 $\pm$ 0.0150
        & 0.9809 $\pm$ 0.0079
        & 0.9834 $\pm$ 0.0298
        & 0.9999 $\pm$ 0.0002
        & 59.80 $\pm$ 8.55
        & 0.041 $\pm$ 0.003 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}


% =========================================================
% AUTOENCODER NON-CRACK MULTI-CLASS: UNBALANCED + BALANCED (SAME PAGE)
% =========================================================
\noindent
Tables~\ref{tab:autoencoder_pixel_results_noncrack_multiclass} and~\ref{tab:autoencoder_pixel_results_noncrack_multiclass_balanced} report the non-crack training variant, where the autoencoder was trained on all classes except cracked.

\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on all semantic classes except \textit{Cracked} and evaluated in a multi-class setting (unbalanced sampling).
    Values are reported for the held-out test split.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_noncrack_multiclass}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9969
        & 0.0096
        & 0.1110
        & 0.0177
        & 0.4575
        & 0.0083
        & 1429.22
        & 0.388 \\

        Autoencoder (128--64--32)
        & 0.9986
        & 0.0118
        & 0.0126
        & 0.0122
        & 0.4513
        & 0.0084
        & 999.73
        & 0.255 \\

        Autoencoder (64--32--8)
        & 0.9984
        & 0.0207
        & 0.0316
        & 0.0250
        & 0.4542
        & 0.0110
        & 916.85
        & 0.141 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.0cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on all semantic classes except \textit{Cracked} and evaluated in a multi-class setting (balanced sampling).
    Values are reported for the held-out test split.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_noncrack_multiclass_balanced}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9791
        & 0.0000
        & 0.0000
        & 0.0000
        & 0.2472
        & 0.0083
        & 2567.17
        & 0.381 \\

        Autoencoder (128--64--32)
        & 0.9789
        & 0.0000
        & 0.0000
        & 0.0000
        & 0.2847
        & 0.0084
        & 2260.73
        & 0.299 \\

        Autoencoder (64--32--8)
        & 0.9781
        & 0.0000
        & 0.0000
        & 0.0000
        & 0.4513
        & 0.0110
        & 1853.00
        & 0.227 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}

\medskip
\noindent
Tables~\ref{tab:autoencoder_pixel_results}--\ref{tab:autoencoder_pixel_results_noncrack_multiclass_balanced} summarize pixel-level anomaly detection across all three training formulations and both sampling regimes.
The crack-only multi-class formulation achieves the highest PR--AUC\textsubscript{CRACK} ($\approx 0.9999$ across all architectures). The binary one-class variant reaches PR--AUC of 0.925--0.970 with higher fold variance, while the non-crack multi-class formulation yields PR--AUC $\approx 0.008$--$0.011$ across all architectures.
The next section evaluates whole-image crack detection by aggregating pixel-level outputs into image-level decisions.




\section{Whole-Image Crack Detection}
\label{sec:full-image-early-late}

\noindent
Whole-image detection was evaluated under \textit{early-stage} (initial onset) and \textit{late-stage} (advanced cracking) acquisition conditions.
The primary metric is \textbf{F1\textsubscript{CRACK}}, supported by Precision, Recall, and MCC; additional measures (F2, specificity, NPV) and confusion matrices are included for completeness.
Post-processing parameters were optimized on a Row~1 calibration split and assessed on an independent Row~2 test set.

\noindent
In addition to the supervised pixel-level classifier, the pipeline was evaluated using an autoencoder-based anomaly detector (128--64--32, trained on \textit{Cracked} pixels), with reconstruction errors converted to pseudo-probabilities via exponential decay mapping.

\noindent
Table~\ref{tab:full_image_dataset_composition} reports the dataset composition; Table~\ref{tab:full_image_early_late_metrics} summarizes detection performance.



% =========================================================
% FULL-IMAGE DATASET COMPOSITION TABLE (PRETTY)
% =========================================================
\begin{table}[htbp]
\centering
\caption{Dataset composition for full-image crack-detection experiments under early- and late-stage acquisition conditions (initial/minimal vs.\ advanced cracking).
Row~1 was used for calibration, while Row~2 served as an independent test set.}
\label{tab:full_image_dataset_composition}

\small
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}

\begin{tabular}{@{}lllcccc@{}}

\toprule
Row & Stage & Split & Total & Unique & Pos & Neg \\
\midrule
Row 1 & Early & Calibration & 172 & 60 & 32 & 140 \\
Row 1 & Late  & Calibration & 160 & 60 & 35 & 125 \\
\addlinespace[4pt]
Row 2 & Early & Test        & 60  & 60 & 32 & 28  \\
Row 2 & Late  & Test        & 60  & 60 & 34 & 26  \\
\bottomrule
\end{tabular}

\end{table}

% =========================================================
% FULL IMAGE EARLY vs LATE – METRICS TABLE
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

\begin{table}[p]
\centering
\caption{Full-image crack detection performance for early- and late-stage acquisition experiments (initial/minimal vs.\ advanced cracking).
Metrics are reported separately for calibration and test splits.
Confusion-matrix entries correspond to the number of grape clusters classified as cracked or healthy.
The main performance indicator is \textbf{F1 (Cracked)}. Accuracy, Balanced Accuracy and MCC are reported overall. Precision/Recall/F1/F2 refer to the \textit{Cracked} class (positive class), while Specificity and NPV refer to the \textit{Healthy} class (negative class).}
\label{tab:full_image_early_late_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
34 & 5 & 120 & 1 &
0.963 & 0.966 & 0.872 & 0.971 & 0.919 & 0.950 & 0.960 & 0.992 & 0.897 \\

Late & Test &
27 & 5 & 21 & 7 &
0.800 & 0.801 & 0.844 & 0.794 & 0.818 & 0.804 & 0.808 & 0.750 & 0.598 \\

\addlinespace[4pt]

Early & Calibration &
23 & 6 & 134 & 9 &
0.913 & 0.838 & 0.793 & 0.719 & 0.754 & 0.732 & 0.957 & 0.937 & 0.703 \\

Early & Test &
19 & 3 & 25 & 13 &
0.733 & 0.743 & 0.864 & 0.594 & 0.704 & 0.633 & 0.893 & 0.658 & 0.504 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\end{landscape}

\medskip
\noindent
Under late-stage conditions the supervised pipeline achieves F1\textsubscript{CRACK} = 0.818 and MCC = 0.598 on the independent test set. Under early-stage conditions F1\textsubscript{CRACK} is 0.704 and MCC is 0.504.

% =========================================================
% FULL IMAGE AUTOENCODER – METRICS TABLE
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

\begin{table}[p]
\centering
\caption{Full-image crack detection using autoencoder-based anomaly detection (128--64--32, trained on \textit{Cracked} pixels).
Reconstruction errors (MSE) were converted to pseudo-probabilities via exponential decay, $p_{\mathrm{CRACK}}(x)=\exp(-e(x)/\tau)$;
the same post-processing pipeline (thresholding, morphological filtering, patch-based aggregation) was applied.
Metrics are reported for calibration and test splits under early- and late-stage conditions.
The main performance indicator is \textbf{F1 (Cracked)}. Accuracy, Balanced Accuracy and MCC are reported overall. Precision/Recall/F1/F2 refer to the \textit{Cracked} class (positive class), while Specificity and NPV refer to the \textit{Healthy} class (negative class).}
\label{tab:full_image_autoencoder_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
26 & 7 & 118 & 9 &
0.900 & 0.843 & 0.788 & 0.743 & 0.765 & 0.751 & 0.944 & 0.929 & 0.702 \\

Late & Test &
22 & 7 & 19 & 12 &
0.683 & 0.689 & 0.759 & 0.647 & 0.698 & 0.667 & 0.731 & 0.613 & 0.375 \\

\addlinespace[4pt]

Early & Calibration &
19 & 17 & 123 & 13 &
0.826 & 0.736 & 0.528 & 0.594 & 0.559 & 0.579 & 0.879 & 0.904 & 0.452 \\

Early & Test &
13 & 13 & 15 & 19 &
0.467 & 0.471 & 0.500 & 0.406 & 0.448 & 0.422 & 0.536 & 0.441 & $-$0.058 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\end{landscape}

\medskip
\noindent
Table~\ref{tab:full_image_autoencoder_metrics} reports whole-image performance when driven by autoencoder-based scores.
The autoencoder-based pipeline yields lower F1\textsubscript{CRACK} than the supervised classifier in all conditions (Late Test: 0.698 vs.\ 0.818; Early Test: 0.448 vs.\ 0.704); MCC under early-stage conditions is $-0.058$.
The next section evaluates the effect of wavelength selection on this pipeline using Backward Feature Selection.


% ------------------------------------------------------------
\section{Wavelength Selection via Backward Feature Selection}
\label{sec:bfs-setup}

\noindent
Backward Feature Selection (BFS) was applied to evaluate whole-image crack-detection performance as a function of the number of retained wavelengths.
The primary objective is PR--AUC\textsubscript{CRACK}; the full-spectrum baseline corresponds to 159~wavelengths (450--925\,nm).

\subsection{BFS Trajectories and Subset Selection}

\FloatBarrier

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/bfs_dual_metric.png}
    \caption{BFS trajectories for the whole-image pipeline: CRACK-class PR--AUC (primary objective) and CRACK-class F1 versus the number of retained wavelengths ($n$; full baseline $n{=}159$).}
    \label{fig:bfs_dual_metric}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/feature_selection/bfs_f1_thresholds.png}
        \caption{CRACK-class F1 vs. number of wavelengths ($n$), with markers at the maximum and at 0.5\%/1.0\% drop points.}
        \label{fig:bfs_f1_thresholds}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/feature_selection/bfs_prauc_thresholds.png}
        \caption{CRACK-class PR--AUC (primary BFS objective) vs. number of wavelengths ($n$), with the same threshold markers.}
        \label{fig:bfs_prauc_thresholds}
    \end{subfigure}
    \caption{BFS threshold-marker visualization on CRACK metrics. Markers indicate the maximum and the first $n$ where performance drops more than 0.5\% and 1.0\% below that maximum.}
    \label{fig:bfs_threshold_markers}
\end{figure}

\begin{table}[!htbp]
\centering
\caption{Summary of BFS wavelength-count thresholds for CRACK metrics.}
\label{tab:bfs_thresholds_summary}
\begin{tabular}{lcccccc}
\toprule
Metric & Max score & $n_{\max}$ & 0.5\% drop: $n$ & score & 1.0\% drop: $n$ & score \\
\midrule
CRACK PR--AUC & 0.9948 & 30  & 11  & 0.9878 & 9  & 0.9790 \\
CRACK F1      & 0.9854 & 159 & 105 & 0.9800 & 88 & 0.9751 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Subset selection criteria.}
Subset sizes are derived from the BFS trajectories (Figure~\ref{fig:bfs_threshold_markers}; Table~\ref{tab:bfs_thresholds_summary}).
The \textbf{top-30} subset maximizes PR--AUC\textsubscript{CRACK} ($n_{\max}{=}30$), while the \textbf{top-11} subset corresponds to the first count where PR--AUC\textsubscript{CRACK} drops by more than 0.5\% from its maximum ($n{=}11$).
The analogous 1.0\% drop threshold yields $n{=}9$ (Table~\ref{tab:full_image_wavelength_results}).


\FloatBarrier


\FloatBarrier

\subsection{End-to-End Wavelength-Selection Results}

\noindent
Table~\ref{tab:full_image_wavelength_results} reports whole-image detection metrics for representative wavelength subsets, evaluated separately for early- and late-stage acquisitions on the held-out test set.

\begin{table}[!htbp]
\centering
\caption{Full-image crack-detection performance for different wavelength subsets under early- and late-stage conditions.}
\label{tab:full_image_wavelength_results}
\begin{tabular}{cccccccc}
\toprule
Stage & \# Wavelengths & Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & MCC \\
\midrule
Early & 9 & 0.7833 & 0.7902 & 0.8800 & 0.6875 & 0.7719 & 0.5873 \\
Late  & 9 & 0.8000 & 0.8054 & 0.8667 & 0.7647 & 0.8125 & 0.6054 \\
\midrule
Early & 11 & 0.7500 & 0.7567 & 0.8400 & 0.6562 & 0.7368 & 0.5195 \\
Late  & 11 & 0.8167 & 0.8111 & 0.8286 & 0.8529 & 0.8406 & 0.6254 \\
\midrule
Early & 30 & \textbf{0.8167} & \textbf{0.8214} & \textbf{0.8889} & \textbf{0.7500} & \textbf{0.8136} & \textbf{0.6447} \\
Late  & 30 & \textbf{0.8333} & \textbf{0.8348} & \textbf{0.8750} & 0.8235 & 0.8485 & \textbf{0.6652} \\
\midrule
Early & 159 & 0.8000 & 0.8036 & 0.8571 & \textbf{0.7500} & 0.8000 & 0.6071 \\
Late  & 159 & \textbf{0.8333} & 0.8258 & 0.8333 & \textbf{0.8824} & \textbf{0.8571} & 0.6591 \\
\bottomrule
\end{tabular}
\end{table}


\noindent
The top-30 wavelength subset (30 of 159 bands) matches or exceeds the full-spectrum baseline in both acquisition stages (Early F1\textsubscript{CRACK}: 0.8136 vs.\ 0.8000; Late F1\textsubscript{CRACK}: 0.8485 vs.\ 0.8571). The top-11 subset achieves F1\textsubscript{CRACK} $\geq 0.737$.

\begin{table}[!htbp]
\centering
\caption{Model Generalization Analysis: Comparison between Calibration and Test MCC.}
\label{tab:mcc_generalization}
\begin{tabular}{ccccc}
\toprule
Stage & \# Features & MCC Calibration & MCC Test & \textbf{Gap ($\Delta$)} \\
\midrule
Early & 9   & 0.7381 & 0.5873 & 0.1508 \\
      & 30  & 0.7232 & \textbf{0.6447} & \textbf{0.0785} \\
      & 159 & 0.6631 & 0.6071 & 0.0560 \\
\midrule
Late  & 9   & 0.8968 & 0.6054 & 0.2914 \\
      & 30  & 0.8771 & \textbf{0.6652} & \textbf{0.2119} \\
      & 159 & 0.8931 & 0.6591 & 0.2340 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Calibration-to-test generalization gap.}
Table~\ref{tab:mcc_generalization} shows a consistent calibration-to-test gap across all wavelength subsets and both acquisition stages.
Early-stage gaps range from 0.056 to 0.151; late-stage gaps are larger (0.212--0.291).

\noindent
\textbf{Note on uncertainty.} Tables~\ref{tab:full_image_wavelength_results} and \ref{tab:mcc_generalization} report point estimates; confidence intervals for metric differences are not provided because the exported results lack per-unit identifiers for resampling.

\FloatBarrier

\medskip
\noindent
The following subsection assesses whether BFS outcomes are robust to random variation by repeating the elimination sequence under multiple seeds.


% ============================================================
% FEATURE SELECTION STABILITY ANALYSIS
% ============================================================
\subsection{Stability Analysis}
\label{sec:bfs-stability}

\noindent
The BFS elimination sequence was repeated five times using seeds~1--5 for both the data split and the model initialization.
The objective was PR--AUC\textsubscript{CRACK}, with elimination from 159 down to 1~wavelength.
For each seed, the top-30 ($n_{\max}$) and top-11 (0.5\% drop) subsets were recorded.

\noindent
Table~\ref{tab:bfs_stability_summary} summarizes the per-seed results.
Best PR--AUC ranged from 0.812 to 0.998, and the optimal wavelength count varied between 64 and 153.

\begin{table}[!htbp]
\centering
\caption{BFS stability: per-seed results under varying train/test splits.}
\label{tab:bfs_stability_summary}
\begin{tabular}{ccccc}
\toprule
Seed & Split seed & Model seed & Best PR--AUC (Cracked) & Best $n$ (wavelengths) \\
\midrule
1 & 1 & 1 & 0.9014 & 153 \\
2 & 2 & 2 & 0.9933 &  66 \\
3 & 3 & 3 & 0.8714 & 111 \\
4 & 4 & 4 & 0.9979 &  64 \\
5 & 5 & 5 & 0.8119 & 149 \\
\midrule
\multicolumn{3}{c}{Mean $\pm$ Std} & $0.915 \pm 0.077$ & $108.6 \pm 42.2$ \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

% ── Jaccard tables ──
\subsubsection{Wavelength Overlap Across Seeds}

\noindent
Tables~\ref{tab:jaccard_30} and~\ref{tab:jaccard_11} report the pairwise Jaccard similarity
between the wavelength subsets selected by each seed for the top-30 and top-11 configurations, respectively.

\begin{table}[!htbp]
\centering
\caption{Pairwise Jaccard similarity of top-30 wavelengths across seeds (varying split). Mean Jaccard = 0.493.}
\label{tab:jaccard_30}
\begin{tabular}{cccccc}
\toprule
 & Seed~1 & Seed~2 & Seed~3 & Seed~4 & Seed~5 \\
\midrule
Seed~1 & ---   & 0.409 & 0.590 & 0.722 & 0.442 \\
Seed~2 & 0.409 & ---   & 0.378 & 0.409 & 0.476 \\
Seed~3 & 0.590 & 0.378 & ---   & 0.590 & 0.442 \\
Seed~4 & 0.722 & 0.409 & 0.590 & ---   & 0.476 \\
Seed~5 & 0.442 & 0.476 & 0.442 & 0.476 & ---   \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Pairwise Jaccard similarity of top-11 wavelengths across seeds (varying split). Mean Jaccard = 0.398.}
\label{tab:jaccard_11}
\begin{tabular}{cccccc}
\toprule
 & Seed~1 & Seed~2 & Seed~3 & Seed~4 & Seed~5 \\
\midrule
Seed~1 & ---   & 0.500 & 0.412 & 0.333 & 0.412 \\
Seed~2 & 0.500 & ---   & 0.412 & 0.412 & 0.333 \\
Seed~3 & 0.412 & 0.412 & ---   & 0.500 & 0.333 \\
Seed~4 & 0.333 & 0.412 & 0.500 & ---   & 0.333 \\
Seed~5 & 0.412 & 0.333 & 0.333 & 0.333 & ---   \\
\bottomrule
\end{tabular}
\end{table}

The top-30 subsets share 13 wavelengths across all five seeds (42\%), with a mean Jaccard of 0.493.

\noindent
The top-11 subsets share 4 wavelengths across all five seeds (33\%), with a mean Jaccard of 0.398.
Based on the per-seed wavelength export files (\url{experiments/feature\_selection/stability\_bfs\_prauc/varyingsplit/*/selected\_wavelengths\_at11.csv}),
the cross-seed intersection at this setting comprises exactly four wavelengths:
(452.25, 548.55, 580.90, and 729.53)\,nm.
These same four wavelengths also appear in the 13-wavelength intersection of the top-30 subsets (validated from the corresponding \texttt{selected\_wavelengths\_at30.csv} files), indicating that they are consistently selected under both subset sizes.
The recurrence of the same wavelengths across independent random seeds indicates that these bands correspond to stable spectral features of cracked tissue, a property that supports the future design of targeted narrow-band or multispectral sensors.

\FloatBarrier

% ── Top-30 WL Figures ──
\subsubsection{Wavelength Popularity and Stability -- Top 30}

\noindent
Figures~\ref{fig:stability_30_popularity}--\ref{fig:stability_30_spectrum}
visualise the selection frequency and stability of the top-30 wavelength subsets across the five seeds.
Figure~\ref{fig:stability_30_binning} further shows that the same dominant regions persist under coarser binning, indicating that the stability conclusions are not an artefact of plotting resolution.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-30 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_30_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_binning_comparison.png}
    \caption{Wavelength popularity at different bin sizes (1, 5, 10, 20\,nm) for the top-30 subsets.}
    \label{fig:stability_30_binning}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_stability_heatmap.png}
    \caption{Stability heatmap for the top-30 BFS subsets across five seeds (objective: PR--AUC\textsubscript{CRACK}). Each cell indicates how many seeds selected a wavelength within that 5\,nm bin.}
    \label{fig:stability_30_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-30 subsets.}
    \label{fig:stability_30_spectrum}
\end{figure}

\FloatBarrier

% ── Top-11 WL Figures ──
\subsubsection{Wavelength Popularity and Stability -- Top 11}

\noindent
The same analysis was repeated for the top-11 wavelength subsets
(Figures~\ref{fig:stability_11_popularity}--\ref{fig:stability_11_spectrum}),
corresponding to the most aggressive wavelength reduction evaluated
(within 1\% of the maximum PR--AUC).

\noindent
Figure~\ref{fig:stability_11_binning} shows wavelength selection frequency at different bin sizes, while Figure~\ref{fig:stability_11_heatmap} highlights which 5\,nm bins are consistently selected across seeds.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-11 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_11_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_binning_comparison.png}
    \caption{Wavelength popularity at different bin sizes (1, 5, 10, 20\,nm) for the top-11 subsets.}
    \label{fig:stability_11_binning}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_stability_heatmap.png}
    \caption{Stability heatmap for the top-11 BFS subsets across five seeds (objective: PR--AUC\textsubscript{CRACK}; 0.5\% drop setting). Each cell indicates how many seeds selected a wavelength within that 5\,nm bin.}
    \label{fig:stability_11_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-11 subsets.}
    \label{fig:stability_11_spectrum}
\end{figure}

\FloatBarrier
