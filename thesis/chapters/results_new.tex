\chapter{Experimental Results}
\label{ch:results}

\section{Overview of Experimental Results}

\noindent
This chapter presents the experimental evaluation of the hyperspectral crack-detection framework. The findings are structured to systematically address the research hypotheses formulated in Chapter~3, progressing from fundamental spectral characterization to operational field-level detection. Specifically, the chapter is divided into four main sections: 

\begin{enumerate}
    \item Validation of the physical and spectral distinguishability between healthy and cracked tissues.
    \item Pixel-level classification benchmarking.
    \item Whole-image spatial aggregation and detection under early- and late-stage field conditions.
    \item Wavelength selection analysis to determine the minimal spectral bands required for robust inference.
\end{enumerate}

\medskip
\noindent
\textbf{Presentation Note:} For clarity and readability, the performance metrics reported in the main text and tables are presented as mean values truncated to three decimal places. Comprehensive cross-validation statistics, including standard deviations and fold-level data, are provided in Appendix~\ref{app:cv_statistics}.

\clearpage
\subsection{Evaluation Baselines}
\label{subsec:results_baselines}

\noindent
To ensure a rigorous and transparent evaluation, all experimental results are benchmarked against the following explicit baselines:

\begin{itemize}
    \item \textbf{Pixel-Level Classification Baseline:} The reference model is \textbf{XGBoost} trained on the \textbf{full SNV-normalized hyperspectral signature} (450--925\,nm, 159 contiguous bands), as defined in Section~\ref{subsec:pixel_level_framework}. Binary-task performance is reported in the XGBoost rows of Table~\ref{tab:pixel_metrics_unbalanced}. Results for extended labeling and balancing regimes appear in Table~\ref{tab:pixel_metrics_unbalanced_all}, with balanced-training baselines detailed in Appendix~\ref{app:balanced_results}.
    
    \item \textbf{Whole-Image Crack-Detection Baseline:} System-level detection is evaluated using the spatial aggregation pipeline described in Section~\ref{subsec:full_image_pipeline}. The baseline performance for this task relies on probability maps generated by the full-spectrum (159-band) XGBoost pixel-level model, establishing the upper bound for detection accuracy prior to any wavelength reduction. Early- and late-stage calibration and held-out test performances are summarized in Table~\ref{tab:full_image_early_late_metrics}.
    
    \item \textbf{Wavelength Selection (BFS) Reference:} The efficacy of Backward Feature Selection (BFS) is quantified by comparing the classification performance of reduced wavelength subsets directly against the full 159-band baseline. Elimination trajectories are illustrated in Figures~\ref{fig:bfs_dual_metric} and~\ref{fig:bfs_threshold_markers}, and whole-image test-set results at critical wavelength thresholds are reported in Table~\ref{tab:full_image_wavelength_results}.
\end{itemize}

\newpage
\section{Spectral Separability and Informative Wavelengths}
\label{sec:spectral-characterization}

\noindent
Per-band spectral separability between healthy and cracked tissue was assessed prior to modeling, addressing \hyperref[hyp:spectral_distinguishability]{Hypothesis~1}. Preprocessing followed Section~\ref{subsec:global_preprocessing}.

\subsection{Mean Spectral Signatures of Healthy and Cracked Tissue}

\noindent
Mean spectral signatures ($\pm$\,SD) for healthy and cracked tissue are shown in Figure~\ref{fig:mean_signatures}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/spectral_signatures_mean_std.png}
    \caption{Mean spectral signatures ($\pm$ standard deviation) for healthy and cracked tissue across the VIS--NIR range.}
    \label{fig:mean_signatures}
\end{figure}

\FloatBarrier

\newpage

\subsection{Spectral Separability Across Wavelength Regions}

\noindent
Cohen's $d$~\cite{cohen1988statistical} ($d = |\mu_1 - \mu_2| / \sigma_{\mathrm{pooled}}$) and Fisher Score~\cite{duda2001pattern} (inter-class to intra-class variance ratio) were computed per band; results are shown in Figure~\ref{fig:spectral_separability}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/separability_metrics.png}
    \caption{Wavelength-wise spectral separability metrics between healthy and cracked tissue, including absolute mean difference, Cohen's $d$, and Fisher score.}
    \label{fig:spectral_separability}
\end{figure}

\FloatBarrier

\noindent
Separation is substantially higher in the NIR, with peak discriminability at 727--757\,nm. Regional VIS--NIR summaries are in Appendix~\ref{app:vis_nir_summary}.

\clearpage
\begin{landscape}
\thispagestyle{plain}

\section{Pixel-Level Binary Classification}
\label{sec:pixel-level-classification}

\noindent
Supervised pixel-level classification addresses \hyperref[hyp:pixel_level_feasibility]{Hypothesis~2}, following the LOGO cross-validation protocol described in Section~\ref{subsec:cv_strategy}. PR--AUC\textsubscript{CRACK} is the primary metric.

\graphicspath{{results/Binary_Pixel_Level_Classification/}}




\FloatBarrier
% =========================================================
% BOTH TABLES ON SAME LANDSCAPE PAGE (WITH VALUES)
% =========================================================


\noindent
Table~\ref{tab:pixel_metrics_unbalanced} reports performance across LOGO folds (Section~\ref{subsec:cv_strategy}; unbalanced class distribution). Balanced-training results are in Appendix~\ref{app:balanced_results}.

\medskip

% Binary pixel table (compact version)
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level classification metrics (unbalanced, binary setting). Truncated mean values. Full cross-validation statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:pixel_metrics_unbalanced}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Accuracy & Precision (Cracked) & Recall (Cracked) & Weighted F1 & ROC--AUC (Cracked) \\
        \midrule
        PLS-DA                    & 0.992 & 0.999 & 0.968 & 0.992 & 1.000 \\
        Logistic Regression (L1)  & 0.996 & 0.989 & 0.994 & 0.996 & 0.999 \\
        SVM (RBF)                 & 0.993 & 0.994 & 0.978 & 0.993 & 0.999 \\
        Random Forest            & 0.996 & 0.996 & 0.990 & 0.996 & 0.999 \\
        XGBoost                  & 0.994 & 0.995 & 0.979 & 0.994 & 1.000 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\medskip
\noindent
All five models achieve PR--AUC\textsubscript{CRACK} $\geq 0.987$ (Table~\ref{tab:pixel_metrics_unbalanced}), supporting \hyperref[hyp:pixel_level_feasibility]{Hypothesis~2}.

\end{landscape}

\clearpage
\begin{landscape}
\thispagestyle{plain}
\section{Pixel-Level Multi-Class Classification}
\label{sec:pixel-multiclass}

\noindent
Binary detection was extended to three labeling strategies (\textit{crack\_regular\_rest}, \textit{crack\_vs\_rest}, \textit{multi\_class}) as specified in Section~\ref{sec:modeling_evaluation}; PR--AUC\textsubscript{CRACK} is the primary metric. Balanced-training results are in Appendix~\ref{app:balanced_results}.

\begin{table}[h!]
    \centering
    \caption{Pixel-level classification metrics (unbalanced, multi-class). Truncated mean values. Full cross-validation statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:pixel_metrics_unbalanced_all}
    \small
    \renewcommand{\arraystretch}{1.10}
    \setlength{\tabcolsep}{3.5pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}llccccccc@{}}
        \toprule
        Label setup & Model & Accuracy & Balanced Acc. & Precision (Cracked) & Recall (Cracked) & Macro-F1 & ROC--AUC & PR--AUC \\
        \midrule
        \multicolumn{9}{@{}l@{}}{\textbf{crack\_regular\_rest}} \\
        & PLS-DA                & 0.508 & 0.687 & 0.882 & 0.753 & 0.581 & 0.997 & 0.863 \\
        & Logistic Regression (L1) & 0.853 & 0.868 & 0.965 & 0.966 & 0.801 & 0.999 & 0.983 \\
        & Random Forest         & 0.917 & 0.917 & 0.894 & 0.939 & 0.890 & 0.997 & 0.962 \\
        & XGBoost               & 0.921 & 0.916 & 0.980 & 0.949 & 0.883 & 0.999 & 0.990 \\
        & MLP (Small)           & 0.917 & 0.916 & 0.953 & 0.972 & 0.875 & 0.999 & 0.991 \\
        \addlinespace[4pt]
        \multicolumn{9}{@{}l@{}}{\textbf{crack\_vs\_rest}} \\
        & PLS-DA                & 0.512 & 0.694 & 0.884 & 0.762 & 0.589 & 0.997 & 0.866 \\
        & Logistic Regression (L1) & 0.857 & 0.870 & 0.966 & 0.965 & 0.804 & 0.999 & 0.984 \\
        & Random Forest         & 0.914 & 0.915 & 0.887 & 0.936 & 0.885 & 0.997 & 0.961 \\
        & XGBoost               & 0.920 & 0.915 & 0.980 & 0.950 & 0.882 & 0.999 & 0.990 \\
        & MLP (Small)           & 0.916 & 0.915 & 0.952 & 0.973 & 0.874 & 0.999 & 0.991 \\
        \addlinespace[4pt]
        \multicolumn{9}{@{}l@{}}{\textbf{multi\_class}} \\
        & PLS-DA                & 0.967 & 0.978 & 0.720 & 0.992 & 0.901 & 0.997 & 0.967 \\
        & Logistic Regression (L1) & 0.997 & 0.995 & 0.967 & 0.997 & 0.992 & 0.999 & 0.997 \\
        & Random Forest         & 0.996 & 0.994 & 0.967 & 0.996 & 0.990 & 0.999 & 0.997 \\
        & XGBoost               & 0.997 & 0.994 & 0.981 & 0.990 & 0.992 & 0.999 & 0.997 \\
        & MLP (Small)           & 0.990 & 0.989 & 0.924 & 0.986 & 0.969 & 0.996 & 0.989 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
The \textit{multi\_class} formulation yields the highest PR--AUC\textsubscript{CRACK} ($\geq 0.967$); LR, RF, and XGBoost all exceed 0.99 (Table~\ref{tab:pixel_metrics_unbalanced_all}).

\end{landscape}


\clearpage
\begin{landscape}
\thispagestyle{plain}
% ==========================================================
% Autoencoder-Based Anomaly Detection
% ==========================================================
\section{Autoencoder-Based Anomaly Detection}
\label{sec:anomaly-detection}

\noindent
In the one-class binary setting, the autoencoder achieved PR--AUC\textsubscript{CRACK} ranging from 0.925 to 0.970 (Table~\ref{tab:autoencoder_pixel_results}).

\medskip
\noindent
When trained on CRACK pixels as the normal class, PR--AUC\textsubscript{CRACK} $\approx 1.0$ (Table~\ref{tab:autoencoder_pixel_results_crack_only_multiclass}), indicating trivial reconstruction of CRACK signatures. Training on all non-CRACK classes with CRACK defined as the anomaly yielded PR--AUC\textsubscript{CRACK} $\approx 0.01$ (Table~\ref{tab:autoencoder_pixel_results_noncrack_multiclass}).

\medskip
\noindent
Supervised models outperform autoencoders for CRACK detection. Additional autoencoder formulations are reported in Appendix~\ref{app:cv_statistics}.

% =========================================================
% AUTOENCODER TABLES – Tables 1.3 & 1.4 on one landscape page
% =========================================================
% \clearpage
% \begin{landscape}
% \thispagestyle{plain}






\begin{table}[!htbp]
    \centering
    \small
    \caption{Pixel-level autoencoder anomaly detection (binary setting). Truncated mean values. Full statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:autoencoder_pixel_results}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{4pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Architecture & Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule
        Autoencoder (64--32--16) & 0.896 & 0.822 & 0.997 & 0.901 & 0.981 & 0.955 \\
        Autoencoder (128--64--32) & 0.859 & 0.754 & 0.997 & 0.862 & 0.967 & 0.925 \\
        Autoencoder (64--32--8) & 0.892 & 0.812 & 0.997 & 0.896 & 0.979 & 0.951 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}
\newpage
\vspace{-6pt}

\begin{table}[h!]
    \centering
    \small
    \caption{Pixel-level autoencoder anomaly detection (crack-only training). Truncated mean values. Full statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:autoencoder_pixel_results_crack_only_multiclass}
    \renewcommand{\arraystretch}{1.15}
    \setlength{\tabcolsep}{4pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Architecture & Accuracy & Precision (non-CRACK) & Recall (non-CRACK) & F1 (non-CRACK) & ROC--AUC (non-CRACK) & PR--AUC (non-CRACK) \\
        \midrule
        Autoencoder (64--32--16) & 0.966 & 0.999 & 0.966 & 0.982 & 0.985 & 0.999 \\
        Autoencoder (128--64--32) & 0.986 & 0.999 & 0.986 & 0.993 & 0.984 & 0.999 \\
        Autoencoder (64--32--8) & 0.962 & 0.999 & 0.963 & 0.980 & 0.983 & 0.999 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\vspace{-6pt}

\begin{table}[h!]
    \centering
    \small
    \caption{Pixel-level autoencoder anomaly detection (all non-CRACK training). Truncated mean values. Full statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:autoencoder_pixel_results_noncrack_multiclass}
    \renewcommand{\arraystretch}{1.15}
    \setlength{\tabcolsep}{4pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Architecture & Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule
        Autoencoder (64--32--16) & 0.996 & 0.009 & 0.111 & 0.017 & 0.457 & 0.008 \\
        Autoencoder (128--64--32) & 0.998 & 0.011 & 0.012 & 0.012 & 0.451 & 0.008 \\
        Autoencoder (64--32--8) & 0.998 & 0.020 & 0.031 & 0.025 & 0.454 & 0.011 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
Overall, autoencoder-based formulations yielded lower PR--AUC\textsubscript{CRACK} than the supervised XGBoost baseline and were therefore not adopted for subsequent pixel-level or whole-image detection stages.

\end{landscape}

\section{Whole-Image Crack Detection}
\label{sec:full-image-early-late}

\noindent
Using the protocol described in Section~\ref{subsec:full_image_pipeline}, whole-image detection was assessed under early- and late-stage conditions, addressing \hyperref[hyp:whole_image_robustness]{Hypothesis~3}. The primary metric is \textbf{F1\textsubscript{CRACK}}. The baseline for whole-image comparison is the full-spectrum XGBoost aggregation pipeline (159 bands, 450--925\,nm) without wavelength selection, as defined in Section~\ref{subsec:results_baselines}. Dataset composition is in Table~\ref{tab:full_image_dataset_composition}; supervised and autoencoder results are in Tables~\ref{tab:full_image_early_late_metrics} and~\ref{tab:full_image_autoencoder_metrics}, respectively.



% =========================================================
% FULL-IMAGE DATASET COMPOSITION TABLE (PRETTY)
% =========================================================
\begin{table}[htbp]
\centering
\caption{Dataset composition for full-image crack-detection experiments under early- and late-stage acquisition conditions (initial/minimal vs.\ advanced cracking).
Row~1 was used for calibration, while Row~2 served as an independent test set.}
\label{tab:full_image_dataset_composition}

\small
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}

\begin{tabular}{@{}lllcccc@{}}

\toprule
Row & Stage & Split & Total & Unique & Pos & Neg \\
\midrule
Row 1 & Early & Calibration & 172 & 60 & 32 & 140 \\
Row 1 & Late  & Calibration & 160 & 60 & 35 & 125 \\
\addlinespace[4pt]
Row 2 & Early & Test        & 60  & 60 & 32 & 28  \\
Row 2 & Late  & Test        & 60  & 60 & 34 & 26  \\
\bottomrule
\end{tabular}

\end{table}

% =========================================================
% FULL IMAGE EARLY vs LATE â€“ METRICS TABLE
% =========================================================
\begin{landscape}
\thispagestyle{plain}
\FloatBarrier

\begin{table}[!h]
\centering
\caption{Full-image crack detection performance for early- and late-stage acquisition conditions. F1 (Cracked) is the primary metric; the Cracked class is positive.}
\label{tab:full_image_early_late_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
34 & 5 & 120 & 1 &
0.963 & 0.966 & 0.872 & 0.971 & 0.919 & 0.950 & 0.960 & 0.992 & 0.897 \\

Late & Test &
27 & 5 & 21 & 7 &
0.800 & 0.801 & 0.844 & 0.794 & 0.818 & 0.804 & 0.808 & 0.750 & 0.598 \\

\addlinespace[4pt]

Early & Calibration &
23 & 6 & 134 & 9 &
0.913 & 0.838 & 0.793 & 0.719 & 0.754 & 0.732 & 0.957 & 0.937 & 0.703 \\

Early & Test &
19 & 3 & 25 & 13 &
0.733 & 0.743 & 0.864 & 0.594 & 0.704 & 0.633 & 0.893 & 0.658 & 0.504 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\noindent
On the independent test set (Table~\ref{tab:full_image_early_late_metrics}), under late-stage conditions the supervised pipeline achieves F1\textsubscript{CRACK} = 0.818. Under early-stage conditions F1\textsubscript{CRACK} = 0.704.

\FloatBarrier

\end{landscape}

% =========================================================
% FULL IMAGE AUTOENCODER â€“ METRICS TABLE
% =========================================================
\begin{landscape}
\thispagestyle{plain}
\FloatBarrier

\begin{table}[!h]
\centering
\caption{Full-image crack detection using autoencoder-based anomaly detection. F1 (Cracked) is the primary metric; the Cracked class is positive.}
\label{tab:full_image_autoencoder_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
26 & 7 & 118 & 9 &
0.900 & 0.843 & 0.788 & 0.743 & 0.765 & 0.751 & 0.944 & 0.929 & 0.702 \\

Late & Test &
22 & 7 & 19 & 12 &
0.683 & 0.689 & 0.759 & 0.647 & 0.698 & 0.667 & 0.731 & 0.613 & 0.375 \\

\addlinespace[4pt]

Early & Calibration &
19 & 17 & 123 & 13 &
0.826 & 0.736 & 0.528 & 0.594 & 0.559 & 0.579 & 0.879 & 0.904 & 0.452 \\

Early & Test &
13 & 13 & 15 & 19 &
0.467 & 0.471 & 0.500 & 0.406 & 0.448 & 0.422 & 0.536 & 0.441 & $-$0.058 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\noindent
On the independent test set (Table~\ref{tab:full_image_autoencoder_metrics}), the autoencoder-based pipeline yields lower F1\textsubscript{CRACK} than the supervised classifier in both stages (Late: F1\textsubscript{CRACK} = 0.698 vs. F1\textsubscript{CRACK} = 0.818; Early: F1\textsubscript{CRACK} = 0.448 vs. F1\textsubscript{CRACK} = 0.704).

\noindent
The effect of wavelength selection on the whole-image pipeline is evaluated in the following section using backward feature selection.

\FloatBarrier
\end{landscape}

% ------------------------------------------------------------
\section{Wavelength Selection via Backward Feature Selection}
\label{sec:bfs-setup}

\noindent
BFS was applied using the protocol described in Section~\ref{subsec:wavelength_selection}, optimizing PR--AUC\textsubscript{CRACK} on the whole-image pipeline. The full-spectrum baseline is 159 wavelengths (450--925\,nm).

\subsection{BFS Trajectories and Subset Selection}

\FloatBarrier

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/bfs_dual_metric.png}
    \caption{BFS trajectories for the whole-image pipeline: CRACK-class PR--AUC (primary objective) and CRACK-class F1 versus the number of retained wavelengths ($n$; full baseline $n{=}159$).}
    \label{fig:bfs_dual_metric}
\end{figure}

\begin{figure}[!htbp]
    \centering
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/results/feature_selection/bfs_f1_thresholds.png}
        \caption{CRACK-class F1 vs. number of wavelengths ($n$), with markers at the maximum and at 0.5\%/1.0\% drop points.}
        \label{fig:bfs_f1_thresholds}
    \end{subfigure}
    
    \vspace{0.5em}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/results/feature_selection/bfs_prauc_thresholds.png}
        \caption{CRACK-class PR--AUC (primary BFS objective) vs. number of wavelengths ($n$), with the same threshold markers.}
        \label{fig:bfs_prauc_thresholds}
    \end{subfigure}
    
    \caption{BFS threshold-marker visualization on CRACK metrics. Markers indicate the maximum and the first $n$ where performance drops more than 0.5\% and 1.0\% below that maximum.}
    \label{fig:bfs_threshold_markers}
\end{figure}

\begin{table}[!htbp]
\centering
\caption{Summary of BFS wavelength-count thresholds for CRACK metrics. (Full precision in Appendix~\ref{app:bfs_full_precision})}
\label{tab:bfs_thresholds_summary}
\begin{tabular}{lcccccc}
\toprule
Metric & Max score & $n_{\max}$ & 0.5\% drop: $n$ & score & 1.0\% drop: $n$ & score \\
\midrule
CRACK PR--AUC & 0.994 & 30  & 11  & 0.987 & 9  & 0.979 \\
CRACK F1      & 0.985 & 159 & 105 & 0.980 & 88 & 0.975 \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier


\FloatBarrier

\clearpage
\begin{landscape}

\subsection{End-to-End Wavelength-Selection Results}

\noindent
Table~\ref{tab:full_image_wavelength_results} reports whole-image detection metrics for representative wavelength subsets, evaluated separately for early- and late-stage acquisitions on the held-out test set.

\begin{table}[!htbp]
\centering
\caption{Full-image crack-detection performance for different wavelength subsets under early- and late-stage conditions. (Full precision in Appendix~\ref{app:wavelength_full_precision})}
\label{tab:full_image_wavelength_results}
\begin{tabular}{cccccccc}
\toprule
Stage & \# Wavelengths & Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & MCC \\
\midrule
Early & 9 & 0.783 & 0.790 & 0.880 & 0.687 & 0.771 & 0.587 \\
Late  & 9 & 0.800 & 0.805 & 0.866 & 0.764 & 0.812 & 0.605 \\
\midrule
Early & 11 & 0.750 & 0.757 & 0.840 & 0.656 & 0.736 & 0.519 \\
Late  & 11 & 0.816 & 0.811 & 0.828 & 0.852 & 0.840 & 0.625 \\
\midrule
Early & 30 & \textbf{0.816} & \textbf{0.821} & \textbf{0.888} & \textbf{0.750} & \textbf{0.813} & \textbf{0.644} \\
Late  & 30 & \textbf{0.833} & \textbf{0.834} & \textbf{0.875} & 0.823 & 0.848 & \textbf{0.665} \\
\midrule
Early & 159 & 0.800 & 0.803 & 0.857 & \textbf{0.750} & 0.800 & 0.607 \\
Late  & 159 & \textbf{0.833} & 0.825 & 0.833 & \textbf{0.882} & \textbf{0.857} & 0.659 \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

\noindent
The top-30 subset achieves F1\textsubscript{CRACK} = 0.8136 (Early) and 0.8485 (Late), matching the 159-band baseline in Early stage (0.8000) and approaching it in Late stage (0.8571); the top-11 subset yields F1\textsubscript{CRACK} = 0.7368 (Early) and 0.8406 (Late). Additional metrics and generalization-gap analysis are provided in Appendix~\ref{app:wavelength_details}.

\end{landscape}
\clearpage

\FloatBarrier

\subsection{BFS Wavelength-Selection Stability Analysis}
\label{sec:bfs-stability}

\noindent
Cross-seed wavelength overlap statistics are reported in Appendix~\ref{app:wavelength_details}.

\FloatBarrier

\subsubsection{Wavelength Popularity and Stability -- Top 30}

\noindent
The top-30 wavelength subsets demonstrate strong cross-seed consistency in their selection patterns, with dominant spectral regions remaining stable under coarser binning schemes. The top-30 subset exhibits substantially higher cross-seed stability compared to smaller subsets such as the top-11. The consistently selected regions are predominantly located in the near-infrared (NIR) portion of the spectrum. Figures~\ref{fig:stability_30_popularity}--\ref{fig:stability_30_spectrum} provide comprehensive visualization of this cross-seed wavelength stability.

\noindent
Exact wavelength identities and cross-seed overlap statistics are provided in Appendix~\ref{app:wavelength_details}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_30wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-30 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_30_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_30wl_wavelength_binning_comparison.png}
    \caption{Wavelength popularity at different bin sizes (1, 5, 10, 20\,nm) for the top-30 subsets.}
    \label{fig:stability_30_binning}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth,height=0.85\textheight,keepaspectratio]{figures/results/feature_selection/stability_varying_30wl_wavelength_stability_heatmap.png}
    \caption{Stability heatmap for the top-30 BFS subsets across five seeds (objective: PR--AUC\textsubscript{CRACK}). Each cell indicates how many seeds selected a wavelength within that 5\,nm bin.}
    \label{fig:stability_30_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_30wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-30 subsets.}
    \label{fig:stability_30_spectrum}
\end{figure}

\FloatBarrier

% â”€â”€ Top-11 WL Figures â”€â”€
\subsubsection{Wavelength Popularity and Stability -- Top 11}

\noindent
The top-11 wavelength subsets (within 1\% of peak PR--AUC\textsubscript{CRACK}) exhibit lower cross-seed stability than the top-30 subset, a consequence of the stronger dimensionality reduction. Nevertheless, consistent spectral regions remain identifiable across seeds (Figures~\ref{fig:stability_11_popularity}--\ref{fig:stability_11_spectrum}). Detailed overlap statistics are reported in Appendix~\ref{app:wavelength_details}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_11wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-11 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_11_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_11wl_wavelength_binning_comparison.png}
    \caption{Wavelength popularity at different bin sizes (1, 5, 10, 20\,nm) for the top-11 subsets.}
    \label{fig:stability_11_binning}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth,height=0.85\textheight,keepaspectratio]{figures/results/feature_selection/stability_varying_11wl_wavelength_stability_heatmap.png}
    \caption{Stability heatmap for the top-11 BFS subsets across five seeds (objective: PR--AUC\textsubscript{CRACK}; 0.5\% drop setting). Each cell indicates how many seeds selected a wavelength within that 5\,nm bin.}
    \label{fig:stability_11_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_11wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-11 subsets.}
    \label{fig:stability_11_spectrum}
\end{figure}

\FloatBarrier
