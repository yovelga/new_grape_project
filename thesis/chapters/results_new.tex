\chapter{Experimental Results}
\label{ch:results}

\section{Overview of Experimental Results}

\noindent
This chapter presents the experimental evaluation of the hyperspectral crack-detection framework. The findings are structured to systematically address the research hypotheses formulated in Chapter~3, progressing from fundamental spectral characterization to operational field-level detection. Specifically, the chapter is divided into four main sections: 

\begin{enumerate}
    \item Validation of the physical and spectral distinguishability between healthy and cracked tissues.
    \item Pixel-level classification benchmarking.
    \item Whole-image spatial aggregation and detection under early- and late-stage field conditions.
    \item Wavelength selection analysis to determine the minimal spectral bands required for robust inference.
\end{enumerate}

\medskip
\noindent
\textbf{Presentation Note:} For clarity and readability, the performance metrics reported in the main text and tables are presented as mean values truncated to three decimal places. Comprehensive cross-validation statistics, including standard deviations and fold-level data, are provided in Appendix~\ref{app:cv_statistics}.

\clearpage
\subsection{Evaluation Baselines}
\label{subsec:results_baselines}

\noindent
To ensure a rigorous and transparent evaluation, all experimental results are benchmarked against the following explicit baselines:

\begin{itemize}
    \item \textbf{Pixel-Level Classification Baseline:} The reference model is \textbf{XGBoost} trained on the \textbf{full SNV-normalized hyperspectral signature} (450--925\,nm, 159 contiguous bands), as defined in Section~\ref{subsec:pixel_level_framework}. Binary-task performance is reported in the XGBoost rows of Table~\ref{tab:pixel_metrics_unbalanced}. Results for extended labeling and balancing regimes appear in Table~\ref{tab:pixel_metrics_unbalanced_all}, with balanced-training baselines detailed in Appendix~\ref{app:balanced_results}.
    
    \item \textbf{Whole-Image Crack-Detection Baseline:} System-level detection is evaluated using the spatial aggregation pipeline described in Section~\ref{subsec:full_image_pipeline}. The baseline performance for this task relies on probability maps generated by the full-spectrum (159-band) XGBoost pixel-level model, establishing the upper bound for detection accuracy prior to any wavelength reduction. Early- and late-stage calibration and held-out test performances are summarized in Table~\ref{tab:full_image_early_late_metrics}.
    
    \item \textbf{Wavelength Selection (BFS) Reference:} The efficacy of Backward Feature Selection (BFS) is quantified by comparing the classification performance of reduced wavelength subsets directly against the full 159-band baseline. Elimination trajectories are illustrated in Figures~\ref{fig:bfs_dual_metric} and~\ref{fig:bfs_threshold_markers}, and whole-image test-set results at critical wavelength thresholds are reported in Table~\ref{tab:full_image_wavelength_results}.
\end{itemize}

\newpage
\section{Spectral Separability and Informative Wavelengths}
\label{sec:spectral-characterization}

\noindent
Prior to evaluating machine learning algorithms, a per-band spectral separability analysis was conducted to verify that healthy and cracked tissues exhibit physically distinguishable signatures, addressing \hyperref[hyp:spectral_distinguishability]{Hypothesis~1}. For this fundamental physical characterization, the relative reflectance spectra were analyzed \textit{prior} to the application of per-pixel SNV normalization in order to preserve and quantify the absolute amplitude differences between the classes.

\subsection{Mean Spectral Signatures of Healthy and Cracked Tissue}

\noindent
The mean spectral reflectance signatures ($\pm$\,SD) for healthy and cracked grape tissues across the VIS--NIR range are presented in Figure~\ref{fig:mean_signatures}. Cracked tissue consistently exhibits lower absolute reflectance across the full spectrum compared to intact healthy tissue. As visually evident in the plot, this divergence is exceptionally pronounced in the Near-Infrared (NIR) region, specifically between 700\,nm and 850\,nm.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/spectral_signatures_mean_std.png}
    \caption{Mean spectral signatures ($\pm$ standard deviation) for healthy and cracked tissue across the VIS--NIR range.}
    \label{fig:mean_signatures}
\end{figure}

\FloatBarrier

\newpage

\subsection{Spectral Separability Across Wavelength Regions}

\noindent
To systematically quantify the discriminatory power of individual spectral bands, three complementary metrics were computed (Figure~\ref{fig:spectral_separability}): \textbf{Absolute Mean Difference ($|\Delta\mu|$)} to capture the raw reflectance gap, \textbf{Cohen's $d$~\cite{cohen1988statistical}} to measure the standardized effect size, and the \textbf{Fisher Score~\cite{duda2001pattern}} to evaluate the ratio of inter-class separation to intra-class spread.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/results/spectral_signatures/separability_metrics.png}
    \caption{Wavelength-wise spectral separability metrics between healthy and cracked tissue, including absolute mean difference, Cohen's $d$, and Fisher score.}
    \label{fig:spectral_separability}
\end{figure}

\FloatBarrier

\noindent
The separability metrics indicate that class separation is substantially higher in the NIR region, with peak discriminability consistently observed between 727--757\,nm. As detailed in the regional summaries in Appendix~\ref{app:vis_nir_summary}, healthy tissue reflectance exceeds that of cracked tissue by an average of 32\% in the NIR and 37\% in the VIS range. These clear physical differences confirm \hyperref[hyp:spectral_distinguishability]{Hypothesis~1} and provide the basis for automated classification.
\clearpage
\begin{landscape}
\thispagestyle{plain}

\section{Pixel-Level Binary Classification}
\label{sec:pixel-level-classification}

\noindent
Supervised pixel-level classification addresses \hyperref[hyp:pixel_level_feasibility]{Hypothesis~2}, following the LOGO cross-validation protocol described in Section~\ref{subsec:cv_strategy}. PR--AUC\textsubscript{CRACK} is the primary metric.

\graphicspath{{results/Binary_Pixel_Level_Classification/}}




\FloatBarrier
% =========================================================
% BOTH TABLES ON SAME LANDSCAPE PAGE (WITH VALUES)
% =========================================================


\noindent
Table~\ref{tab:pixel_metrics_unbalanced} reports performance across LOGO folds (Section~\ref{subsec:cv_strategy}; unbalanced class distribution). Balanced-training results are in Appendix~\ref{app:balanced_results}.

\medskip

% Binary pixel table (compact version)
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level classification metrics (unbalanced, binary setting). Truncated mean values. Full cross-validation statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:pixel_metrics_unbalanced}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Accuracy & Precision (Cracked) & Recall (Cracked) & Weighted F1 & ROC--AUC (Cracked) \\
        \midrule
        PLS-DA                    & 0.992 & 0.999 & 0.968 & 0.992 & 1.000 \\
        Logistic Regression (L1)  & 0.996 & 0.989 & 0.994 & 0.996 & 0.999 \\
        SVM (RBF)                 & 0.993 & 0.994 & 0.978 & 0.993 & 0.999 \\
        Random Forest            & 0.996 & 0.996 & 0.990 & 0.996 & 0.999 \\
        XGBoost                  & 0.994 & 0.995 & 0.979 & 0.994 & 1.000 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\medskip
\noindent
All five models achieve PR--AUC\textsubscript{CRACK} $\geq 0.987$ (Table~\ref{tab:pixel_metrics_unbalanced}), supporting \hyperref[hyp:pixel_level_feasibility]{Hypothesis~2}.

\end{landscape}

\clearpage
\begin{landscape}
\thispagestyle{plain}
\section{Pixel-Level Multi-Class Classification}
\label{sec:pixel-multiclass}

\noindent
While binary classification validates baseline spectral separability, operational field detection requires robustness against severe background clutter. To evaluate this, the detection task was expanded to a 10-class taxonomy encompassing diverse vineyard elements (Section~\ref{subsec:pixel_level_framework}). Performance was assessed across three labeling strategies: \textit{crack\_regular\_rest}, \textit{crack\_vs\_rest}, and \textit{multi\_class}. Balanced-training results are detailed in Appendix~\ref{app:balanced_results}.

\begin{table}[h!]
    \centering
    \caption{Pixel-level classification metrics (unbalanced, multi-class). Truncated mean values. Full cross-validation statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:pixel_metrics_unbalanced_all}
    \small
    \renewcommand{\arraystretch}{1.10}
    \setlength{\tabcolsep}{3.5pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}llccccccc@{}}
        \toprule
        Label setup & Model & Accuracy & Balanced Acc. & Precision (Cracked) & Recall (Cracked) & Macro-F1 & ROC--AUC & PR--AUC \\
        \midrule
        % \multicolumn{9}{@{}l@{}}{\textbf{crack\_regular\_rest}} \\
        % & PLS-DA                & 0.508 & 0.687 & 0.882 & 0.753 & 0.581 & 0.997 & 0.863 \\
        % & Logistic Regression (L1) & 0.853 & 0.868 & 0.965 & 0.966 & 0.801 & 0.999 & 0.983 \\
        % & Random Forest         & 0.917 & 0.917 & 0.894 & 0.939 & 0.890 & 0.997 & 0.962 \\
        % & XGBoost               & 0.921 & 0.916 & 0.980 & 0.949 & 0.883 & 0.999 & 0.990 \\
        % & MLP (Small)           & 0.917 & 0.916 & 0.953 & 0.972 & 0.875 & 0.999 & 0.991 \\
        \addlinespace[4pt]
        \multicolumn{9}{@{}l@{}}{\textbf{crack\_vs\_rest}} \\
        & PLS-DA                & 0.512 & 0.694 & 0.884 & 0.762 & 0.589 & 0.997 & 0.866 \\
        & Logistic Regression (L1) & 0.857 & 0.870 & 0.966 & 0.965 & 0.804 & 0.999 & 0.984 \\
        & Random Forest         & 0.914 & 0.915 & 0.887 & 0.936 & 0.885 & 0.997 & 0.961 \\
        & XGBoost               & 0.920 & 0.915 & 0.980 & 0.950 & 0.882 & 0.999 & 0.990 \\
        & MLP (Small)           & 0.916 & 0.915 & 0.952 & 0.973 & 0.874 & 0.999 & 0.991 \\
        \addlinespace[4pt]
        \multicolumn{9}{@{}l@{}}{\textbf{multi\_class}} \\
        & PLS-DA                & 0.967 & 0.978 & 0.720 & 0.992 & 0.901 & 0.997 & 0.967 \\
        & Logistic Regression (L1) & 0.997 & 0.995 & 0.967 & 0.997 & 0.992 & 0.999 & 0.997 \\
        & Random Forest         & 0.996 & 0.994 & 0.967 & 0.996 & 0.990 & 0.999 & 0.997 \\
        & XGBoost               & 0.997 & 0.994 & 0.981 & 0.990 & 0.992 & 0.999 & 0.997 \\
        & MLP (Small)           & 0.990 & 0.989 & 0.924 & 0.986 & 0.969 & 0.996 & 0.989 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
The results demonstrate that supervised models maintain high discriminative power even in the presence of complex background materials. The \textit{multi\_class} formulation yielded the highest overall performance, with Logistic Regression, Random Forest, and XGBoost all exceeding a PR--AUC\textsubscript{CRACK} of 0.990 (Table~\ref{tab:pixel_metrics_unbalanced_all}). This confirms that crack signatures remain distinct from background clutter, justifying the use of the trained multi-class XGBoost model as the core engine for the whole-image aggregation pipeline.

\end{landscape}


\clearpage
\begin{landscape}
\thispagestyle{plain}
% ==========================================================
% Autoencoder-Based Anomaly Detection
% ==========================================================
\section{Autoencoder-Based Anomaly Detection}
\label{sec:anomaly-detection}

\noindent
To explore whether crack damage could be identified without the need for exhaustive pixel-level annotation, an unsupervised anomaly detection framework was evaluated (Section~\ref{subsec:anomaly_detection}).

\begin{table}[!htbp]
    \centering
    \small
    \caption{Pixel-level autoencoder anomaly detection (binary setting). Truncated mean values. Full statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:autoencoder_pixel_results}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{4pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Architecture & Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule
        Autoencoder (64--32--16) & 0.896 & 0.822 & 0.997 & 0.901 & 0.981 & 0.955 \\
        Autoencoder (128--64--32) & 0.859 & 0.754 & 0.997 & 0.862 & 0.967 & 0.925 \\
        Autoencoder (64--32--8) & 0.892 & 0.812 & 0.997 & 0.896 & 0.979 & 0.951 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\vspace{-6pt}

\begin{table}[h!]
    \centering
    \small
    \caption{Pixel-level autoencoder anomaly detection (crack-only training). Truncated mean values. Full statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:autoencoder_pixel_results_crack_only_multiclass}
    \renewcommand{\arraystretch}{1.15}
    \setlength{\tabcolsep}{4pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Architecture & Accuracy & Precision (non-CRACK) & Recall (non-CRACK) & F1 (non-CRACK) & ROC--AUC (non-CRACK) & PR--AUC (non-CRACK) \\
        \midrule
        Autoencoder (64--32--16) & 0.966 & 0.999 & 0.966 & 0.982 & 0.985 & 0.999 \\
        Autoencoder (128--64--32) & 0.986 & 0.999 & 0.986 & 0.993 & 0.984 & 0.999 \\
        Autoencoder (64--32--8) & 0.962 & 0.999 & 0.963 & 0.980 & 0.983 & 0.999 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\vspace{-6pt}

\begin{table}[h!]
    \centering
    \small
    \caption{Pixel-level autoencoder anomaly detection (all non-CRACK training). Truncated mean values. Full statistics in Appendix~\ref{app:cv_statistics}.}
    \label{tab:autoencoder_pixel_results_noncrack_multiclass}
    \renewcommand{\arraystretch}{1.15}
    \setlength{\tabcolsep}{4pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Architecture & Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule
        Autoencoder (64--32--16) & 0.996 & 0.009 & 0.111 & 0.017 & 0.457 & 0.008 \\
        Autoencoder (128--64--32) & 0.998 & 0.011 & 0.012 & 0.012 & 0.451 & 0.008 \\
        Autoencoder (64--32--8) & 0.998 & 0.020 & 0.031 & 0.025 & 0.454 & 0.011 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
In the one-class binary setting, the autoencoder achieved PR--AUC\textsubscript{CRACK} ranging from 0.925 to 0.955 (Table~\ref{tab:autoencoder_pixel_results}). However, expanding this to multi-class environments proved ineffective: training exclusively on CRACK pixels resulted in trivial reconstruction (Table~\ref{tab:autoencoder_pixel_results_crack_only_multiclass}), while training on all non-CRACK classes collapsed performance (PR--AUC\textsubscript{CRACK} $\approx 0.01$, Table~\ref{tab:autoencoder_pixel_results_noncrack_multiclass}).

\medskip
\noindent
Overall, autoencoder-based formulations yielded substantially lower precision-recall performance at the pixel level compared to the supervised XGBoost baseline. Despite this performance gap, the best-performing binary autoencoder was carried forward to the whole-image detection stage to serve as an unsupervised system-level baseline. However, due to its superior discriminative power, the supervised multi-class XGBoost model was selected as the primary engine for the core evaluations and all subsequent wavelength selection (BFS) experiments.
\end{landscape}

\section{Whole-Image Crack Detection}
\label{sec:full-image-early-late}

\noindent
Having established the core predictive engine at the pixel level, the evaluation transitions to the primary operational objective: detecting cracking within full spatial scenes acquired under field conditions. This section specifically addresses \hyperref[hyp:whole_image_robustness]{Hypothesis~3} (robustness to background clutter) and \hyperref[hyp:early_detection]{Hypothesis~4} (early-stage detection capability). System performance was evaluated using the spatial aggregation pipeline (Section~\ref{subsec:full_image_pipeline}) and stratified by early- and late-stage acquisitions. The dataset composition for this phase is detailed in Table~\ref{tab:full_image_dataset_composition}.

\subsection{Supervised Detection Performance}

\noindent
The baseline for whole-image comparison is the full-spectrum XGBoost aggregation pipeline (159 bands). Table~\ref{tab:full_image_early_late_metrics} presents the detection metrics on the spatially held-out Row 2 test set, compared against the Row 1 calibration set.

% =========================================================
% FULL-IMAGE DATASET COMPOSITION TABLE (PRETTY)
% =========================================================
\begin{table}[htbp]
\centering
\caption{Dataset composition for full-image crack-detection experiments under early- and late-stage acquisition conditions (initial/minimal vs.\ advanced cracking). Row~1 was used for calibration, while Row~2 served as an independent test set.}
\label{tab:full_image_dataset_composition}

\small
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}

\begin{tabular}{@{}lllcccc@{}}

\toprule
Row & Stage & Split & Total & Unique & Pos & Neg \\
\midrule
Row 1 & Early & Calibration & 172 & 60 & 32 & 140 \\
Row 1 & Late  & Calibration & 160 & 60 & 35 & 125 \\
\addlinespace[4pt]
Row 2 & Early & Test        & 60  & 60 & 32 & 28  \\
Row 2 & Late  & Test        & 60  & 60 & 34 & 26  \\
\bottomrule
\end{tabular}

\end{table}

% =========================================================
% FULL IMAGE EARLY vs LATE -- METRICS TABLE
% =========================================================
\begin{landscape}
\thispagestyle{plain}
\FloatBarrier

\begin{table}[!h]
\centering
\caption{Full-image crack detection performance for early- and late-stage acquisition conditions. F1 (Cracked) is the primary metric; the Cracked class is positive.}
\label{tab:full_image_early_late_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
34 & 5 & 120 & 1 &
0.963 & 0.966 & 0.872 & 0.971 & 0.919 & 0.950 & 0.960 & 0.992 & 0.897 \\

Late & Test &
27 & 5 & 21 & 7 &
0.800 & 0.801 & 0.844 & 0.794 & 0.818 & 0.804 & 0.808 & 0.750 & 0.598 \\

\addlinespace[4pt]

Early & Calibration &
23 & 6 & 134 & 9 &
0.913 & 0.838 & 0.793 & 0.719 & 0.754 & 0.732 & 0.957 & 0.937 & 0.703 \\

Early & Test &
19 & 3 & 25 & 13 &
0.733 & 0.743 & 0.864 & 0.594 & 0.704 & 0.633 & 0.893 & 0.658 & 0.504 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\noindent
The results on the independent test set demonstrate robust system-level performance. Under late-stage conditions (advanced cracking), the supervised pipeline achieved a high F1\textsubscript{CRACK} score of 0.818. More importantly, under early-stage conditions--where cracks are minimal and visually indistinct--the system maintained strong predictive capability with an F1\textsubscript{CRACK} of 0.704. The expected performance drop between late and early stages reflects the physical difficulty of detecting incipient micro-cracks. Nevertheless, these results confirm both \hyperref[hyp:whole_image_robustness]{Hypothesis~3} and \hyperref[hyp:early_detection]{Hypothesis~4}: the spatial aggregation logic successfully suppresses background noise, enabling reliable early-warning triggers on entirely unseen vines.

\FloatBarrier

\end{landscape}

% =========================================================
% FULL IMAGE AUTOENCODER -- METRICS TABLE
% =========================================================
\begin{landscape}
\thispagestyle{plain}
\FloatBarrier

\subsection{Unsupervised Baseline Comparison}

\noindent
To verify whether the supervised XGBoost engine was strictly necessary for whole-image inference, the best-performing unsupervised autoencoder was evaluated using the exact same spatial aggregation pipeline (Table~\ref{tab:full_image_autoencoder_metrics}).

\begin{table}[!h]
\centering
\caption{Full-image crack detection using autoencoder-based anomaly detection. F1 (Cracked) is the primary metric; the Cracked class is positive.}
\label{tab:full_image_autoencoder_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
26 & 7 & 118 & 9 &
0.900 & 0.843 & 0.788 & 0.743 & 0.765 & 0.751 & 0.944 & 0.929 & 0.702 \\

Late & Test &
22 & 7 & 19 & 12 &
0.683 & 0.689 & 0.759 & 0.647 & 0.698 & 0.667 & 0.731 & 0.613 & 0.375 \\

\addlinespace[4pt]

Early & Calibration &
19 & 17 & 123 & 13 &
0.826 & 0.736 & 0.528 & 0.594 & 0.559 & 0.579 & 0.879 & 0.904 & 0.452 \\

Early & Test &
13 & 13 & 15 & 19 &
0.467 & 0.471 & 0.500 & 0.406 & 0.448 & 0.422 & 0.536 & 0.441 & $-$0.058 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\noindent
The autoencoder-based pipeline yielded substantially lower performance across both stages on the test set (Late: F1\textsubscript{CRACK} = 0.698; Early: F1\textsubscript{CRACK} = 0.448). The sharp degradation in early-stage detection highlights that unsupervised reconstruction error is insufficiently sensitive to the subtle spectral shifts of incipient cracking. Consequently, the supervised XGBoost model is confirmed as the required foundation for the system, and is solely utilized in the subsequent wavelength selection experiments.

\FloatBarrier
\end{landscape}
% ------------------------------------------------------------
\section{Wavelength Selection via Backward Feature Selection}
\label{sec:bfs-setup}

\noindent
Having demonstrated robust whole-image detection using the full 159-band spectrum, the final evaluation phase addresses \hyperref[hyp:spectral_efficiency]{Hypothesis~5}: whether comparable performance can be maintained using a significantly reduced subset of spectral bands. To explore this, Backward Feature Selection (BFS) was applied to the pixel-level XGBoost model using the protocol described in Section~\ref{subsec:wavelength_selection}, with the elimination trajectory guided by the PR--AUC\textsubscript{CRACK} metric.

\subsection{BFS Trajectories and Subset Selection}

\noindent
The elimination trajectories for both the primary objective (PR--AUC$_{\text{CRACK}}$) and the corresponding F1 score are illustrated in Figure~\ref{fig:bfs_dual_metric}. The curves illustrate a high degree of spectral redundancy; performance remains remarkably stable even as the majority of the original 159 wavelengths are discarded. This suggests that the key physiological markers of grape cracking are tightly concentrated within specific spectral regions rather than distributed across the entire contiguous spectrum.

To systematically define candidate subsets for the whole-image pipeline, performance drop thresholds of 0.5\% and 1.0\% below the maximum PR--AUC score were established (Figure~\ref{fig:bfs_threshold_markers}). As summarized in Table~\ref{tab:bfs_thresholds_summary}, the more conservative 0.5\% tolerance yields a 30-wavelength subset, while the 1.0\% tolerance isolates an extremely compact 11-wavelength subset.

\FloatBarrier

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/bfs_dual_metric.png}
    \caption{BFS trajectories for the whole-image pipeline: CRACK-class PR--AUC (primary objective) and CRACK-class F1 versus the number of retained wavelengths ($n$; full baseline $n=159$).}
    \label{fig:bfs_dual_metric}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/results/feature_selection/bfs_f1_thresholds.png}
        \caption{CRACK-class F1 vs. number of wavelengths ($n$), with markers at the maximum and at 0.5\%/1.0\% drop points.}
        \label{fig:bfs_f1_thresholds}
    \end{subfigure}
    \vspace{0.5em}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/results/feature_selection/bfs_prauc_thresholds.png}
        \caption{CRACK-class PR--AUC (primary BFS objective) vs. number of wavelengths ($n$), with the same threshold markers.}
        \label{fig:bfs_prauc_thresholds}
    \end{subfigure}
    \caption{BFS threshold-marker visualization on CRACK metrics. Markers indicate the maximum and the first $n$ where performance drops more than 0.5\% and 1.0\% below that maximum.}
    \label{fig:bfs_threshold_markers}
\end{figure}

\begin{table}[!htbp]
\centering
\caption{Summary of BFS wavelength-count thresholds for CRACK metrics (Full precision in Appendix~\ref{app:bfs_full_precision}).}
\label{tab:bfs_thresholds_summary}
\begin{tabular}{lcccccc}
\toprule
Metric & Max score & $n_{\max}$ & 0.5\% drop: $n$ & score & 1.0\% drop: $n$ & score \\
\midrule
CRACK PR--AUC & 0.994 & 30  & 11  & 0.987 & 9   & 0.979 \\
CRACK F1      & 0.985 & 159 & 105 & 0.980 & 88  & 0.975 \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

\clearpage
\begin{landscape}

\subsection{End-to-End Wavelength-Selection Results}

\noindent
To validate the operational efficacy of these reduced subsets, the top-30 and top-11 configurations were integrated into the whole-image aggregation pipeline and evaluated on the held-out Row 2 test set. Table~\ref{tab:full_image_wavelength_results} compares their performance against the full 159-band baseline under both early- and late-stage conditions.

\begin{table}[!htbp]
\centering
\caption{Full-image crack-detection performance for different wavelength subsets under early- and late-stage conditions.}
\label{tab:full_image_wavelength_results}
\begin{tabular}{cccccccc}
\toprule
Stage & \# Wavelengths & Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & MCC \\
\midrule
Early & 9 & 0.783 & 0.790 & 0.880 & 0.687 & 0.771 & 0.587  \\
Late  & 9 & 0.800 & 0.805 & 0.866 & 0.764 & 0.812 & 0.605  \\
\midrule
Early & 11 & 0.750 & 0.757 & 0.840 & 0.656 & 0.736 & 0.519  \\
Late  & 11 & 0.816 & 0.811 & 0.828 & 0.852 & 0.840 & 0.625  \\
\midrule
Early & 30 & \textbf{0.816} & \textbf{0.821} & \textbf{0.888} & \textbf{0.750} & \textbf{0.813} & \textbf{0.644}  \\
Late  & 30 & \textbf{0.833} & \textbf{0.834} & \textbf{0.875} & 0.823 & 0.848 & \textbf{0.665}  \\
\midrule
Early & 159 & 0.800 & 0.803 & 0.857 & \textbf{0.750} & 0.800 & 0.607  \\
Late  & 159 & \textbf{0.833} & 0.825 & 0.833 & \textbf{0.882} & \textbf{0.857} & 0.659  \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

\noindent
The results strongly confirm \hyperref[hyp:spectral_efficiency]{Hypothesis~5}. Notably, the top-30 subset achieves an F1\textsubscript{CRACK} of 0.813 under early-stage conditions, effectively matching and even slightly exceeding the 159-band baseline ($F1{=}0.800$). This slight improvement suggests that removing noisy or redundant spectral bands can actually enhance model generalization. Furthermore, even the extremely compact top-11 subset maintains robust late-stage detection ($F1_{CRACK} = 0.840$), demonstrating that the core discriminative information is preserved despite a 93\% reduction in spectral dimensionality.

\end{landscape}
\clearpage

\FloatBarrier

\subsection{BFS Wavelength-Selection Stability Analysis}
\label{sec:bfs-stability}

\noindent
Beyond raw detection performance, the physical reliability of the selected bands was evaluated through a cross-seed stability analysis. By varying the train-test data split across five random seeds, the spatial and spectral consistency of the selected subsets was quantified. 

\subsubsection{Wavelength Popularity and Stability}

\noindent
The top-30 wavelength subsets demonstrated strong cross-seed consistency, predominantly converging on specific spectral regions in the near-infrared (NIR) portion of the spectrum (Figures~\ref{fig:stability_30_popularity} and \ref{fig:stability_30_spectrum}). This physical consistency is maintained even under coarser binning schemes, indicating that the selection algorithm successfully targets robust tissue characteristics rather than overfitting to dataset-specific noise. Detailed binning and heatmap visualizations for the top-30 subsets are provided in Appendix~\ref{app:wavelength_details}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_30wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-30 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_30_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{figures/results/feature_selection/stability_varying_30wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-30 subsets.}
    \label{fig:stability_30_spectrum}
\end{figure}

\FloatBarrier

\noindent
As expected with aggressive dimensionality reduction, the top-11 subsets exhibited more variance across seeds. This occurs because multiple highly correlated adjacent bands often act as interchangeable proxies for the same underlying physical trait. Nevertheless, consistent spectral regions remain clearly identifiable, supporting the feasibility of targeted multispectral sensor design. (Detailed popularity, binning, heatmap, and frequency spectrum visualizations for the top-11 subsets are provided in Appendix~\ref{app:wavelength_details}).

\FloatBarrier

\noindent
Overall, the BFS procedure revealed a highly consistent selection pattern, heavily concentrated in the NIR region (700--850\,nm). Specific anchors, such as 730\,nm and 750\,nm, were repeatedly identified as primary contributors to model performance, perfectly aligning with the peak spectral separability metrics established earlier in this chapter. Even under the extreme constraints of the 11-wavelength subset, the selection reliably converged on key markers like 452\,nm (visible blue) and 729\,nm (NIR-edge) across diverse data splits. While these findings definitively validate the statistical importance of these specific bands for robust crack detection, a deeper interpretation of their physiological relevance and biomechanical implications is reserved for the Discussion (Chapter~\ref{ch:discussion}).