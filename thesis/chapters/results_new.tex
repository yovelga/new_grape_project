\chapter{Experimental Results}
\label{ch:results}

\section{Overview of Experimental Results}

\noindent
This chapter presents the quantitative results obtained from the experimental evaluation of hyperspectral-based crack detection and early-warning screening methods. The results are organized to follow the logical progression of the experimental pipeline, beginning with spectral characterization and continuing through pixel-level then to whole-image modeling, followed by wavelength-selection analyses.



\subsection{Baselines}
\label{subsec:results_baselines}

\noindent
To support consistent comparisons across tasks, each set of results in this chapter is reported relative to an explicit baseline configuration:

\begin{itemize}
    \item \textbf{Pixel-level classification baseline.} The baseline classifier is \textbf{XGBoost} trained on the \textbf{full hyperspectral signature} (450--925\,nm) after \textbf{SNV normalization}. Baseline performance for the binary task is reported in the XGBoost rows of Table~\ref{tab:pixel_metrics_unbalanced} and Table~\ref{tab:pixel_metrics_balanced}. For the extended labeling/balancing regimes, the corresponding XGBoost baselines appear in Table~\ref{tab:pixel_metrics_unbalanced_all} and Table~\ref{tab:pixel_metrics_balanced_all}.

    \item \textbf{Whole-image crack-detection baseline.} The baseline whole-image system uses \textbf{pixel-level model outputs} as inputs to the same \textbf{spatial post-processing and aggregation pipeline} (thresholding, morphological filtering, patch-level aggregation, and image-level decision logic), \textbf{without any wavelength selection}. Its early/late calibration and held-out test performance is summarized in Table~\ref{tab:full_image_early_late_metrics} (with dataset composition in Table~\ref{tab:full_image_dataset_composition}).

    \item \textbf{BFS wavelength-selection reference.} BFS results are reported as \textbf{performance vs. number of selected wavelengths} relative to the \textbf{full-spectrum baseline} (159 wavelengths within 450--925\,nm). The performance trajectories are shown in Figure~\ref{fig:bfs_dual_metric} and Figure~\ref{fig:bfs_threshold_markers}, and the derived wavelength-count thresholds are summarized in Table~\ref{tab:bfs_thresholds_summary}. Whole-image test-set performance at representative wavelength counts (including the full-spectrum baseline at 159 wavelengths) is reported in Table~\ref{tab:full_image_wavelength_results}.
\end{itemize}

\section{Spectral Signature Characterization}
\label{sec:spectral-characterization}

\noindent
This section examines the spectral properties of grape tissue to establish whether measurable differences exist between healthy and cracked samples prior to any modeling stage.

\subsection{Mean Spectral Signatures of Healthy and Cracked Tissue}

\noindent
Mean spectral signatures were computed for pixels sampled from healthy and cracked grape tissue across the full VIS--NIR range. 
The analysis focuses on class-level spectral trends and variability, without fitting any classification model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/spectral_signatures_mean_std.png}
    \caption{Mean spectral signatures ($\pm$ standard deviation) for healthy and cracked tissue across the VIS--NIR range.}
    \label{fig:mean_signatures}
\end{figure}

\FloatBarrier

\noindent
To provide a compact quantitative summary, reflectance values were aggregated over two broad spectral regions corresponding to the visible (VIS) and near-infrared (NIR) ranges.

\FloatBarrier

\begin{table}[htbp]
    \centering
    \caption{Summary statistics of reflectance values across spectral regions for healthy and cracked tissue.}
    \label{tab:reflectance_stats}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Spectral Region & Class & Mean Reflectance & Std. Dev. & Median \\
        \midrule
        VIS (400--700\,nm) & Healthy & 0.218 & 0.128 & 0.186 \\
                          & Cracked & 0.137 & 0.076 & 0.123 \\
        NIR (700--1000\,nm) & Healthy & 0.660 & 0.114 & 0.630 \\
                           & Cracked & 0.450 & 0.121 & 0.425 \\
        \bottomrule
    \end{tabular}
\end{table}

\FloatBarrier
\newpage

\subsection{Spectral Separability Across Wavelength Regions}

\noindent
This subsection quantifies the degree of spectral separation between healthy and cracked tissue across predefined wavelength regions.
The goal is to identify spectral ranges exhibiting the strongest inter-class contrast, without invoking classification models.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/separability_metrics.png}
    \caption{Wavelength-wise spectral separability metrics between healthy and cracked tissue, including absolute mean difference, Cohen's $d$, and Fisher score.}
    \label{fig:spectral_separability}
\end{figure}

\noindent
To facilitate regional-level comparison, the wavelength-wise separability measures were aggregated over predefined spectral regions.

\begin{table}[htbp]
    \centering
    \caption{Regional spectral separability metrics between healthy and cracked tissue.
    Mean values are averaged across wavelengths within each range, while peak values
    indicate the maximum separability and the corresponding wavelength.}
    \label{tab:separability_scores}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Metric & VIS Range & NIR Range & Full Spectrum \\
        \midrule
        Mean $|\Delta \mu|$ (Abs. Mean Diff.) 
            & 0.0810 & 0.2101 & 0.1422 \\
        Max $|\Delta \mu|$ (nm) 
            & 0.2325 (708.6) & 0.3704 (726.5) & 0.3704 (726.5) \\
        \midrule
        Mean Cohen's $d$ 
            & 0.6330 & 1.6217 & 1.1014 \\
        Max Cohen's $d$ (nm) 
            & 1.1910 (699.5) & 2.6877 (756.6) & 2.6877 (756.6) \\
        \midrule
        Mean Fisher Score 
            & 0.2895 & 1.6590 & 0.9436 \\
        Max Fisher Score (nm) 
            & 0.8821 (694.5) & 3.6311 (747.5) & 3.6311 (747.5) \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/results/spectral_signatures/regional_comparison.png}
    \caption{Regional visual summary of mean reflectance ($\pm$1 STD) and absolute mean difference between healthy and cracked tissue in the VIS and NIR ranges. This figure provides an intuitive visualization of the regional trends quantified in Table~\ref{tab:separability_scores}.}
    \label{fig:regional_summary}
\end{figure}

\FloatBarrier

\medskip
\noindent
Figures~\ref{fig:spectral_separability} and \ref{fig:regional_summary}, together with Table~\ref{tab:separability_scores}, quantify wavelength-wise and regional separability between healthy and cracked tissue without invoking a classifier.
The next section evaluates whether supervised learning models can exploit these measured differences to discriminate cracked pixels under the group-aware protocol.
Binary and extended labeling results are reported in Tables~\ref{tab:pixel_metrics_unbalanced}--\ref{tab:pixel_metrics_balanced} and Tables~\ref{tab:pixel_metrics_unbalanced_all}--\ref{tab:pixel_metrics_balanced_all}.


\section{Pixel-Level Classification Performance}
\label{sec:pixel-level-classification}

\noindent
This section reports the performance of supervised pixel-level classification models trained on hyperspectral signatures.

\subsection{Experimental Setup and Evaluation Protocol}

\noindent
Pixel-level classification performance was evaluated using a cross-validation framework designed to assess generalization across grape clusters.
Standard classification metrics were computed for all experiments.
For a compact summary of the evaluation units and split strategies used across all experiments (pixel-level, anomaly detection, and whole-image), see Table~\ref{tab:evaluation_units_splits}.

\graphicspath{{results/Binary_Pixel_Level_Classification/}}

% ==========================================================
% Binary Pixel-Level Classification Results
% ==========================================================
\subsection{Binary Pixel-Level Classification Results}

\noindent
Binary pixel-level classification was evaluated for distinguishing healthy from cracked tissue.
Performance was assessed using ranking-based metrics (ROC--AUC and, where reported, PR--AUC) together with class-specific Precision and Recall
for the \textit{Cracked} class (positive class). Under class imbalance, PR--AUC\textsubscript{CRACK} is treated as the primary separability metric (see Section~\ref{subsec:evaluation_protocols}).

Two sampling regimes were considered: the original naturally imbalanced distribution and a class-balanced configuration.

\subsubsection{Unbalanced vs.\ Balanced Training Sets}

\noindent
Results obtained under both sampling regimes are summarized jointly on the following landscape page.
Table~\ref{tab:pixel_metrics_unbalanced} reports the classification performance obtained using the original unbalanced dataset,
while Table~\ref{tab:pixel_metrics_balanced} presents the corresponding results obtained under balanced sampling.
The identical evaluation protocol and model set are used in both cases, enabling a direct comparison between regimes.

\paragraph{Uncertainty and statistical comparisons.}
Tables~\ref{tab:pixel_metrics_unbalanced} and \ref{tab:pixel_metrics_balanced} report mean $\pm$ standard deviation across cross-validation folds.
These descriptive summaries indicate variability across folds, but they do not by themselves quantify uncertainty of \emph{differences} between models or sampling regimes.
Accordingly, claims in this section are interpreted descriptively; no hypothesis tests or confidence intervals for model-to-model differences are reported.
A stronger comparison would require retaining fold-wise metric values (or per-sample model outputs with fold/group identifiers) and applying a paired procedure (e.g., a paired bootstrap or permutation test over folds/groups).

\FloatBarrier
% =========================================================
% BOTH TABLES ON SAME LANDSCAPE PAGE (WITH VALUES)
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level classification metrics (unbalanced training set). Values are reported as mean $\pm$ standard deviation across folds where available. Precision/recall and ROC--AUC refer to the \textit{Cracked} class (positive class); F1 is the weighted F1-score.}
    \label{tab:pixel_metrics_unbalanced}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Accuracy & Precision (Cracked) & Recall (Cracked) & Weighted F1 & ROC--AUC (Cracked) \\
        \midrule
        PLS-DA                    & 0.9927 $\pm$ 0.0168 & 0.9999 & 0.9682 & 0.9924 & 1.0000 $\pm$ 0.0000 \\
        Logistic Regression (L1)  & 0.9962 $\pm$ 0.0074 & 0.9890 & 0.9948 & 0.9963 & 0.9996 $\pm$ 0.0012 \\
        SVM (RBF)                 & 0.9938 $\pm$ 0.0094 & 0.9943 & 0.9787 & 0.9938 & 0.9997 $\pm$ 0.0010 \\
        Random Forest            & 0.9968 $\pm$ 0.0055 & 0.9964 & 0.9904 & 0.9968 & 0.9990 $\pm$ 0.0035 \\
        XGBoost                  & 0.9947 $\pm$ 0.0103 & 0.9953 & 0.9798 & 0.9946 & 1.0000 $\pm$ 0.0000 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.3cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level classification metrics (balanced training set). Values are reported as mean $\pm$ standard deviation across folds where available. Precision/recall and ROC--AUC refer to the \textit{Cracked} class (positive class); F1 is the weighted F1-score.}
    \label{tab:pixel_metrics_balanced}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Accuracy & Precision (Cracked) & Recall (Cracked) & Weighted F1 & ROC--AUC (Cracked) \\
        \midrule
        PLS-DA                    & 0.9961 $\pm$ 0.0086 & 0.9998 & 0.9929 & 0.9961 & 1.0000 $\pm$ 0.0000 \\
        Logistic Regression (L1)  & 0.9930 $\pm$ 0.0148 & 0.9994 & 0.9861 & 0.9930 & 0.9999 $\pm$ 0.0002 \\
        SVM (RBF)                 & 0.9876 $\pm$ 0.0202 & 0.9948 & 0.9771 & 0.9875 & 0.9998 $\pm$ 0.0007 \\
        Random Forest            & 0.9935 $\pm$ 0.0110 & 0.9970 & 0.9882 & 0.9934 & 0.9990 $\pm$ 0.0038 \\
        XGBoost                  & 0.9934 $\pm$ 0.0107 & 0.9972 & 0.9882 & 0.9933 & 1.0000 $\pm$ 0.0001 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}




\subsection{Pixel-Level Classification Results Under Different Labeling and Balancing Regimes (10 classes)}

\noindent
This subsection reports pixel-level classification performance under three labeling strategies:
(i) \textit{crack\_regular\_rest},
(ii) \textit{crack\_vs\_rest}, and
(iii) a \textit{multi\_class} formulation.
All models were evaluated using identical domain-aware LOGO cross-validation splits.
For each labeling strategy, two sampling regimes were considered: a class-balanced configuration and the original naturally imbalanced distribution, resulting in six experimental configurations in total.

\noindent
Performance is reported as mean $\pm$ standard deviation across folds.
Reported metrics include global measures (Accuracy, Balanced Accuracy and Macro-F1), CRACK-specific metrics (Precision, Recall, F1, ROC--AUC and PR--AUC), and total runtime per experiment.
The evaluated classifiers include Logistic Regression (L1), Random Forest, XGBoost, a shallow Multi-Layer Perceptron (MLP), and PLS-DA.

\noindent
As with the binary results above, these fold-aggregated summaries are used descriptively; no hypothesis tests or confidence intervals for differences between labeling/balancing regimes are reported.

\subsubsection{Balanced Training Sets}

\noindent
Results obtained under balanced sampling are summarized in Table~\ref{tab:pixel_metrics_balanced_all}.
The table reports performance for both binary labeling schemes and for the multi-class configuration.

\subsubsection{Unbalanced Training Sets}

\noindent
Results obtained on naturally imbalanced data are summarized in Table~\ref{tab:pixel_metrics_unbalanced_all}.
The same set of models and metrics are reported for direct comparison with the balanced regime.

\FloatBarrier

% =========================================================
% PAGE 1 (LANDSCAPE): UNBALANCED - SINGLE TABLE
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

\begin{table}[p]
    \centering
    \caption{Pixel-level classification metrics (\textbf{unbalanced} training set). Values are reported as mean $\pm$ standard deviation across folds.}
    \label{tab:pixel_metrics_unbalanced_all}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}llccccccc@{}}
        \toprule
        Label setup & Model &
        Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & Macro-F1 & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_regular\_rest}} \\
        & PLS-DA                & 0.5087 $\pm$ 0.0044 & 0.6879 $\pm$ 0.0239 & 0.8825 $\pm$ 0.1174 & 0.7537 $\pm$ 0.2385 & 0.5819 $\pm$ 0.0199 & 0.9977 $\pm$ 0.0049 & 0.8630 $\pm$ 0.1242 \\
        & Logistic Regression (L1) & 0.8537 $\pm$ 0.0021 & 0.8683 $\pm$ 0.0075 & 0.9654 $\pm$ 0.0185 & 0.9663 $\pm$ 0.0748 & 0.8010 $\pm$ 0.0049 & 0.9993 $\pm$ 0.0021 & 0.9839 $\pm$ 0.0116 \\
        & Random Forest         & 0.9175 $\pm$ 0.0056 & 0.9179 $\pm$ 0.0116 & 0.8942 $\pm$ 0.0644 & 0.9394 $\pm$ 0.0701 & 0.8904 $\pm$ 0.0106 & 0.9975 $\pm$ 0.0037 & 0.9622 $\pm$ 0.0495 \\
        & XGBoost               & 0.9212 $\pm$ 0.0040 & 0.9162 $\pm$ 0.0094 & 0.9804 $\pm$ 0.0118 & 0.9493 $\pm$ 0.0894 & 0.8832 $\pm$ 0.0074 & 0.9992 $\pm$ 0.0017 & 0.9902 $\pm$ 0.0160 \\
        & MLP (Small)           & 0.9175 $\pm$ 0.0059 & 0.9166 $\pm$ 0.0122 & 0.9530 $\pm$ 0.0272 & 0.9729 $\pm$ 0.0398 & 0.8759 $\pm$ 0.0176 & 0.9994 $\pm$ 0.0017 & 0.9911 $\pm$ 0.0122 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_vs\_rest}} \\
        & PLS-DA                & 0.5120 $\pm$ 0.0047 & 0.6940 $\pm$ 0.0189 & 0.8843 $\pm$ 0.1117 & 0.7623 $\pm$ 0.2303 & 0.5893 $\pm$ 0.0164 & 0.9979 $\pm$ 0.0045 & 0.8669 $\pm$ 0.1186 \\
        & Logistic Regression (L1) & 0.8574 $\pm$ 0.0022 & 0.8704 $\pm$ 0.0076 & 0.9669 $\pm$ 0.0190 & 0.9651 $\pm$ 0.0759 & 0.8049 $\pm$ 0.0050 & 0.9993 $\pm$ 0.0022 & 0.9847 $\pm$ 0.0114 \\
        & Random Forest         & 0.9147 $\pm$ 0.0060 & 0.9159 $\pm$ 0.0126 & 0.8871 $\pm$ 0.0701 & 0.9368 $\pm$ 0.0751 & 0.8857 $\pm$ 0.0116 & 0.9975 $\pm$ 0.0041 & 0.9610 $\pm$ 0.0535 \\
        & XGBoost               & 0.9200 $\pm$ 0.0043 & 0.9154 $\pm$ 0.0098 & 0.9809 $\pm$ 0.0120 & 0.9508 $\pm$ 0.0888 & 0.8825 $\pm$ 0.0078 & 0.9992 $\pm$ 0.0018 & 0.9900 $\pm$ 0.0165 \\
        & MLP (Small)           & 0.9163 $\pm$ 0.0062 & 0.9158 $\pm$ 0.0129 & 0.9521 $\pm$ 0.0282 & 0.9736 $\pm$ 0.0390 & 0.8741 $\pm$ 0.0188 & 0.9994 $\pm$ 0.0017 & 0.9912 $\pm$ 0.0122 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{multi\_class}} \\
        & PLS-DA                & 0.9670 $\pm$ 0.0211 & 0.9787 $\pm$ 0.0121 & 0.7204 $\pm$ 0.1749 & 0.9927 $\pm$ 0.0127 & 0.9013 $\pm$ 0.0709 & 0.9978 $\pm$ 0.0025 & 0.9677 $\pm$ 0.0372 \\
        & Logistic Regression (L1) & 0.9972 $\pm$ 0.0031 & 0.9959 $\pm$ 0.0097 & 0.9675 $\pm$ 0.0225 & 0.9977 $\pm$ 0.0078 & 0.9920 $\pm$ 0.0085 & 0.9996 $\pm$ 0.0016 & 0.9971 $\pm$ 0.0079 \\
        & Random Forest         & 0.9965 $\pm$ 0.0030 & 0.9940 $\pm$ 0.0120 & 0.9675 $\pm$ 0.0234 & 0.9969 $\pm$ 0.0090 & 0.9906 $\pm$ 0.0100 & 0.9994 $\pm$ 0.0026 & 0.9970 $\pm$ 0.0077 \\
        & XGBoost               & 0.9978 $\pm$ 0.0021 & 0.9946 $\pm$ 0.0103 & 0.9812 $\pm$ 0.0149 & 0.9908 $\pm$ 0.0206 & 0.9923 $\pm$ 0.0074 & 0.9995 $\pm$ 0.0021 & 0.9977 $\pm$ 0.0069 \\
        & MLP (Small)           & 0.9905 $\pm$ 0.0249 & 0.9890 $\pm$ 0.0224 & 0.9244 $\pm$ 0.1648 & 0.9866 $\pm$ 0.0259 & 0.9699 $\pm$ 0.0704 & 0.9965 $\pm$ 0.0083 & 0.9898 $\pm$ 0.0247 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

% =========================================================
% PAGE 2 (LANDSCAPE): BALANCED - SINGLE TABLE
% =========================================================
\clearpage
\thispagestyle{plain}

\begin{table}[p]
    \centering
    \caption{Pixel-level classification metrics (\textbf{balanced} training set). Values are reported as mean $\pm$ standard deviation across folds.}
    \label{tab:pixel_metrics_balanced_all}
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}llccccccc@{}}
        \toprule
        Label setup & Model &
        Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & Macro-F1 & ROC--AUC (Cracked) & PR--AUC (Cracked) \\
        \midrule

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_regular\_rest}} \\
        & PLS-DA                & 0.4771 $\pm$ 0.0088 & 0.7061 $\pm$ 0.0107 & 0.8433 $\pm$ 0.0910 & 0.9451 $\pm$ 0.1069 & 0.5875 $\pm$ 0.0090 & 0.9982 $\pm$ 0.0046 & 0.9500 $\pm$ 0.0899 \\
        & Logistic Regression (L1) & 0.8629 $\pm$ 0.0033 & 0.8653 $\pm$ 0.0073 & 0.9865 $\pm$ 0.0067 & 0.9671 $\pm$ 0.0739 & 0.8097 $\pm$ 0.0054 & 0.9994 $\pm$ 0.0018 & 0.9874 $\pm$ 0.0090 \\
        & Random Forest         & 0.9277 $\pm$ 0.0046 & 0.9199 $\pm$ 0.0110 & 0.9101 $\pm$ 0.0533 & 0.9194 $\pm$ 0.1124 & 0.9047 $\pm$ 0.0087 & 0.9975 $\pm$ 0.0035 & 0.9725 $\pm$ 0.0438 \\
        & XGBoost               & 0.9242 $\pm$ 0.0026 & 0.9111 $\pm$ 0.0084 & 0.9932 $\pm$ 0.0042 & 0.9437 $\pm$ 0.0954 & 0.8839 $\pm$ 0.0067 & 0.9993 $\pm$ 0.0016 & 0.9919 $\pm$ 0.0175 \\
        & MLP (Small)           & 0.9219 $\pm$ 0.0053 & 0.9140 $\pm$ 0.0125 & 0.9831 $\pm$ 0.0123 & 0.9832 $\pm$ 0.0223 & 0.8869 $\pm$ 0.0167 & 0.9982 $\pm$ 0.0057 & 0.9934 $\pm$ 0.0097 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{crack\_vs\_rest}} \\
        & PLS-DA                & 0.4782 $\pm$ 0.0090 & 0.7078 $\pm$ 0.0106 & 0.8445 $\pm$ 0.0904 & 0.9455 $\pm$ 0.1063 & 0.5889 $\pm$ 0.0093 & 0.9982 $\pm$ 0.0046 & 0.9507 $\pm$ 0.0886 \\
        & Logistic Regression (L1) & 0.8642 $\pm$ 0.0032 & 0.8662 $\pm$ 0.0074 & 0.9866 $\pm$ 0.0067 & 0.9674 $\pm$ 0.0738 & 0.8109 $\pm$ 0.0055 & 0.9994 $\pm$ 0.0018 & 0.9876 $\pm$ 0.0089 \\
        & Random Forest         & 0.9261 $\pm$ 0.0049 & 0.9191 $\pm$ 0.0113 & 0.9092 $\pm$ 0.0546 & 0.9192 $\pm$ 0.1128 & 0.9038 $\pm$ 0.0091 & 0.9974 $\pm$ 0.0036 & 0.9722 $\pm$ 0.0444 \\
        & XGBoost               & 0.9238 $\pm$ 0.0027 & 0.9110 $\pm$ 0.0085 & 0.9931 $\pm$ 0.0042 & 0.9439 $\pm$ 0.0954 & 0.8836 $\pm$ 0.0068 & 0.9993 $\pm$ 0.0016 & 0.9919 $\pm$ 0.0176 \\
        & MLP (Small)           & 0.9221 $\pm$ 0.0052 & 0.9143 $\pm$ 0.0126 & 0.9830 $\pm$ 0.0124 & 0.9833 $\pm$ 0.0222 & 0.8871 $\pm$ 0.0165 & 0.9982 $\pm$ 0.0057 & 0.9934 $\pm$ 0.0097 \\
        \addlinespace[4pt]

        \multicolumn{9}{@{}l@{}}{\textbf{multi\_class}} \\
        & PLS-DA                & 0.9666 $\pm$ 0.0210 & 0.9786 $\pm$ 0.0120 & 0.7179 $\pm$ 0.1759 & 0.9926 $\pm$ 0.0129 & 0.9010 $\pm$ 0.0707 & 0.9978 $\pm$ 0.0025 & 0.9674 $\pm$ 0.0375 \\
        & Logistic Regression (L1) & 0.9973 $\pm$ 0.0031 & 0.9960 $\pm$ 0.0097 & 0.9677 $\pm$ 0.0224 & 0.9977 $\pm$ 0.0078 & 0.9921 $\pm$ 0.0085 & 0.9996 $\pm$ 0.0016 & 0.9971 $\pm$ 0.0079 \\
        & Random Forest         & 0.9966 $\pm$ 0.0030 & 0.9941 $\pm$ 0.0120 & 0.9676 $\pm$ 0.0233 & 0.9969 $\pm$ 0.0090 & 0.9907 $\pm$ 0.0100 & 0.9994 $\pm$ 0.0026 & 0.9970 $\pm$ 0.0077 \\
        & XGBoost               & 0.9979 $\pm$ 0.0021 & 0.9947 $\pm$ 0.0103 & 0.9813 $\pm$ 0.0149 & 0.9908 $\pm$ 0.0206 & 0.9924 $\pm$ 0.0074 & 0.9995 $\pm$ 0.0021 & 0.9977 $\pm$ 0.0069 \\
        & MLP (Small)           & 0.9906 $\pm$ 0.0249 & 0.9891 $\pm$ 0.0224 & 0.9246 $\pm$ 0.1647 & 0.9866 $\pm$ 0.0259 & 0.9700 $\pm$ 0.0704 & 0.9965 $\pm$ 0.0083 & 0.9898 $\pm$ 0.0247 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\end{landscape}

\medskip
\noindent
The supervised pixel-level results above (Tables~\ref{tab:pixel_metrics_unbalanced} and \ref{tab:pixel_metrics_balanced}, and the broader regimes in Tables~\ref{tab:pixel_metrics_unbalanced_all} and \ref{tab:pixel_metrics_balanced_all}) summarize discriminative performance when labeled training data are available.
To complement this setting, the next subsection evaluates reconstruction-based anomaly detection with autoencoders, where crack detection is driven by reconstruction error rather than a direct discriminative classifier.
Quantitative results for these anomaly-detection variants are summarized in Tables~\ref{tab:autoencoder_pixel_results}--\ref{tab:autoencoder_pixel_results_noncrack_multiclass_balanced}.



% % ==========================================================
% % Autoencoder-Based Pixel-Level Anomaly Detection
% % ==========================================================
% \subsection{Autoencoder-Based Pixel-Level Anomaly Detection}

% In addition to supervised classifiers, pixel-level crack detection was evaluated using unsupervised reconstruction-based anomaly detection with autoencoders trained exclusively on non-cracked samples.
% Three fully-connected architectures were examined: \textit{Autoencoder (64--32--16)}, \textit{Autoencoder (128--64--32)}, and \textit{Autoencoder (64--32--8)}.
% All models were trained and evaluated using the same domain-aware LOGO cross-validation splits employed in the supervised experiments.

% Anomaly scores were computed from reconstruction errors and converted to binary class labels using a percentile-based threshold estimated on the training folds.
% Performance is reported as mean $\pm$ standard deviation across folds.
% Reported metrics include Accuracy, Precision, Recall, F1, ROC--AUC and PR--AUC for the \textit{Cracked} class, together with aggregate confusion counts across folds and computational cost indicators.

% \subsubsection{Comparison Between Autoencoder Architectures}

% Quantitative results for all evaluated autoencoder variants are summarized in Table~\ref{tab:autoencoder_pixel_results}.
% The comparison highlights the trade-off between detection performance, reconstruction separation between normal and anomalous pixels, and computational efficiency.

% ==========================================================
% Autoencoder-Based Pixel-Level Anomaly Detection
% ==========================================================
\subsection{Autoencoder-Based Pixel-Level Anomaly Detection}

\noindent
In addition to supervised classifiers, pixel-level crack detection was evaluated using reconstruction-based anomaly detection with autoencoders.
The experiments were designed in a progressive manner.
First, a binary formulation was considered, in which the autoencoder was trained exclusively on healthy (non-cracked) pixels and evaluated against cracked tissue.
Subsequently, the setting was extended to a multi-class environment, in which background and nuisance categories (e.g., leaves, branches, plastic and calibration targets) were explicitly included during testing.

Within the multi-class regime, two complementary training strategies were investigated.
In the first, the autoencoder was trained solely on pixels belonging to the \textit{Cracked} class, such that crack signatures define the reconstruction manifold and all remaining classes are treated as anomalous.
In the second strategy, the model was trained on all semantic classes except \textit{Cracked}, thereby enforcing accurate reconstruction of diverse non-crack appearances while promoting elevated reconstruction errors for crack pixels at inference time.

Across all configurations, three fully-connected architectures were examined: \textit{Autoencoder (64--32--16)}, \textit{Autoencoder (128--64--32)}, and \textit{Autoencoder (64--32--8)}.
All models were trained and evaluated using identical domain-aware LOGO cross-validation splits employed in the supervised experiments.

Anomaly scores were computed from reconstruction errors and converted into binary class labels using a percentile-based threshold estimated on the training folds.
Performance is reported as mean $\pm$ standard deviation across folds.
Reported metrics include overall Accuracy together with class-conditional metrics (Precision, Recall, F1, ROC--AUC and PR--AUC) for the \textit{Cracked} class (positive class), alongside aggregate confusion counts across folds and computational cost indicators.
For each autoencoder formulation, two sampling regimes were considered: the original naturally imbalanced distribution and a class-balanced configuration.
For a compact summary of the evaluation units and split strategies used across all experiments (including the LOGO variants used here), see Table~\ref{tab:evaluation_units_splits}.


% =========================================================
% AUTOENCODER ONE-CLASS BINARY: UNBALANCED + BALANCED (SAME PAGE)
% =========================================================
\noindent
Table~\ref{tab:autoencoder_pixel_results_balanced} summarizes the balanced-sampling performance of the one-class autoencoder trained on non-cracked samples; the key takeaway is that anomaly detection can separate cracked pixels in this controlled pixel-level setting, but these results should be interpreted as complementary to (not a replacement for) the supervised whole-image pipeline evaluated under field conditions.

\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on non-cracked samples (unbalanced sampling).
    Values are reported as mean $\pm$ standard deviation across folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.8969 $\pm$ 0.1646
        & 0.8226 $\pm$ 0.1925
        & 0.9971 $\pm$ 0.0016
        & 0.9017 $\pm$ 0.1047
        & 0.9814 $\pm$ 0.0327
        & 0.9554 $\pm$ 0.0900
        & 63.34 $\pm$ 2.80
        & 0.043 $\pm$ 0.004 \\

        Autoencoder (128--64--32)
        & 0.8597 $\pm$ 0.1813
        & 0.7544 $\pm$ 0.2308
        & 0.9974 $\pm$ 0.0015
        & 0.8628 $\pm$ 0.1270
        & 0.9673 $\pm$ 0.0469
        & 0.9253 $\pm$ 0.1190
        & 60.90 $\pm$ 1.48
        & 0.057 $\pm$ 0.001 \\

        Autoencoder (64--32--8)
        & 0.8923 $\pm$ 0.1648
        & 0.8126 $\pm$ 0.1938
        & 0.9978 $\pm$ 0.0013
        & 0.8965 $\pm$ 0.1050
        & 0.9799 $\pm$ 0.0334
        & 0.9511 $\pm$ 0.0913
        & 61.02 $\pm$ 1.46
        & 0.041 $\pm$ 0.001 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.0cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on non-cracked samples (balanced sampling).
    Values are reported as mean $\pm$ standard deviation across folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_balanced}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9343 $\pm$ 0.0989
        & 0.9122 $\pm$ 0.1348
        & 0.9906 $\pm$ 0.0340
        & 0.9437 $\pm$ 0.0820
        & 0.9823 $\pm$ 0.0457
        & 0.9702 $\pm$ 0.0592
        & 65.35 $\pm$ 1.90
        & 0.008 $\pm$ 0.002 \\

        Autoencoder (128--64--32)
        & 0.9083 $\pm$ 0.1332
        & 0.8771 $\pm$ 0.1576
        & 0.9982 $\pm$ 0.0058
        & 0.9256 $\pm$ 0.1016
        & 0.9919 $\pm$ 0.0178
        & 0.9833 $\pm$ 0.0357
        & 58.49 $\pm$ 3.87
        & 0.007 $\pm$ 0.001 \\

        Autoencoder (64--32--8)
        & 0.9353 $\pm$ 0.1027
        & 0.9074 $\pm$ 0.1338
        & 0.9977 $\pm$ 0.0100
        & 0.9449 $\pm$ 0.0833
        & 0.9858 $\pm$ 0.0344
        & 0.9757 $\pm$ 0.0501
        & 52.04 $\pm$ 2.44
        & 0.006 $\pm$ 0.002 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}


% =========================================================
% AUTOENCODER CRACK-ONLY MULTI-CLASS: UNBALANCED + BALANCED (SAME PAGE)
% =========================================================
\noindent
Tables~\ref{tab:autoencoder_pixel_results_crack_only_multiclass} and~\ref{tab:autoencoder_pixel_results_crack_only_multiclass_balanced} report the crack-only training variant; the key takeaway is that when the ``normal'' class is restricted to cracked spectra, reconstruction error becomes a strong discriminator against all other semantic classes.

\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained exclusively on \textit{Cracked} pixels and evaluated in a multi-class setting (unbalanced sampling).
    Values are reported as mean across LOGO folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_crack_only_multiclass}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9660
        & 0.9995
        & 0.9662
        & 0.9826
        & 0.9858
        & 0.9999
        & 26.96
        & 0.148 \\

        Autoencoder (128--64--32)
        & 0.9861
        & 0.9991
        & 0.9869
        & 0.9930
        & 0.9846
        & 0.9999
        & 27.12
        & 0.207 \\

        Autoencoder (64--32--8)
        & 0.9629
        & 0.9994
        & 0.9632
        & 0.9809
        & 0.9834
        & 0.9999
        & 26.20
        & 0.155 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.0cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained exclusively on \textit{Cracked} pixels and evaluated in a multi-class setting (balanced sampling).
    Values are reported as mean $\pm$ standard deviation across LOGO folds.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_crack_only_multiclass_balanced}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9660 $\pm$ 0.0106
        & 0.9995 $\pm$ 0.0008
        & 0.9662 $\pm$ 0.0108
        & 0.9826 $\pm$ 0.0055
        & 0.9858 $\pm$ 0.0281
        & 0.9999 $\pm$ 0.0001
        & 61.06 $\pm$ 7.56
        & 0.045 $\pm$ 0.013 \\

        Autoencoder (128--64--32)
        & 0.9861 $\pm$ 0.0052
        & 0.9991 $\pm$ 0.0017
        & 0.9869 $\pm$ 0.0051
        & 0.9930 $\pm$ 0.0027
        & 0.9846 $\pm$ 0.0274
        & 0.9999 $\pm$ 0.0001
        & 63.03 $\pm$ 8.09
        & 0.056 $\pm$ 0.004 \\

        Autoencoder (64--32--8)
        & 0.9629 $\pm$ 0.0149
        & 0.9994 $\pm$ 0.0011
        & 0.9632 $\pm$ 0.0150
        & 0.9809 $\pm$ 0.0079
        & 0.9834 $\pm$ 0.0298
        & 0.9999 $\pm$ 0.0002
        & 59.80 $\pm$ 8.55
        & 0.041 $\pm$ 0.003 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}


% =========================================================
% AUTOENCODER NON-CRACK MULTI-CLASS: UNBALANCED + BALANCED (SAME PAGE)
% =========================================================
\noindent
Table~\ref{tab:autoencoder_pixel_results_noncrack_multiclass} captures the failure mode of the non-crack-trained autoencoder in the multi-class setting: despite high overall accuracy, cracked-class recall and PR--AUC collapse, indicating that ``normal'' is too heterogeneous for reconstruction-error thresholding to reliably isolate cracks.

\clearpage
\begin{landscape}
\thispagestyle{plain}

% -----------------------------
% UNBALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on all semantic classes except \textit{Cracked} and evaluated in a multi-class setting (unbalanced sampling).
    Values are reported for the held-out test split.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_noncrack_multiclass}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9969
        & 0.0096
        & 0.1110
        & 0.0177
        & 0.4575
        & 0.0083
        & 1429.22
        & 0.388 \\

        Autoencoder (128--64--32)
        & 0.9986
        & 0.0118
        & 0.0126
        & 0.0122
        & 0.4513
        & 0.0084
        & 999.73
        & 0.255 \\

        Autoencoder (64--32--8)
        & 0.9984
        & 0.0207
        & 0.0316
        & 0.0250
        & 0.4542
        & 0.0110
        & 916.85
        & 0.141 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\vspace{1.0cm}

% -----------------------------
% BALANCED
% -----------------------------
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Pixel-level anomaly detection performance using autoencoders trained on all semantic classes except \textit{Cracked} and evaluated in a multi-class setting (balanced sampling).
    Values are reported for the held-out test split.
    Accuracy is reported overall, while Precision/Recall/F1/ROC--AUC/PR--AUC correspond to the \textit{Cracked} class (positive class).}
    \label{tab:autoencoder_pixel_results_noncrack_multiclass_balanced}
    \small
    \renewcommand{\arraystretch}{1.4}
    \setlength{\tabcolsep}{7pt}
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        Architecture &
        Accuracy &
        Precision (Cracked) &
        Recall (Cracked) &
        F1 (Cracked) &
        ROC--AUC (Cracked) &
        PR--AUC (Cracked) &
        Train Time [s] &
        Infer Time [s] \\
        \midrule

        Autoencoder (64--32--16)
        & 0.9791
        & 0.0000
        & 0.0000
        & 0.0000
        & 0.2472
        & 0.0083
        & 2567.17
        & 0.381 \\

        Autoencoder (128--64--32)
        & 0.9789
        & 0.0000
        & 0.0000
        & 0.0000
        & 0.2847
        & 0.0084
        & 2260.73
        & 0.299 \\

        Autoencoder (64--32--8)
        & 0.9781
        & 0.0000
        & 0.0000
        & 0.0000
        & 0.4513
        & 0.0110
        & 1853.00
        & 0.227 \\

        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{minipage}

\end{landscape}

\medskip
\noindent
The pixel-level anomaly detection experiments above quantify crack separability using reconstruction-error scores across several training formulations and sampling regimes (Tables~\ref{tab:autoencoder_pixel_results}--\ref{tab:autoencoder_pixel_results_noncrack_multiclass_balanced}).
Since the deployment objective is detection at the grape-cluster level, the following subsection evaluates an end-to-end full-image inference pipeline that aggregates pixel-level outputs into patch- and image-level decisions.
Dataset composition is reported in Table~\ref{tab:full_image_dataset_composition}, and early/late full-image performance is summarized in Table~\ref{tab:full_image_early_late_metrics}.




\subsection{Full-Image Inference: Early vs.\ Late Acquisition-Stage Experiments}
\label{sec:full-image-early-late}

\noindent
To evaluate robustness across acquisition stages, two independent optimization and evaluation scenarios were considered, corresponding to \textit{early-stage} (initial onset / minimal cracking) and \textit{late-stage} (advanced, macroscopically visible cracking) acquisition subsets.
In both cases, post-processing and aggregation parameters were optimized on a calibration split using Optuna and subsequently assessed on an unseen test set.
For a compact summary of the evaluation units and split strategies used across all experiments (including the Row~1 calibration vs. Row~2 held-out test setup), see Table~\ref{tab:evaluation_units_splits}.
The corresponding calibration-to-test performance gaps are quantified later in Table~\ref{tab:mcc_generalization}.

\noindent
Each experiment employed identical inference logic, including pixel-level probability thresholding, morphological filtering, patch-level aggregation and global crack-percentage criteria.
In addition to the supervised pixel-level classifier, the same full-image pipeline was evaluated using an autoencoder-based anomaly detector (architecture 128--64--32, trained exclusively on \textit{Cracked} pixels).
For the autoencoder, reconstruction errors (MSE) were converted to pseudo-probabilities using an exponential decay mapping, $p_{\mathrm{CRACK}}(x)=\exp(-e(x)/\tau)$, where $\tau$ is the stored training-derived threshold, after which the identical post-processing and aggregation logic was applied.
Performance was quantified on both calibration and test splits using a comprehensive set of metrics. For system-level detection, the main performance indicator is \textbf{F1 (Cracked)}, supported by \textbf{Precision (Cracked)}, \textbf{Recall (Cracked)}, and \textbf{MCC}; accuracy is reported for completeness. Additional measures (F2, specificity, NPV) and the underlying confusion matrices are included to characterize error trade-offs.

\noindent
The dataset composition for the two acquisition scenarios is reported in Table~\ref{tab:full_image_dataset_composition}.
Quantitative detection performance is summarized in Table~\ref{tab:full_image_early_late_metrics}, enabling a direct comparison between early- and late-stage conditions and between calibration and test generalization behavior.



% =========================================================
% FULL-IMAGE DATASET COMPOSITION TABLE (PRETTY)
% =========================================================
\begin{table}[htbp]
\centering
\caption{Dataset composition for full-image crack-detection experiments under early- and late-stage acquisition conditions (initial/minimal vs.\ advanced cracking).
Row~1 was used for calibration, while Row~2 served as an independent test set.}
\label{tab:full_image_dataset_composition}

\small
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}

\begin{tabular}{@{}lllcccc@{}}

\toprule
Row & Stage & Split & Total & Unique & Pos & Neg \\
\midrule
Row 1 & Early & Calibration & 172 & 60 & 32 & 140 \\
Row 1 & Late  & Calibration & 160 & 60 & 35 & 125 \\
\addlinespace[4pt]
Row 2 & Early & Test        & 60  & 60 & 32 & 28  \\
Row 2 & Late  & Test        & 60  & 60 & 34 & 26  \\
\bottomrule
\end{tabular}

\end{table}

% =========================================================
% FULL IMAGE EARLY vs LATE – METRICS TABLE
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

\begin{table}[p]
\centering
\caption{Full-image crack detection performance for early- and late-stage acquisition experiments (initial/minimal vs.\ advanced cracking).
Metrics are reported separately for calibration and test splits.
Confusion-matrix entries correspond to the number of grape clusters classified as cracked or healthy.
The main performance indicator is \textbf{F1 (Cracked)}. Accuracy, Balanced Accuracy and MCC are reported overall. Precision/Recall/F1/F2 refer to the \textit{Cracked} class (positive class), while Specificity and NPV refer to the \textit{Healthy} class (negative class).}
\label{tab:full_image_early_late_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
34 & 5 & 120 & 1 &
0.963 & 0.966 & 0.872 & 0.971 & 0.919 & 0.950 & 0.960 & 0.992 & 0.897 \\

Late & Test &
27 & 5 & 21 & 7 &
0.800 & 0.801 & 0.844 & 0.794 & 0.818 & 0.804 & 0.808 & 0.750 & 0.598 \\

\addlinespace[4pt]

Early & Calibration &
23 & 6 & 134 & 9 &
0.913 & 0.838 & 0.793 & 0.719 & 0.754 & 0.732 & 0.957 & 0.937 & 0.703 \\

Early & Test &
19 & 3 & 25 & 13 &
0.733 & 0.743 & 0.864 & 0.594 & 0.704 & 0.633 & 0.893 & 0.658 & 0.504 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\end{landscape}


% =========================================================
% FULL IMAGE AUTOENCODER – METRICS TABLE
% =========================================================
\clearpage
\begin{landscape}
\thispagestyle{plain}

\begin{table}[p]
\centering
\caption{Full-image crack detection using autoencoder-based anomaly detection (128--64--32, trained on \textit{Cracked} pixels).
Reconstruction errors (MSE) were converted to pseudo-probabilities via exponential decay, $p_{\mathrm{CRACK}}(x)=\exp(-e(x)/\tau)$;
the same post-processing pipeline (thresholding, morphological filtering, patch-based aggregation) was applied.
Metrics are reported for calibration and test splits under early- and late-stage conditions.
The main performance indicator is \textbf{F1 (Cracked)}. Accuracy, Balanced Accuracy and MCC are reported overall. Precision/Recall/F1/F2 refer to the \textit{Cracked} class (positive class), while Specificity and NPV refer to the \textit{Healthy} class (negative class).}
\label{tab:full_image_autoencoder_metrics}

\small
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}

\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}{@{}llccccccccccccc@{}}

\toprule
Stage &
Split &
TP &
FP &
TN &
FN &
Acc &
Balanced Accuracy &
Precision (Cracked) &
Recall (Cracked) &
F1 (Cracked) &
F2 (Cracked) &
Specificity (Healthy) &
NPV (Healthy) &
MCC \\
\midrule

Late & Calibration &
26 & 7 & 118 & 9 &
0.900 & 0.843 & 0.788 & 0.743 & 0.765 & 0.751 & 0.944 & 0.929 & 0.702 \\

Late & Test &
22 & 7 & 19 & 12 &
0.683 & 0.689 & 0.759 & 0.647 & 0.698 & 0.667 & 0.731 & 0.613 & 0.375 \\

\addlinespace[4pt]

Early & Calibration &
19 & 17 & 123 & 13 &
0.826 & 0.736 & 0.528 & 0.594 & 0.559 & 0.579 & 0.879 & 0.904 & 0.452 \\

Early & Test &
13 & 13 & 15 & 19 &
0.467 & 0.471 & 0.500 & 0.406 & 0.448 & 0.422 & 0.536 & 0.441 & $-$0.058 \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\end{landscape}

\medskip
\noindent
Full-image results in Table~\ref{tab:full_image_early_late_metrics} quantify system-level detection performance under early- and late-stage conditions using a fixed post-processing and aggregation pipeline, and Table~\ref{tab:full_image_autoencoder_metrics} reports the analogous evaluation when driven by autoencoder-based scores.
The next experiment investigates the effect of spectral wavelength selection on this same end-to-end pipeline by applying Backward Feature Selection (BFS) and tracking CRACK metrics as a function of the number of retained wavelengths.
The resulting performance trajectories are shown in Figures~\ref{fig:bfs_dual_metric} and \ref{fig:bfs_threshold_markers}.


% ------------------------------------------------------------
\subsection{Wavelength selection (BFS) setup}
\label{sec:bfs-setup}

\noindent
Backward Feature Selection (BFS) was applied to the hyperspectral channels to evaluate
full-image crack-detection performance as a function of the number of selected wavelengths.
At each BFS step, the pipeline performance was recorded using CRACK-class metrics, with \textbf{CRACK PR--AUC} treated as the primary objective (Section~\ref{subsec:wavelength_selection}).
The \textbf{full-spectrum baseline} corresponds to \textbf{159 wavelengths} (450--925\,nm) and serves as the reference point for the performance curves and for the subset comparisons reported below.

\FloatBarrier

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/bfs_dual_metric.png}
    \caption{Backward Feature Selection (BFS) trajectories for the full-image pipeline, plotted as CRACK-class PR--AUC (primary objective) and CRACK-class F1 versus the number of retained wavelengths ($n$, unitless count; full baseline $n{=}159$ over 450--925\,nm). Y-axis metrics are unitless in $[0,1]$. Takeaway: PR--AUC exhibits a broad near-optimal plateau, motivating reduced-band configurations.}
    \label{fig:bfs_dual_metric}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/feature_selection/bfs_f1_thresholds.png}
        \caption{CRACK-class F1 vs. number of wavelengths ($n$), with markers at the maximum and at 0.5\%/1.0\% drop points.}
        \label{fig:bfs_f1_thresholds}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/feature_selection/bfs_prauc_thresholds.png}
        \caption{CRACK-class PR--AUC (primary BFS objective) vs. number of wavelengths ($n$), with the same threshold markers.}
        \label{fig:bfs_prauc_thresholds}
    \end{subfigure}
    \caption{BFS threshold-marker visualization on CRACK metrics, used to define representative reduced-band subset sizes. Markers indicate the maximum and the first $n$ where performance falls more than 0.5\% and 1.0\% below that maximum (unitless drop). Takeaway: these markers operationalize ``top-$n$'' selection without introducing an arbitrary $n$.}
    \label{fig:bfs_threshold_markers}
\end{figure}

\begin{table}[!htbp]
\centering
\caption{Summary of BFS wavelength-count thresholds for CRACK metrics.}
\label{tab:bfs_thresholds_summary}
\begin{tabular}{lcccccc}
\toprule
Metric & Max score & $n_{\max}$ & 0.5\% drop: $n$ & score & 1.0\% drop: $n$ & score \\
\midrule
CRACK PR--AUC & 0.9948 & 30  & 11  & 0.9878 & 9  & 0.9790 \\
CRACK F1      & 0.9854 & 159 & 105 & 0.9800 & 88 & 0.9751 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Subset selection criteria.}
\noindent
Representative subset sizes were selected directly from the BFS performance trajectories using the same threshold-marker convention shown in Figure~\ref{fig:bfs_threshold_markers} and summarized in Table~\ref{tab:bfs_thresholds_summary}.
Unless stated otherwise, subset choice is based on the \textbf{CRACK PR--AUC} curve (Figure~\ref{fig:bfs_prauc_thresholds}), while CRACK F1 (Figure~\ref{fig:bfs_f1_thresholds}) is plotted as a complementary, threshold-dependent operating-point summary.
Specifically, the \textbf{top-30} subset corresponds to the wavelength count $n_{\max}$ that maximizes CRACK PR--AUC (here $n_{\max}{=}30$), and the \textbf{top-11} subset corresponds to the first wavelength count encountered when reducing features for which CRACK PR--AUC drops by more than 0.5\% from its maximum (i.e., falls below $0.995\times\mathrm{PR\mbox{--}AUC}_{\max}$; here $n{=}11$).
For reference, the analogous 1.0\% drop threshold on CRACK PR--AUC corresponds to $n{=}9$ and is included as a more aggressive reduced-band setting in the end-to-end full-image evaluation (Table~\ref{tab:full_image_wavelength_results}).


\FloatBarrier


\FloatBarrier

\subsection{Full-image wavelength-selection results}

\noindent
Full-image crack-detection experiments were conducted for several wavelength subsets
after complete spatial post-processing, including probability thresholding,
morphological filtering, and blob-based aggregation. Performance was evaluated
separately for early- and late-stage acquisitions on an independent test set.

\noindent
Table~\ref{tab:full_image_wavelength_results} reports the obtained CRACK-class metrics
for representative wavelength counts.

\begin{table}[!htbp]
\centering
\caption{Full-image crack-detection performance for different wavelength subsets under early- and late-stage conditions.}
\label{tab:full_image_wavelength_results}
\begin{tabular}{cccccccc}
\toprule
Stage & \# Wavelengths & Accuracy & Balanced Accuracy & Precision (Cracked) & Recall (Cracked) & F1 (Cracked) & MCC \\
\midrule
Early & 9 & 0.7833 & 0.7902 & 0.8800 & 0.6875 & 0.7719 & 0.5873 \\
Late  & 9 & 0.8000 & 0.8054 & 0.8667 & 0.7647 & 0.8125 & 0.6054 \\
\midrule
Early & 11 & 0.7500 & 0.7567 & 0.8400 & 0.6562 & 0.7368 & 0.5195 \\
Late  & 11 & 0.8167 & 0.8111 & 0.8286 & 0.8529 & 0.8406 & 0.6254 \\
\midrule
Early & 30 & \textbf{0.8167} & \textbf{0.8214} & \textbf{0.8889} & \textbf{0.7500} & \textbf{0.8136} & \textbf{0.6447} \\
Late  & 30 & \textbf{0.8333} & \textbf{0.8348} & \textbf{0.8750} & 0.8235 & 0.8485 & \textbf{0.6652} \\
\midrule
Early & 159 & 0.8000 & 0.8036 & 0.8571 & \textbf{0.7500} & 0.8000 & 0.6071 \\
Late  & 159 & \textbf{0.8333} & 0.8258 & 0.8333 & \textbf{0.8824} & \textbf{0.8571} & 0.6591 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Model Generalization Analysis: Comparison between Calibration and Test MCC.}
\label{tab:mcc_generalization}
\begin{tabular}{ccccc}
\toprule
Stage & \# Features & MCC Calibration & MCC Test & \textbf{Gap ($\Delta$)} \\
\midrule
Early & 9   & 0.7381 & 0.5873 & 0.1508 \\
      & 30  & 0.7232 & \textbf{0.6447} & \textbf{0.0785} \\
      & 159 & 0.6631 & 0.6071 & 0.0560 \\
\midrule
Late  & 9   & 0.8968 & 0.6054 & 0.2914 \\
      & 30  & 0.8771 & \textbf{0.6652} & \textbf{0.2119} \\
      & 159 & 0.8931 & 0.6591 & 0.2340 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Calibration-to-test generalization gap (primary result).}
\noindent
A consistent calibration-to-test generalization gap is observed across all evaluated wavelength subset sizes and in both acquisition-stage conditions.
As summarized in Table~\ref{tab:mcc_generalization}, early-stage calibration MCC spans 0.663--0.738, while early-stage test MCC spans 0.587--0.645, corresponding to gaps of 0.056--0.151.
For the late-stage scenario, calibration MCC spans 0.877--0.897, while test MCC spans 0.605--0.665, yielding larger gaps of 0.212--0.291.
This indicates that performance on the calibration row (Row~1) can materially overestimate performance on the independent test row (Row~2), and that reduced-band conclusions should be anchored primarily in the held-out test results rather than calibration scores alone.

\noindent
\textbf{Note on uncertainty.} Table~\ref{tab:full_image_wavelength_results} (and the generalization gaps in Table~\ref{tab:mcc_generalization}) report point estimates on the held-out test set.
Where the narrative compares the full-spectrum baseline (159 wavelengths) to reduced-band subsets (e.g., 30 or 9 wavelengths), the strength of these comparisons would be improved by confidence intervals for the metric \emph{differences}.
A statistically appropriate approach would be a cluster-aware bootstrap over independent units (e.g., image/cluster-level samples), or a paired resampling procedure if per-sample outputs are retained.
Such uncertainty quantification is not included here because the exported summary results provide aggregate scores without per-unit identifiers, preventing a reliable post-hoc resampling analysis.

\FloatBarrier

\medskip
\noindent
The wavelength-selection results above provide end-to-end test-set performance at representative subset sizes (Table~\ref{tab:full_image_wavelength_results}) and summarize calibration-to-test generalization gaps (Table~\ref{tab:mcc_generalization}), complementing the full BFS performance curves in Figure~\ref{fig:bfs_dual_metric}.
To assess whether these outcomes are robust to random variation in the data partitioning and model initialization, the following stability analysis repeats the BFS elimination sequence under multiple seeds.
Per-seed results are summarized in Table~\ref{tab:bfs_stability_summary}.


% ============================================================
% FEATURE SELECTION STABILITY ANALYSIS
% ============================================================
\subsection{BFS Wavelength-Selection Stability Analysis}
\label{sec:bfs-stability}

\noindent
To assess the robustness of the Backward Feature Selection (BFS) procedure,
the full elimination sequence was repeated five times using different random seeds.
In each run, both the train/validation data split seed and the XGBoost model seed
were set to the same value (seeds 1--5), so that every repetition uses a different
data partition \emph{and} a different model initialisation.
The objective metric was CRACK PR--AUC, and elimination proceeded from 159 down to 1 wavelength.
For each repetition, the top-30 and top-11 surviving wavelengths were recorded.
Here, ``top-30'' denotes the PR--AUC-maximizing point ($n_{\max}{=}30$), while ``top-11'' denotes the 0.5\% PR--AUC drop threshold (Table~\ref{tab:bfs_thresholds_summary}; Figure~\ref{fig:bfs_threshold_markers}).

Table~\ref{tab:bfs_stability_summary} summarises the per-seed results.
Best PR--AUC ranged from 0.812 to 0.998, and the optimal number of features
varied between 64 and 153, revealing substantial sensitivity to the data partition.

\begin{table}[!htbp]
\centering
\caption{BFS stability: per-seed results under varying train/test splits.}
\label{tab:bfs_stability_summary}
\begin{tabular}{ccccc}
\toprule
Seed & Split seed & Model seed & Best PR--AUC (Cracked) & Best $n$ (wavelengths) \\
\midrule
1 & 1 & 1 & 0.9014 & 153 \\
2 & 2 & 2 & 0.9933 &  66 \\
3 & 3 & 3 & 0.8714 & 111 \\
4 & 4 & 4 & 0.9979 &  64 \\
5 & 5 & 5 & 0.8119 & 149 \\
\midrule
\multicolumn{3}{c}{Mean $\pm$ Std} & $0.915 \pm 0.077$ & $108.6 \pm 42.2$ \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

% ── Jaccard tables ──
\subsubsection{Wavelength Overlap Across Seeds}

\noindent
Tables~\ref{tab:jaccard_30} and~\ref{tab:jaccard_11} report the pairwise Jaccard similarity
between the wavelength subsets selected by each seed for the top-30 and top-11 configurations, respectively.

\begin{table}[!htbp]
\centering
\caption{Pairwise Jaccard similarity of top-30 wavelengths across seeds (varying split). Mean Jaccard = 0.493.}
\label{tab:jaccard_30}
\begin{tabular}{cccccc}
\toprule
 & Seed~1 & Seed~2 & Seed~3 & Seed~4 & Seed~5 \\
\midrule
Seed~1 & ---   & 0.409 & 0.590 & 0.722 & 0.442 \\
Seed~2 & 0.409 & ---   & 0.378 & 0.409 & 0.476 \\
Seed~3 & 0.590 & 0.378 & ---   & 0.590 & 0.442 \\
Seed~4 & 0.722 & 0.409 & 0.590 & ---   & 0.476 \\
Seed~5 & 0.442 & 0.476 & 0.442 & 0.476 & ---   \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Pairwise Jaccard similarity of top-11 wavelengths across seeds (varying split). Mean Jaccard = 0.398.}
\label{tab:jaccard_11}
\begin{tabular}{cccccc}
\toprule
 & Seed~1 & Seed~2 & Seed~3 & Seed~4 & Seed~5 \\
\midrule
Seed~1 & ---   & 0.500 & 0.412 & 0.333 & 0.412 \\
Seed~2 & 0.500 & ---   & 0.412 & 0.412 & 0.333 \\
Seed~3 & 0.412 & 0.412 & ---   & 0.500 & 0.333 \\
Seed~4 & 0.333 & 0.412 & 0.500 & ---   & 0.333 \\
Seed~5 & 0.412 & 0.333 & 0.333 & 0.333 & ---   \\
\bottomrule
\end{tabular}
\end{table}

The top-30 subsets share 13 wavelengths across all five seeds (42\%), with a mean Jaccard of 0.493.

\noindent
The top-11 subsets share 4 wavelengths across all five seeds (33\%), with a mean Jaccard of 0.398.
Based on the per-seed wavelength export files (\url{experiments/feature\_selection/stability\_bfs\_prauc/varyingsplit/*/selected\_wavelengths\_at11.csv}),
the cross-seed intersection at this setting comprises exactly four wavelengths (reported to two decimal places as stored in the exports):
(452.25, 548.55, 580.90, and 729.53)\,nm.
These same four wavelengths also appear in the 13-wavelength intersection of the top-30 subsets (validated from the corresponding \texttt{selected\_wavelengths\_at30.csv} files), indicating that they are consistently selected under both subset sizes.

\FloatBarrier

% ── Top-30 WL Figures ──
\subsubsection{Wavelength Popularity and Stability -- Top 30}

\noindent
Figures~\ref{fig:stability_30_popularity}--\ref{fig:stability_30_spectrum}
visualise the selection frequency and stability of the top-30 wavelength subsets across the five seeds.
Figure~\ref{fig:stability_30_binning} further shows that the same dominant regions persist under coarser binning, indicating that the stability conclusions are not an artefact of plotting resolution.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-30 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_30_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_binning_comparison.png}
    \caption{Wavelength popularity at different bin sizes (1, 5, 10, 20\,nm) for the top-30 subsets.}
    \label{fig:stability_30_binning}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_stability_heatmap.png}
    \caption{Stability heatmap for the top-30 BFS subsets across five varying-split seeds (objective: CRACK PR--AUC). The x-axis is wavelength binned at 5\,nm; each cell value (0--5) indicates how many seeds selected at least one wavelength within that bin (unitless count). Takeaway: selection concentrates in a limited set of repeatable spectral regions, but exact channel identities vary across splits.}
    \label{fig:stability_30_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_30wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-30 subsets.}
    \label{fig:stability_30_spectrum}
\end{figure}

\FloatBarrier

% ── Top-11 WL Figures ──
\subsubsection{Wavelength Popularity and Stability -- Top 11}

\noindent
The same analysis was repeated for the top-11 wavelength subsets
(Figures~\ref{fig:stability_11_popularity}--\ref{fig:stability_11_spectrum}),
corresponding to the most aggressive wavelength reduction evaluated
(within 1\% of the maximum PR--AUC).

\noindent
For interpretability, Figure~\ref{fig:stability_11_binning} shows how apparent wavelength ``popularity'' changes with bin size, while Figure~\ref{fig:stability_11_heatmap} highlights which 5\,nm bins are consistently selected across seeds; the key takeaway is that only a small core set remains stable under this aggressive reduction.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_popularity_overview.png}
    \caption{Distribution and popularity of the top-11 selected wavelengths across five varying-split seeds.}
    \label{fig:stability_11_popularity}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_binning_comparison.png}
    \caption{Wavelength popularity at different bin sizes (1, 5, 10, 20\,nm) for the top-11 subsets.}
    \label{fig:stability_11_binning}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_stability_heatmap.png}
    \caption{Stability heatmap for the top-11 BFS subsets across five varying-split seeds (objective: CRACK PR--AUC; 0.5\% drop setting). The x-axis is wavelength binned at 5\,nm; each cell value (0--5) indicates how many seeds selected a wavelength within that bin (unitless count). Takeaway: under aggressive reduction, only a small core of bins remains consistently selected across splits.}
    \label{fig:stability_11_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/results/feature_selection/stability_varying_11wl_wavelength_frequency_spectrum.png}
    \caption{Wavelength selection frequency along the spectrum (2\,nm sliding window) for the top-11 subsets.}
    \label{fig:stability_11_spectrum}
\end{figure}

\FloatBarrier
